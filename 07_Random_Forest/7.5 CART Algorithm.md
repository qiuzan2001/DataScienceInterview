## 0‚ÄÉWhere CART Fits in the Family Tree

| Algorithm | Split Metric               | Split Type      | Pruning?                        | Handles Regression? |
| --------- | -------------------------- | --------------- | ------------------------------- | ------------------- |
| **ID3**   | Entropy / Gain             | Multi-way       | None                            | ‚úñ                   |
| **C4.5**  | Gain Ratio                 | Multi-way       | Error-based (post)              | ‚úñ                   |
| **CART**  | Gini (class)  /  SSE (reg) | **Binary-only** | **Cost-complexity (Œ±-pruning)** | **‚úî**               |

*Mnemonic:* **CART = ‚ÄúBinary + Pruned + Works for Y as number.‚Äù**

---

## 1‚ÄÉIntuition Before Equations

1. **Binary questions only.**
   Every internal node asks a yes/no: *‚ÄúIs feature ‚â§ œÑ?‚Äù* (numeric) or *‚ÄúIs item in subset S?‚Äù* (categorical).
   ‚Üí Leads to simpler, more balanced search and faster evaluation later.

2. **‚ÄúImpurity‚Äù still rules the game.**

   * *Classification*‚ÄÇ‚ñ∏‚ÄÇpick the split that most reduces **Gini impurity**.
   * *Regression*‚ÄÇ‚ñ∏‚ÄÇpick the split that most reduces **sum-of-squared errors** (variance).

3. **Grow first, prune later.**
   Build an over-fit tree; then lop off weak subtrees by trading off accuracy vs complexity with a *single* hyper-parameter Œ±.

---

## 2‚ÄÉCore Math Pieces

### 2.1‚ÄÉGini Impurity (Classification)

$$
G(S)=\sum_{c=1}^{C}p(c)\,[1-p(c)]=1-\sum_{c} p(c)^2
$$

| Viewpoint                                                       | Take-away                                         |
| --------------------------------------------------------------- | ------------------------------------------------- |
| **Probability-of-mis-pairing** two random samples from the node | 0 when pure, peaks when classes are equally mixed |
| **Computational bonus**                                         | No logarithms ‚Üí faster than entropy               |

### 2.2‚ÄÉVariance / SSE (Regression)

For node $S$ with targets $y_i$:

$$
\text{Var}(S)=\frac{1}{|S|}\sum_{i}(y_i-\bar{y})^2
\qquad\Longrightarrow\qquad
\text{SSE}(S)=|S|\;\text{Var}(S)
$$

Smaller variance ‚áí responses cluster tighter ‚áí easier to predict with one number.

>[! Example]-
>
>  
>  ## üéØ Goal:
>  
>  We want to **predict house prices** based on one feature: **size in square meters**.
>  
>  ### üß™ Sample Data:
>  
>  | House | Size (sqm) | Price (\$1000s) ||
>  | ----- | ---------- | --------------- |-|
>  | A     | 30         | 100             ||
>  | B     | 50         | 150             ||
>  | C     | 70         | 200             ||
>  | D     | 90         | 230             ||
>  | E     | 110        | 265             ||
>  
>  ---
>  
>  ## ü™µ Step 1: Start with all data (root)
>  
>  The CART regression tree will try to **split the data into two groups** to **minimize Mean Squared Error (MSE)**.
>  
>  ### Candidate splits (midpoints between sizes):
>  
>  * Split at **(30 + 50)/2 = 40**
>  * Split at **(50 + 70)/2 = 60**
>  * Split at **(70 + 90)/2 = 80**
>  * Split at **(90 + 110)/2 = 100**
>  
>  ---
>  
>  ## üîç Step 2: Try each split
>  
>  Let‚Äôs test **split at 60 sqm**.
>  
>  ### Left group (Size < 60):
>  
>  * Houses A (30), B (50) ‚Üí Prices: \[100, 150] ‚Üí mean = 125
>  * MSE = (100 - 125)¬≤ + (150 - 125)¬≤ = 625 + 625 = **1250**
>  
>  ### Right group (Size ‚â• 60):
>  
>  * Houses C (70), D (90), E (110) ‚Üí Prices: \[200, 230, 265] ‚Üí mean = 231.7
>  * MSE = (200 - 231.7)¬≤ + (230 - 231.7)¬≤ + (265 - 231.7)¬≤
>    ‚âà 1012.89 + 2.89 + 1106.89 ‚âà **2122.67**
>  
>  ### Total MSE = 1250 + 2122.67 = **3372.67**
>  
>  We do this for each split and pick the one with **lowest total MSE**.
>  
>  ---
>  
>  ## ‚úÇÔ∏è Step 3: Choose best split
>  
>  Suppose splitting at 60 sqm gives the **lowest total MSE**, so the tree splits here.
>  
>  ```
>             [Size < 60?]
>             /         \
>          Yes           No
>         /               \
>     Predict 125      Continue splitting...
>  ```
>  
>  Each branch is now a smaller regression problem, and the tree can **recursively split** again to minimize error further ‚Äî until:
>  
>  * A max depth is reached
>  * Or samples in node are too small
>  * Or improvement is negligible
>  
>  ---
>  
>  ## üßÆ Final prediction:
>  
>  To **predict a new house** of 85 sqm:
>  
>  * 85 ‚â• 60 ‚Üí go right
>  * If there‚Äôs another split, keep following
>  * Eventually reach a leaf node with a predicted price (e.g., 230)
>  
>  ---
>  
>  ## üèÅ Summary:
>  
>  1. CART regression splits data to minimize **MSE**
>  2. It can use the **same feature multiple times**
>  3. Predictions come from the **average value in the leaf**
### 2.3‚ÄÉWeighted Post-Split Impurity (same idea as ID3)

$$
\text{ImpurityAfter}(S,\text{split})
     =\frac{|S_L|}{|S|}\,I(S_L)+\frac{|S_R|}{|S|}\,I(S_R)
$$

Choose the split that **minimises** this value
(or equivalently **maximises** the impurity *reduction*).

---

## 3‚ÄÉFinding the Best Split Efficiently

### 3.1‚ÄÉNumeric Attributes

1. **Sort once** the $(x,y)$ pairs for the attribute.
2. Scan left ‚Üí right, updating class counts / running sums.
3. Evaluate impurity only at boundaries where $x$ changes.
   *Cost per node:* $O(n_a)$ (like the linear trick you saw for C4.5).

### 3.2‚ÄÉCategorical Attributes

* Because CART insists on a binary question, it must divide the category set into **two** groups.
* Optimal split is NP-hard if you brute-force all $2^{k-1}-1$ partitions.
* Practical fix:

  1. Order the categories by increasing class-1 probability.
  2. Treat that order as numeric and apply the numeric scan above.
     *(Breiman showed this gives the exact optimum for Gini in two-class cases.)*

### 3.3‚ÄÉMissing Values ‚Äî Surrogate Splits

If the primary split uses attribute $A$ but $A=?$ for a test case,
find the *next* attribute whose split mimics the primary partition best on training data.
‚Üí Keeps prediction deterministic without global imputation.

---

## 4‚ÄÉCost-Complexity Pruning (The ‚ÄúWeakest-Link‚Äù Story)
### 4.1‚ÄÉDefine Subtree Cost

The cost of a subtree $T$ under complexity parameter $\alpha$ is:

$$
R_\alpha(T) = R(T) + \alpha \cdot |T|
$$

Where:

* $R(T)$ is the **training error** of the subtree $T$ ‚Äî computed using Gini impurity (for classification) or sum of squared errors (SSE, for regression).
* $|T|$ is the **number of terminal (leaf) nodes** in the subtree.
* $\alpha$ is a **complexity penalty parameter** that controls the trade-off between tree accuracy and size.

The higher the Œ±, the more we penalise large trees‚Äîleading to simpler models.

---

### 4.2‚ÄÉAlgorithm

1. **Grow a large tree $T_0$** (stop only when each leaf is pure or below a min-cases limit).
2. **Generate a sequence** $T_0 \supset T_1 \supset \dots \supset T_M$ by repeatedly collapsing the subtree whose removal least increases $R_\alpha$.

   * This is the ‚Äúweakest link‚Äù because you clip the branch that raises error the *smallest* per leaf removed.
3. **Cross-validate** to pick the best $\alpha$ (equivalently the best index $m$).

   * Common ‚Äúone-standard-error‚Äù rule: choose the smallest tree whose CV error is within 1 œÉ of the minimum.

> **Why this beats C4.5‚Äôs error-based pruning:**
> Cost-complexity treats pruning as an **‚Ñì‚ÇÄ-regularised optimisation** with a *single* knob Œ±, rather than thousands of independent leaf tests. It yields a provable nested sequence and is easy to tune.

---

## 5‚ÄÉPseudocode Snapshot

```text
function CART(S, Features, task):
    # ----- growing -----
    node ‚Üê best_split(S, Features, task)    # Gini or SSE; returns None if stop
    if node is None: return Leaf(prediction(S, task))
    node.left  = CART(S_L, Features, task)
    node.right = CART(S_R, Features, task)
    return node

full_tree = CART(training_data, all_features, task)

# ----- pruning -----
sequence = weakest_link_prune(full_tree)    # produces T_0, T_1, ‚Ä¶, T_M
Œ±*        = choose_alpha_by_CV(sequence)    # k-fold cross-validation
final_tree = tree_in_sequence_closest_to(Œ±*)
```

*Notice:* structure is still ‚Äúsplit ‚Üí recurse,‚Äù but always binary and with a second stage.

---

## 6‚ÄÉComplexity Cheat-Sheet

| Phase              | Time Complexity            | Space Complexity | Notes                                                               |
| ------------------ | -------------------------- | ---------------- | ------------------------------------------------------------------- |
| Grow (per level)   | O(nm)                      | O(nm)            | Same as C4.5; often faster in practice due to Gini‚Äôs simpler math   |
| Prune              | O(n + \|T‚ÇÄ\| √ó log \|T‚ÇÄ\|) | O(\|T‚ÇÄ\|)        | Efficient weakest-link pruning; avoids separate tests for each leaf |
| Predict (1 sample) | Proportional to tree depth | ‚Äî                | Depth ‚âà log(n) in balanced trees ‚áí fast prediction                  |

---

## 7‚ÄÉWhen CART Shines

1. **Mixed tasks** ‚Äì you can solve classification and regression with the *same* engine.
2. **Need for interpretability** ‚Äì post-pruning gives compact models.
3. **Real-time scoring** ‚Äì binary questions are branch-prediction friendly in CPUs.
4. **Bagging / Random Forests** ‚Äì CART is the canonical base learner (fast + binary).

---

## 8‚ÄÉMental Pocket Model

```
ID3          = entropy  + multiway splits + no pruning
C4.5         = ID3      + gain-ratio      + error-prune
CART         = gini/sse + binary splits   + Œ±-prune (+ regression!)
```

Keep this grid in your head; you can instantly recall which tool to reach for and *why*.

---

### üèÅ Quick Self-Checks

1. **Derive Gini reduction** for a tiny 2√ó2 confusion table‚Äîcan you do it without logs?
2. **Manually walk** the weakest-link pruning on a toy tree of 5 leaves‚Äîsee how nested subtrees emerge.
3. **Switch tasks**: on the same numeric dataset, build a classification CART (threshold a target) and a regression CART (continuous target). Observe the metric swap.

Master these and you‚Äôll not only *use* CART‚Äîyou‚Äôll *understand* every line it writes in a log.

---
