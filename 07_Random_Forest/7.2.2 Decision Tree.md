### 1. Decision-Tree Models and the Learning Problem

A **decision tree** is a flow-chart–like structure that recursively partitions the input space.

| Element                     | Role                                                                                                                                    |
| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Root node**               | Contains all training samples and launches the first split.                                                                             |
| **Internal nodes**          | Represent tests on a single feature; each outgoing edge corresponds to a test outcome (e.g., “humidity ≤ 70 %?” **yes/no**).          |
| **Leaves / terminal nodes** | Store a prediction (class label for classification, a real value for regression) and often class-probability estimates or mean targets. |

The **learning objective** is to:

1. Choose, at each node, the feature (and, if numeric, the threshold) that “best” separates the remaining data.
2. Stop splitting when further partitions bring little or negative benefit (to curb over-fitting).
3. Optionally **prune** the fully-grown tree to improve generalisation.

---

### 2. Feature (Attribute) Selection

Selecting the splitting feature is the heart of tree induction. All popular criteria share a common idea: **quantify impurity / disorder** both before and after the split and pick the split that produces the **largest impurity reduction**.

#### 2.1 Entropy and Information Gain

* **Entropy** measures impurity in bits:

  $$
  H(S) \;=\; -\sum_{c=1}^{K} p_c \log_2 p_c,
  $$

  where $p_c$ is the fraction of samples in set $S$ that belong to class $c$; $H\in[0,\log_2K]$.
>[!example]-
> Here’s a concrete mini-example with a binary class (K = 2):
>  
>  * Suppose you have a node $S$ with 6 samples:
>  
>    * 4 are “Yes” ($p_{\text{Yes}} = 4/6 \approx 0.667$)
>    * 2 are “No”  ($p_{\text{No}}  = 2/6 \approx 0.333$)
>  
>  * Plug into the entropy formula:
>  
>    $$
>      H(S)
>      = -\Bigl(0.667\log_2 0.667 \;+\; 0.333\log_2 0.333\Bigr)
>      \;\approx\; 0.918\text{ bits}
>    $$
>  
>  * **Interpretation**
>  
>    * $H=0$ bits when a node is *pure* (e.g.\ all Yes or all No).
>    * $H=1$ bit for a perfectly *balanced* binary split ($p=0.5,0.5$).
>    * Here, $0 < 0.918 < 1$, reflecting that our 4∶2 split is somewhat impure but not maximally so.
>  
>  This shows how entropy quantifies “mixedness” in bits, guiding algorithms (like decision trees) to pick splits that drive entropy toward zero.

* **Information Gain (IG)** of splitting on feature $A$:

  $$
  IG(S, A) \;=\; H(S)\;-\;\sum_{v\in\text{values}(A)} \frac{|S_v|}{|S|}\,H(S_v),
  $$

  where $S_v$ is the subset with feature value $v$.

Intuition → High IG means a large drop in disorder, hence a “good” split.
> [!example]-
> > Here’s a concrete mini‐example with a binary class (K = 2):
>   
>   * **Original node $S$**
>     6 samples: 4 are “Yes” ($p_{\rm Yes}=4/6\approx0.667$), 2 are “No” ($p_{\rm No}=2/6\approx0.333$)
>   
>     $$
>       H(S)
>       = -\bigl(0.667\log_2 0.667 + 0.333\log_2 0.333\bigr)
>       \approx 0.918\text{ bits}
>     $$
>   
>   * **Split on feature $A$** with two values (“True” vs. “False”):
>   
>     * $S_{\rm True}$: 3 “Yes”, 0 “No”
>   
>       $$
>         H(S_{\rm True})
>         = -\bigl(1\!\cdot\!\log_2 1 + 0\!\cdot\!\log_2 0\bigr)
>         = 0\text{ bits}
>       $$
>     * $S_{\rm False}$: 1 “Yes”, 2 “No”
>   
>       $$
>         H(S_{\rm False})
>         = -\bigl(0.333\log_2 0.333 + 0.667\log_2 0.667\bigr)
>         \approx 0.918\text{ bits}
>       $$
>   
>   * **Weighted post‐split entropy**
>   
>     $$
>       \sum_{v\in\{\rm T,F\}}\frac{|S_v|}{|S|}H(S_v)
>       = \frac{3}{6}\cdot0 + \frac{3}{6}\cdot0.918
>       = 0.459\text{ bits}
>     $$
>   
>   * **Information Gain**
>   
>     $$
>       IG(S,A)
>       = H(S)\;-\;0.459
>       = 0.918 - 0.459
>       = 0.459\text{ bits}
>     $$
>   
>   **Intuition →**
>   A “good” split is one that yields a large drop in entropy.
>   Here, splitting on $A$ cuts disorder by about 0.459 bits—half of the original 0.918 bits—so it’s a moderately informative split.
>   

#### 2.2 Information Gain Ratio

A problem: IG is **biased toward features with many distinct values** (e.g., “ID”).
> [!why]-
> **Information Gain (IG) is biased toward features with many distinct values** because those features tend to create very *pure* subsets—each with only one or very few samples—which artificially inflates IG, even if the split is meaningless.
>    
>    ---
>    
>    ### 🔍 **Example: Splitting by "Student ID"**
>    
>    Suppose you’re trying to classify whether students passed an exam (“Yes” or “No”), and you have:
>    
>     | Student ID | Score | Passed |
>     | ---------- | ----- | ------ |
>     | S001       | 85    | Yes    |
>     | S002       | 70    | No     |
>     | S003       | 90    | Yes    |
>     | S004       | 65    | No     |
>    
>    #### If you split on **Student ID**:
>    
>    * Each ID is unique → each subset has exactly 1 sample.
>    * Each subset is **pure** → entropy is 0 for every subset.
>    * So:
>    
>      $$
>      IG = H(S) - \sum \frac{1}{4} \cdot 0 = H(S)
>      $$
>    
>      → **IG is maximized**, even though **“Student ID” has zero predictive value**.
>    
>    #### If you split on **Score (e.g., threshold at 75)**:
>    
>    * Two groups:
>    
>      * Score ≥ 75 → \[S001, S003] → both “Yes”
>      * Score < 75 → \[S002, S004] → both “No”
>    * Still pure, so entropy also drops to 0
>    * But Score is **actually meaningful**
>    
>    ---
>    
>    ### ⚠️ **Key Point**
>    
>    features with many unique values (like IDs, Names, or random hashes) can **perfectly split the data**, but don’t generalize. IG doesn't penalize this, so it gets tricked.
>    
>    ---
>    
>    ### ✅ **Solution: Use Gain Ratio**
>    
>    The **Split Information (SI)** will be high for "Student ID" (since it splits into many tiny groups), so the **Gain Ratio (GR)** will be low—correctly penalizing the useless split.
>    
* **Split Information**:

  $$
  SI(S,A) \;=\; -\sum_{v} \frac{|S_v|}{|S|}\,\log_2\frac{|S_v|}{|S|}.
  $$
* **Gain Ratio (GR)**:

  $$
  GR(S,A)\;=\;\frac{IG(S,A)}{SI(S,A) + \varepsilon},
  $$

  where $\varepsilon$ avoids division by zero.

GR normalises the gain, favouring features that both lower impurity and avoid overly fine partitions.
> [!example]-
>
> 	   | Student ID | Score | Passed |
> 	   | ---------- | ----- | ------ |
> 	   | S001       | 85    | Yes    |
> 	   | S002       | 70    | No     |
> 	   | S003       | 90    | Yes    |
> 	   | S004       | 65    | No     |
>    
>    Let’s say:
>    
>    * Splitting on **Student ID** gives 4 subsets: each with **1 sample**
>    * $|S| = 4$, so for each $S_v$, $\frac{|S_v|}{|S|} = \frac{1}{4}$
>    
>    ---
>    
>    ### 🔹 Step 1: **Compute Split Information**
>    
>    $$
>    SI(S,\text{ID}) = -\sum_{v=1}^4 \frac{1}{4} \log_2 \frac{1}{4}
>    = -4 \cdot \frac{1}{4} \cdot (-2)
>    = 2.0 \text{ bits}
>    $$
>    
>    ---
>    
>    ### 🔹 Step 2: **Compute Information Gain**
>    
>    Since each subset is pure (1 sample per ID), entropy after the split is 0:
>    
>    $$
>    IG(S,\text{ID}) = H(S) - 0 = H(S)
>    $$
>    
>    Let’s compute $H(S)$:
>    
>    * “Passed” distribution: 2 Yes, 2 No → $p = [0.5, 0.5]$
>    
>    $$
>    H(S) = -0.5\log_2 0.5 - 0.5\log_2 0.5 = 1.0 \text{ bit}
>    $$
>    
>    ---
>    
>    ### 🔹 Step 3: **Compute Gain Ratio**
>    
>    $$
>    GR(S, \text{ID}) = \frac{1.0}{2.0 + \varepsilon} \approx 0.5
>    $$
>    
>    ---
>    
>    ### ✅ Final Intuition
>    
>    * **High IG (1.0)** because the split is *perfectly pure*
>    * **But SI = 2.0** → we split into *too many tiny parts*
>    * **GR = 0.5**, a *penalized score* that reflects the feature’s overfitting nature
>    
>    So while “Student ID” gives a high IG, **its low GR warns us it's not a useful feature**.
>    
---

### 3. Generating Decision Trees

#### [[7.2.2.3.1 ID3 Algorithm | 3.1 The ID3 Algorithm]]

1. **Input**: training set $S$, feature list.
2. **If** all examples in $S$ share the same class → make a leaf.
3. **Else** if no features remain → leaf with majority class.
4. **Else**

   * Compute **IG** for every candidate feature.
   * Pick the feature with **max IG** as the test.
   * Recurse on each child subset.

#### 3.2 Limitations of ID3

| Issue                                   | Why it matters                                                 |
| --------------------------------------- | -------------------------------------------------------------- |
| Bias toward multi-valued features       | Can select an ID-like feature that memorises the training set. |
| Handles **only categorical** features   | Numeric variables must be discretised in advance.              |
| No built-in pruning                     | Fully grown trees easily over-fit noisy data.                  |
| Cannot handle missing values gracefully | Usually drops instances or uses ad-hoc fixes.                  |

#### [[7.2.3.3 C4.5 Algorithm| 3.3 The C4.5 Algorithm (Successor to ID3)]]

Key improvements:

| Improvement          | Mechanism                                                                                                                                                       |
| -------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Gain Ratio**       | Replaces bare IG, reducing split bias.                                                                                                                          |
| **Numeric features** | Examines candidate thresholds; selects the one that maximises GR.                                                                                               |
| **Missing values**   | Uses fractional counts / probabilistic splits.                                                                                                                  |
| **Post-pruning**     | Employs *pessimistic error pruning*: estimate leaf error with a continuity-correction; prune if the estimated error of a subtree exceeds that of collapsing it. |
| **Rule extraction**  | Can convert the final tree into an ordered rule set for readability.                                                                                            |

#### 3.4 Drawbacks and Reflections on C4.5

* **Computationally heavier**: sorting numeric features, evaluating many thresholds.
* **Still top-down greedy**: might miss globally optimal trees.
* **Data fragmentation**: deep trees on sparse data can leave very few samples per leaf.
* **Pruning heuristic**: pessimistic error is a rough estimate; cross-validation-based pruning often performs better.

---

### [[7.2.2.4. Pruning Decision Trees | 4. Pruning Decision Trees]]

Pruning fights over-fitting by cutting back parts of a tree that do not improve (or even hurt) predictive accuracy on unseen data.

| Stage                            | Technique                                                         | Strategy                                                                                                                                                              |
| -------------------------------- | ----------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Pre-pruning (early stopping)** | e.g., *max depth*, *min samples per leaf*, chi-square test        | Stop growth **before** the split if improvement is insignificant. Fast but may under-fit.                                                                             |
| **Post-pruning**                 | Reduced-error pruning, cost-complexity pruning, pessimistic error | Grow the **full** tree first, then iteratively replace subtrees with leaves if a validation-set or penalised-error metric improves. Generally yields better accuracy. |

---

### [[7.5 CART Algorithm | 5. CART (Classification and Regression Tree) Algorithm]]

CART, introduced by Breiman et al. (1984), unified classification and regression in a single framework.

#### 5.1 CART Generation

| Property                             | Details                                                                                                                     |
| ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- |
| **Binary splits only**               | Every internal node has exactly two children, even for categorical variables (one subset vs. the rest).                     |
| **Impurity measure**                 | *Gini impurity* for classification:<br>$\displaystyle G(S)=\sum_{c}p_c(1-p_c)$.<br>For regression, uses variance reduction. |
| **Numeric and categorical handling** | Searches thresholds (numeric) or subset partitions (categorical) that maximise impurity reduction.                          |
| **Stopping criteria**                | User-tunable hyper-parameters (min samples, max depth) or impurity gain < ε.                                                |

#### 5.2 CART Pruning – Cost-Complexity Pruning (CCP)

1. Grow a maximal tree $T_{max}$.
2. For each internal node, compute **α-value**: increase in error per leaf removed if that subtree were pruned.
3. Generate a sequence of nested pruned trees $T_1\subset T_2\subset …$ by increasing α.
4. Evaluate these trees with cross-validation; choose the one with minimum estimated risk (or the **1-SE rule** for simplicity).

CCP produces a *regularisation path*: small α → large tree; large α → shallow tree.

#### 5.3 Shortcomings of CART

| Concern                           | Explanation                                                                                               |
| --------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **Deep trees due to binary rule** | Splitting categorical features with many levels may require multiple successive splits.                   |
| **Instability**                   | Small data perturbations can change the chosen splits; ensembles (Bagging, Random Forest) alleviate this. |
| **Search cost**                   | Examining all subset partitions for categorical features is exponential; implementations use heuristics.  |

---

### Putting It All Together – Core Logic Recap

1. **Partitioning principle**: Every decision-tree algorithm greedily chooses the feature/threshold that yields the largest **impurity reduction** on the current node’s data.
2. **Impurity metrics**: Entropy ↔ Information Gain (ID3); Gain Ratio (C4.5); Gini or variance (CART).
3. **Preventing over-fitting**:

   * **Bias correction** (Gain Ratio) combats selection of overly specific features.
   * **Pruning** balances the trade-off between fit and complexity.
4. **Handling data realities**:

   * Numeric features → thresholding (C4.5, CART).
   * Missing values → probabilistic routing (C4.5) or surrogate splits (CART).
   * Computational constraints → binary splits (CART) vs. multi-way (ID3/C4.5).
5. **Limitations & modern practice**: Individual trees are interpretable yet unstable; modern workflows often embed them in **ensembles** (Random Forests, Gradient Boosted Trees) that average/boost away variance while retaining tree-based intuition.

---

#### Suggested Study Path

1. **Hands-on**: Implement a tiny ID3 on toy data to solidify entropy & IG concepts.
2. **Visualise**: Plot decision boundaries vs. depth; watch over-fitting emerge.
3. **Practical toolkit**: Explore scikit-learn’s `DecisionTreeClassifier` (CART) and compare pre- versus post-pruned performance.
4. **Bridge to ensembles**: Observe how Random Forest mitigates instability and how Gradient Boosted Trees focus on residual errors.

---

*With these notes, an undergraduate armed with basic statistics (probabilities, variance) and programming skills should gain a clear, logically-connected grasp of how decision-tree algorithms work, why their design choices matter, and how to apply them effectively.*
