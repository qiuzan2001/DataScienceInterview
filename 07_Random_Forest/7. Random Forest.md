## 7.1. Introduction & Motivation

Random forests sit at the intersection of two powerful ideas in machine learning:

1. **Bagging (Bootstrap Aggregation):**

   * Reduces model variance by training each base learner on a different “view” of the data.
   * Each tree is trained on a bootstrap sample (random sampling with replacement), so no single tree sees the exact same training set.

2. **Random Subspace Method (Feature Sampling):**

   * Further de-correlates the trees by only allowing a random subset of features at each split.
   * Prevents dominant features from appearing in every tree, which would otherwise make trees too similar.

By combining these, a random forest leverages the wisdom of crowds: many slightly different, high-variance trees whose errors largely cancel out when aggregated.

---

## [[7.2. Building the Forest]](Algorithm Steps)

Given a dataset of *n* examples and *p* features:

1. **Repeat** for *b* = 1 to *B* (number of trees):

   * **Bootstrap Sample:** Draw *n* examples with replacement from the original data.
     *Why?* Creates diversity among trees; each tree sees a unique dataset.
   * **Grow Tree:**

     * At each node, randomly select `max_features` out of the *p* total.
     * Choose the best split (e.g. maximize Gini decrease or minimize MSE).
     * Continue until a stopping rule (`max_depth`, `min_samples_leaf`, etc.) is met.
       *Why random features?* Ensures trees don’t all focus on the same strong predictor, boosting ensemble variance reduction.
2. **Aggregate** the *B* fully grown trees without pruning.

Because each tree is both data- and feature-randomized, its errors become largely uncorrelated. When we aggregate, averaging or voting cancels out individual mistakes.

---

## 3. Making Predictions

* **Classification:**

  * Each tree votes for a class.
  * The forest’s prediction is the **majority vote**.
  * Logical link: individual trees may overfit locally, but their consensus smooths out idiosyncrasies.

* **Regression:**

  * Each tree outputs a numeric estimate.
  * The forest’s prediction is the **average** of these estimates.
  * Reasoning: averaging over many noisy estimators yields a more stable, lower-variance result.

---

## [[7.4. Quantifying Feature Importance]]

Random forests provide two complementary measures of how much each feature contributes to predictions:

| Method                              | How It Works                                                                                                                      | Interpretation                                                                              |
| :---------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ |
| **Mean Decrease in Impurity (MDI)** | Sum, over all trees and all splits where feature *j* is used, of the impurity reduction (weighted by node size), then normalized. | Measures how often and how effectively a feature is chosen to split.                        |
| **Permutation Importance**          | On out-of-bag (OOB) data, shuffle feature *j* values and track drop in accuracy (or increase in MSE).                             | Captures true predictive power, including interactions; less biased by feature cardinality. |

* **Key insight:** MDI is fast but can favor high-cardinality features; permutation better reflects real impact yet costs extra computation.

---

## 5. Core Hyperparameters & Their Trade-Offs

| Parameter               | Role                                                                       | Effect on Bias/Variance                                                         |
| :---------------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------------ |
| **n\_estimators**       | Number of trees in the forest                                              | ↑ reduces variance (more trees), ↑ computation & memory                         |
| **max\_depth**          | Maximum depth of each tree                                                 | ↓ depth → ↑ bias, ↓ variance; ↑ depth → ↓ bias, ↑ variance                      |
| **min\_samples\_split** | Min. samples to consider a node for splitting                              | ↑ value → ↑ bias, ↓ variance                                                    |
| **min\_samples\_leaf**  | Min. samples required at a leaf                                            | Guards against tiny leaves; ↑ value → ↑ bias, ↓ variance                        |
| **max\_features**       | Features considered at each split                                          | Lower → more decorrelation, ↑ bias; higher → ↓ bias, ↑ correlation              |
| **bootstrap**           | Use of bootstrap samples (True/False)                                      | False (no bagging) → ↑ variance                                                 |
| **criterion**           | Impurity measure (Gini/entropy for classification; MSE/MAE for regression) | Choice can subtly affect split decisions and speed.                             |
| **class\_weight**       | Class weighting scheme in classification                                   | Balances imbalanced datasets by penalizing misclassification of minority class. |

**Interdependencies to watch:**

* A deeper forest (`max_depth` ↑) may need fewer trees (`n_estimators` ↓) to reach stable error.
* Fewer features per split (`max_features` ↓) can be offset by allowing smaller leaves (`min_samples_leaf` ↓), but risks fragmentation.

---

## 6. Tuning Strategies (Nested in Cross-Validation)

1. **Grid Search**

   * Exhaustive, user-defined grid.
   * Pros: thorough exploration; Cons: exponential growth in combinations.

2. **Random Search**

   * Samples hyperparameter combinations stochastically.
   * Pros: often finds good regions faster; Cons: may miss narrow optima.

3. **Bayesian Optimization** (e.g., Tree-structured Parzen Estimator)

   * Builds a surrogate model of validation performance and sequentially refines search.
   * Pros: efficient use of past evaluations; Cons: more complex to set up.

> **Always** nest hyperparameter search inside cross-validation folds to prevent optimistic bias: split data into *k* folds, tune on (*k–1*) folds, test on the held-out fold, and repeat.

---

## 7. Pros & Cons

| Pros                                                            | Cons                                                        |
| :-------------------------------------------------------------- | :---------------------------------------------------------- |
| High accuracy with minimal tuning                               | Less interpretable than a single decision tree              |
| Robust to overfitting via variance reduction                    | Computationally and memory intensive for large forests      |
| Handles non-linear interactions and mixed data types seamlessly | MDI importance biased toward variables with many categories |
| Built-in OOB error estimate avoids separate validation split    | Cannot extrapolate beyond the range seen in training data   |

---

## 8. Practical Workflow & Best Practices

1. **Defaults to Start:**

   * `n_estimators = 100`, `max_features = "sqrt"` (classification), no depth limit.
2. **Quick Validation:**

   * Use **OOB error** to gauge generalization without a hold-out set.
3. **Error Profiling:**

   * Plot validation error vs. `n_estimators` to identify diminishing returns.
4. **Core Tuning:**

   * Focus on `max_depth`, `min_samples_leaf`, and `max_features` via random or Bayesian search.
5. **Feature Engineering:**

   * Compute **permutation importances** to decide which features to keep, transform, or remove.
6. **Interpretation:**

   * Use partial dependence plots or SHAP values for deeper insights beyond raw importances.

---

## 9. Connecting It All

* **Why variance reduction matters:** Single trees overfit easily; bagging averages out fluctuations.
* **Why de-correlation helps:** If all trees make the same mistake, averaging won’t help—random feature splits ensure diversity.
* **Why feature importance guides engineering:** Identifies predictive signals and redundancy, helping streamline models.
* **Why nested CV prevents over-optimism:** Ensures that tuning decisions don’t leak information from test data, giving realistic performance estimates.

By following this structured approach—understanding the theory, carefully tuning, and validating with OOB or CV—you can harness random forests as a robust, interpretable (via importances) tool for a wide array of predictive tasks.
