Here is a detailed, organized version of your notes in English, matching the original detail level:

---

# Machine Learning - ID3 Algorithm for Decision Trees

## Overview

A **Decision Tree** is a non-parametric supervised learning method that represents decision rules in a tree-like structure. It is used for both **classification** and **regression** tasks. Decision trees are intuitive, easily interpretable, and suitable for various data types. They are especially powerful when used in ensemble models such as Random Forests and Gradient Boosted Trees, with wide applications across industries.

Three core decision tree algorithms are:

* **ID3 (Iterative Dichotomiser 3)**: Uses **Information Gain** to select the best feature for splitting.
* **C4.5**: An improvement over ID3; it uses **Information Gain Ratio** instead of raw Information Gain.
* **CART (Classification and Regression Trees)**: Can handle both classification and regression problems; uses **Gini Index** instead of entropy.

This tutorial focuses on the **ID3 algorithm**.

---

## Dataset Example

To demonstrate ID3, we'll use it to determine whether we should play tennis based on the weather.

| Day | Outlook  | Temperature | Humidity | Wind   | PlayTennis |
| --- | -------- | ----------- | -------- | ------ | ---------- |
| D1  | Sunny    | Hot         | High     | Weak   | No         |
| D2  | Sunny    | Hot         | High     | Strong | No         |
| D3  | Overcast | Hot         | High     | Weak   | Yes        |
| D4  | Rain     | Mild        | High     | Weak   | Yes        |
| D5  | Rain     | Cool        | Normal   | Weak   | Yes        |
| D6  | Rain     | Cool        | Normal   | Strong | No         |
| D7  | Overcast | Cool        | Normal   | Strong | Yes        |
| D8  | Sunny    | Mild        | High     | Weak   | No         |
| D9  | Sunny    | Cool        | Normal   | Weak   | Yes        |
| D10 | Rain     | Mild        | Normal   | Weak   | Yes        |
| D11 | Sunny    | Mild        | Normal   | Strong | Yes        |
| D12 | Overcast | Mild        | High     | Strong | Yes        |
| D13 | Overcast | Hot         | Normal   | Weak   | Yes        |
| D14 | Rain     | Mild        | High     | Strong | No         |

---

## ID3 Algorithm

### Core Idea

ID3 builds the decision tree by selecting the attribute with the **highest Information Gain** at each step.

---

### Step 1: Calculate Entropy of Dataset $S$

Given a dataset $S$ with $n$ classes, the entropy is:

$$
Entropy(S) = -\sum_{i=1}^n p_i \log_2 p_i
$$

Where $p_i = \frac{|S_i|}{|S|}$ is the proportion of class $C_i$ in $S$.

In the dataset, we have 9 “Yes” and 5 “No”:

$$
Entropy(S) = -\left(\frac{9}{14} \log_2 \frac{9}{14} + \frac{5}{14} \log_2 \frac{5}{14}\right) ≈ 0.94
$$

---

### Step 2: Entropy After Splitting on Feature $A$

Assuming feature $A$ has $k$ distinct values and splits $S$ into subsets $\{S_1, ..., S_k\}$:

$$
Entropy_A(S) = \sum_{i=1}^{k} \frac{|S_i|}{|S|} Entropy(S_i)
$$

---

### Step 3: Information Gain

$$
Gain(S, A) = Entropy(S) - Entropy_A(S)
$$

Compute this for all features and select the one with the **highest gain** as the decision node.

---

## Information Gain Calculation

Feature values:

* Outlook: Sunny, Overcast, Rain
* Temperature: Hot, Mild, Cool
* Humidity: High, Normal
* Wind: Weak, Strong

### 1. **Outlook**

Split by Outlook:

#### Sunny:

5 instances → 2 “Yes”, 3 “No”

$$
Entropy(S_{Sunny}) ≈ 0.971
$$

#### Overcast:

4 instances → All “Yes”

$$
Entropy(S_{Overcast}) = 0
$$

#### Rain:

5 instances → 3 “Yes”, 2 “No”

$$
Entropy(S_{Rain}) ≈ 0.971
$$

$$
Entropy_{Outlook}(S) = \frac{5}{14} \cdot 0.971 + \frac{4}{14} \cdot 0 + \frac{5}{14} \cdot 0.971 ≈ 0.693
$$

$$
Gain(S, Outlook) = 0.94 - 0.693 = 0.247
$$

### 2. **Other Features (precomputed)**

* Gain(S, Temperature) ≈ 0.03
* Gain(S, Humidity) ≈ 0.15
* Gain(S, Wind) ≈ 0.05

So, **Outlook** gives the highest gain → root node.

---

## Tree Growth (Split on Outlook)

### Branch 1: **Outlook = Sunny**

Subset:

| Day | Temp | Humidity | Wind   | Play |
| --- | ---- | -------- | ------ | ---- |
| D1  | Hot  | High     | Weak   | No   |
| D2  | Hot  | High     | Strong | No   |
| D8  | Mild | High     | Weak   | No   |
| D9  | Cool | Normal   | Weak   | Yes  |
| D11 | Mild | Normal   | Strong | Yes  |

Subset entropy: 0.971

#### Feature Gains within Subset:

* Gain(Temperature) = 0.571
* Gain(Humidity) = 0.970
* Gain(Wind) = 0.019

→ Split on **Humidity**

### Further Split:

* Humidity = Normal → All “Yes” → entropy = 0
* Humidity = High → All “No” → entropy = 0

→ Pure leaf nodes reached.

---

### Branch 2: **Outlook = Overcast**

All instances “Yes” → Pure leaf node.

---

### Branch 3: **Outlook = Rain**

Subset:

| Day | Temp | Humidity | Wind   | Play |
| --- | ---- | -------- | ------ | ---- |
| D4  | Mild | High     | Weak   | Yes  |
| D5  | Cool | Normal   | Weak   | Yes  |
| D6  | Cool | Normal   | Strong | No   |
| D10 | Mild | Normal   | Weak   | Yes  |
| D14 | Mild | High     | Strong | No   |

Subset entropy: 0.971

#### Feature Gains within Subset:

* Gain(Temperature) ≈ 0.02
* Gain(Humidity) ≈ 0.02
* Gain(Wind) ≈ 0.971

→ Split on **Wind**

### Further Split:

* Wind = Weak → All “Yes” → entropy = 0
* Wind = Strong → All “No” → entropy = 0

→ Pure leaf nodes reached.

---

## Final Decision Tree (Simplified View)

```
          [Outlook]
          /   |   \
     Sunny Overcast Rain
      /              \
  [Humidity]       [Wind]
   /     \         /    \
Normal  High   Weak   Strong
 Yes     No     Yes     No
```

---

Let me know if you'd like a graphical version or Python implementation of this tree!
