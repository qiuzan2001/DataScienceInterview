We‚Äôll walk through each part: entropy calculation, information gain, recursive tree building, and class prediction. This version assumes:

- All attributes are **categorical**.
    
- Data is in tabular form (list of dictionaries).
    
- The target variable is a string key (like `"label"`).
    

---

## ‚úÖ Complete Python Code for ID3 (With Detailed Comments)
### üå≥ The Recursive ID3 Algorithm

```python
def ID3(dataset, attributes, target_attr):
    """Build a decision tree using the ID3 algorithm."""
    labels = [example[target_attr] for example in dataset]

    # Case 1: All examples have the same class ‚Üí make a leaf
    if labels.count(labels[0]) == len(labels):
        return Leaf(labels[0])

    # Case 2: No more attributes to split on ‚Üí return majority class
    if not attributes:
        return Leaf(majority_class(dataset, target_attr))

    # Step 1: Choose attribute with highest information gain
    gains = {attr: info_gain(dataset, attr, target_attr) for attr in attributes}
    best_attr = max(gains, key=gains.get)

    # Step 2: Create internal node
    node = DecisionNode(best_attr)

    # Step 3: Partition dataset and recurse
    attr_values = set(example[best_attr] for example in dataset)

    for value in attr_values:
        subset = [ex for ex in dataset if ex[best_attr] == value]

        if not subset:
            # No data for this value ‚Üí create leaf with majority class of current dataset
            leaf = Leaf(majority_class(dataset, target_attr))
            node.add_branch(value, leaf)
        else:
            # Recurse with remaining attributes
            new_attrs = attributes - {best_attr}
            subtree = ID3(subset, new_attrs, target_attr)
            node.add_branch(value, subtree)

    return node
```

```python
import math
from collections import Counter, defaultdict

class DecisionNode:
    def __init__(self, attribute):
        self.attribute = attribute         # Attribute to split on
        self.branches = {}                 # Dict: attribute value ‚Üí subtree (node or leaf)

    def add_branch(self, value, subtree):
        self.branches[value] = subtree

    def predict(self, example):
        attr_value = example.get(self.attribute)
        subtree = self.branches.get(attr_value)

        if subtree is None:
            # Handle unseen or missing attribute values
            return None
        return subtree.predict(example)

class Leaf:
    def __init__(self, label):
        self.label = label

    def predict(self, example):
        return self.label
```

---

### üî¢ Entropy Calculation

```python
def entropy(dataset, target_attr):
    """Calculate entropy of the dataset with respect to target attribute."""
    total = len(dataset)
    label_counts = Counter(example[target_attr] for example in dataset)

    ent = 0.0
    for count in label_counts.values():
        p = count / total
        ent -= p * math.log2(p)
    return ent
```

---

### üîÅ Information Gain

```python
def info_gain(dataset, attr, target_attr):
    """Compute information gain from splitting on attr."""
    total = len(dataset)
    attr_values = defaultdict(list)

    # Partition data by attribute value
    for example in dataset:
        attr_values[example[attr]].append(example)

    weighted_entropy = 0.0
    for subset in attr_values.values():
        weighted_entropy += (len(subset) / total) * entropy(subset, target_attr)

    return entropy(dataset, target_attr) - weighted_entropy
```

---

### üß† Majority Class Helper

```python
def majority_class(dataset, target_attr):
    """Return the most common class label in the dataset."""
    label_counts = Counter(example[target_attr] for example in dataset)
    return label_counts.most_common(1)[0][0]
```

---

### üß™ Example Usage

```python
dataset = [
    {"Outlook": "Sunny", "Humidity": "High", "Wind": "Weak", "Play": "No"},
    {"Outlook": "Sunny", "Humidity": "High", "Wind": "Strong", "Play": "No"},
    {"Outlook": "Overcast", "Humidity": "High", "Wind": "Weak", "Play": "Yes"},
    {"Outlook": "Rain", "Humidity": "High", "Wind": "Weak", "Play": "Yes"},
    {"Outlook": "Rain", "Humidity": "Normal", "Wind": "Weak", "Play": "Yes"},
]

attributes = {"Outlook", "Humidity", "Wind"}
tree = ID3(dataset, attributes, target_attr="Play")

example = {"Outlook": "Sunny", "Humidity": "Normal", "Wind": "Strong"}
print("Prediction:", tree.predict(example))  # Output might be "No" or "Yes" depending on tree structure
```

---

## üß≠ Summary of Logic in Code

|Part|What It Does|
|---|---|
|`entropy()`|Measures class uncertainty (how ‚Äúmixed‚Äù the labels are)|
|`info_gain()`|Measures how much a split reduces uncertainty|
|`majority_class()`|Picks fallback class label when needed (for leaf nodes or empty branches)|
|`ID3()`|Builds the tree recursively: split on best attribute and build subtrees|
|`DecisionNode` / `Leaf`|Represent internal and terminal tree nodes; both support `predict()` method|
