## 1 Why Decision Trees Ask Questions at All

* **Goal** Predict the class of a new example as quickly (few yes/no questions) and accurately as possible.
* **Idea** Each internal node asks a *question* about one attribute.

  * The “best” first question is the one that makes the follow-up questions easiest—i.e. leaves the child nodes as pure as possible.

---

## 2 ID3 in Depth—The Original Blueprint

### 2.1 Measuring “Purity” with Entropy

$$
H(S)= -\sum_{c} p(c)\log_2 p(c)
$$

| Intuition                                                                      | Translation                                                                         |
| ------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------- |
| *How many bits on average to “say” the class label if you compress perfectly?* | High entropy ⇒ you still have lots to learn; low entropy ⇒ almost sure of the class |

### 2.2 Turning Purity into a Greedy Rule

1. **Expected impurity *after* asking attribute $A$**

   $$
   \overline{H}(S,A)=\sum_{v}\frac{|S_v|}{|S|}\,H(S_v)
   $$
2. **Information Gain** (bits saved by knowing $A$):

   $$
   \text{Gain}(S,A)=H(S)-\overline{H}(S,A)
   $$
3. **Greedy choice** Pick $A$ with *max* Gain; repeat inside each branch.

> **Why greedy works here**
> Total expected bits = bits for root question + bits in the chosen branch.
> Minimising the sum → minimise the first term, *then* recurse (optimal-substructure property).

### 2.3 Handling Continuous Attributes in ID3

* Sort values once at the node.
* Test every mid-point where the class label changes.
* Treat each threshold as a binary attribute for Gain computation.

### 2.4 Key Weak Spots in Plain ID3

| Problem                                  | Symptom                       | Root Cause                              |
| ---------------------------------------- | ----------------------------- | --------------------------------------- |
| Loves attributes with many unique values | “ZIP code” beats everything   | Gain grows just for slicing data thinly |
| Trees can explode                        | Single-example twigs          | No pruning step                         |
| Re-sorting numeric columns every node    | $O(n\log n)$ per attribute    | Naïve “try every split”                 |
| Ignores structure in missing values      | Drop rows or blunt imputation | Can waste data or bias results          |

Keep these in mind—the next algorithm fixes each.

---

## 3 C4.5—ID3 with Street Smarts

### 3.1 Fairer Split Scoring: **Gain Ratio**

1. *Split-Info* (“how fragmented does $A$ make the data on its own?”)

   $$
   \text{SplitInfo}(S,A)= -\!\!\sum_{v}\frac{|S_v|}{|S|}\log_2\frac{|S_v|}{|S|}
   $$
2. *Gain Ratio*

   $$
   \text{GainRatio}(S,A)=\frac{\text{Gain}(S,A)}{\text{SplitInfo}(S,A)}
   $$
3. **Attribute choice rule**

   * Discard attributes with below-average Gain (guards against tiny denominators).
   * Pick highest positive Gain Ratio.

*Effect*: penalises questions that merely *label-spam* without separating classes.

---

### 3.2 Keeping Trees Manageable: **Error-Based Pruning**

1. **Grow full tree** exactly like ID3.
2. **Bottom-up sweep**

   * For a subtree with $N$ weighted cases and $E$ errors, estimate error-rate
     $\hat{p}=\dfrac{E+\frac12}{N+1}$.
   * Replace subtree by a **leaf** if that leaf’s estimated errors ≤ subtree’s—after adding a complexity penalty.
3. **Outcome** Smaller, less overfit trees; easier to read.

---

### 3.3 Turbo-Charging Numeric Splits

* **Sort once** per numeric attribute at the *root* (cost $n\log n$).
* For every descendant node:

  * Maintain running class counts as you sweep the already-sorted list left→right.
  * Evaluate Gain / Gain Ratio only at **label-change boundaries**—linear time in #examples reaching the node.

---

### 3.4 Smarter Missing-Value Handling: **Fractional Instance Weighting**

* When splitting on attribute $A$, an example with $A=?$ is *duplicated fractionally* down every branch.

  * Weight for branch $v$: proportional to frequency of $v$ among known cases at that node.
* All counts (Gain, pruning errors) use these weights, so no data is thrown away and no global guesswork is needed.

---

## 4 C4.5 Algorithm Skeleton

```text
function C45(S, Attr):
    if stop(S): return Leaf(majority(S))
    bestA ← argmax GainRatio(S, Attr)      # numeric branch uses one-pass scan
    node  ← DecisionNode(bestA)
    for each branch b of bestA:
        Sb ← weightedSubset(S, bestA = b)  # weights handle missing values
        if Sb is empty:
            node.addBranch(b, Leaf(majority(S)))
        else:
            node.addBranch(b, C45(Sb, Attr))
    return PostPrune(node)                 # error-based pruning
```

*Notice*: Control flow is still the familiar ID3 loop; only the helper functions changed.

---

## 5 Complexity & Practical Pay-Off

| Aspect                      | ID3                | C4.5 Improvement              | When It Matters                          |
| --------------------------- | ------------------ | ----------------------------- | ---------------------------------------- |
| Numeric split cost per node | $O(n_a\log n_a)$   | $O(n_a)$                      | Datasets with many floats                |
| Attribute bias              | High (multi-value) | Controlled by Gain Ratio      | Categorical data with large vocabularies |
| Tree size                   | Potentially huge   | 25-50 % smaller after pruning | Noisy labels, interpretability           |
| Missing data                | Drop / naive fill  | Fractional routing            | Surveys, sensor gaps                     |

$n_a$ = #-cases at the node.

---

## 6 Mental One-Liner

> **ID3** asks the question that cuts uncertainty fastest.
> **C4.5** still does that—but checks the question isn’t cheating, prunes braggy sub-trees, zooms through numbers, and gracefully handles blanks.

Got it — you're looking for a **more structured, technical transition** that matches the tone of your ID3 → C4.5 upgrade note: precise, educational, and clean. Here’s a revised transition from **C4.5 to CART** that follows that same pattern:

---

## 7 Where C4.5 Shines—and Where [[7.5 CART Algorithm| CART]] Steps In

C4.5 is a robust, production-ready extension of ID3, solving key pain-points like attribute bias, overfitting, and numeric brittleness—all while preserving the original recursive loop. But it still inherits some design choices that, while elegant, are not the only way:

| Trait inherited by C4.5                                                             | Why it’s sometimes a limitation                                      | CART’s alternative                                                          |
| ----------------------------------------------------------------------------------- | -------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| **Multi-way splits by default**<br>Each categorical feature can spawn many branches | Adds complexity; harder to visualize or export to rule-based systems | **Binary splits only**—simpler trees, easier to convert and prune           |
| **Entropy-based scoring**<br>Requires logarithms and fractional bits                | The math is principled but slower, especially on large datasets      | **Gini impurity / MSE**—faster to compute, often just as effective          |
| **Pre-pruning via error estimates**<br>Removes branches while growing               | Makes early pruning decisions with partial information               | **Post-pruning with cost-complexity (`ccp_alpha`)**—evaluates full subtrees |
| **Missing-value handling built-in**                                                 | Elegant but non-standard outside C4.5                                | CART omits this, assuming preprocessing or simpler treatment is used        |

> **Key takeaway:** C4.5 builds smarter trees during growth—but **CART** asks: what if we let the tree grow fully, use faster split criteria, and prune only after seeing the whole picture?
