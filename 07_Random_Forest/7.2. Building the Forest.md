### Notation & Setup

* **Dataset**: $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$, where each $x_i \in \mathbb{R}^p$ and $y_i$ is a label (class or real value).
* **Number of trees**: $B$
* **Features per split**: `max_features` (an integer $m \le p$ or a fraction of $p$)
* **Stopping rules**: parameters like `max_depth`, `min_samples_leaf`, `min_samples_split`

---

## 1. Why Bootstrap?

* **Bootstrap sample**: Random draw **with replacement** of $n$ examples from $\mathcal{D}$.
* **Effect**:

  * Each bootstrap set $\mathcal{D}_b$ omits \~37% of original examples (on average) and duplicates others.
  * Creates diversity: trees trained on different data “views” make different errors.
  * Out-Of-Bag (OOB) samples (those not chosen) can be used for an internal validation estimate.

---
## 2. Growing One Tree

Given a bootstrap sample $\mathcal{D}_b\subset \mathcal{D}$ of size $n$, we recursively build a decision tree $T_b$ that partitions the feature space to minimize impurity. Below is a step-by-step, math-rich walkthrough.

---

### 2.1 Initialization

* **Root node**: contains all examples $\mathcal{D}_b = \{(x_i,y_i)\}_{i=1}^n$.
* **Depth** parameter $d=0$.

We will split nodes until a stopping rule is met, producing leaves that make predictions.

---

### 2.2 Recursive Splitting

At a node with dataset $S\subseteq \mathcal{D}_b$ of size $|S|$, we:

1. **Check stopping rules** (see §2.4).
2. **Otherwise**, choose a split to best reduce impurity.

#### 2.2.1 Feature Subsampling

* Let total features be $p$.
* Randomly select $m = \mathtt{max\_features}$ features without replacement.
* Denote this subset $F \subset \{1,\dots,p\}$, $|F|=m$.

> **Why?** De-correlates trees: if one feature is very strong, random subsets force other trees to use different predictors, increasing ensemble diversity.

#### 2.2.2 Candidate Split Evaluation

For each feature $j\in F$, and for each possible threshold $t$, we consider the binary split:

$$
S_{\text{left}} = \{(x,y)\in S: x_j \le t\}, 
\quad
S_{\text{right}} = S \setminus S_{\text{left}}.
$$

* **Continuous features**: sort the unique values $\{v_1,\dots,v_k\}$ of $x_j$ in $S$, then consider midpoints $\{(v_{\ell}+v_{\ell+1})/2\}$, $\ell=1,\dots,k-1$.
* **Categorical features**: possible splits on subsets of categories (often approximated via ordering by class-mean response).

For each $(j,t)$, compute the **impurity decrease**:

$$
\Delta I(j,t; S)
= I(S) \;-\; \frac{|S_{\text{left}}|}{|S|}\,I(S_{\text{left}})
                 \;-\; \frac{|S_{\text{right}}|}{|S|}\,I(S_{\text{right}}).
$$

We choose $(j^*,t^*)$ maximizing $\Delta I$.

##### Impurity Measures

* **Classification (Gini impurity)**

  $$
    I_{\mathrm{Gini}}(S)
    = 1 - \sum_{k=1}^K \Bigl(\tfrac{|S_k|}{|S|}\Bigr)^2,
  $$

  where $S_k$ is the subset of $S$ with class $k$.

* **Classification (Entropy)**

  $$
    I_{\mathrm{Ent}}(S)
    = -\sum_{k=1}^K \frac{|S_k|}{|S|}\,\log\!\Bigl(\tfrac{|S_k|}{|S|}\Bigr).
  $$

* **Regression (Mean Squared Error)**

  $$
    I_{\mathrm{MSE}}(S)
    = \frac{1}{|S|} \sum_{(x,y)\in S} (y - \bar{y}_S)^2,
    \quad
    \bar{y}_S = \frac{1}{|S|} \sum_{(x,y)\in S} y.
  $$

Each impurity measure quantifies “heterogeneity” of the node; larger decreases $\Delta I$ imply a more effective split.

#### 2.2.3 Applying the Split

* Create two child nodes, assign $S_{\text{left}}$ and $S_{\text{right}}$.
* Recurse on each child with depth $d+1$.

---

### 2.3 Stopping Criteria

We halt splitting at node $S$ if **any** of the following holds:

1. **Depth limit**: current depth $d \ge \mathtt{max\_depth}$.
2. **Minimum samples to split**: $|S| < \mathtt{min\_samples\_split}$.
3. **Minimum leaf size**: splitting would produce a child with $|S_{\text{child}}| < \mathtt{min\_samples\_leaf}$.
4. **Pure node** (classification): all $y$ in $S$ identical.
5. **No split improves impurity**: $\max_{j,t}\Delta I(j,t;S) = 0$.

When stopping, we create a **leaf node**.

---

### 2.4 Leaf Predictions

* **Classification**: predict the majority class

  $$
    \hat y = \arg\max_k |S_k|.
  $$
* **Regression**: predict the mean

  $$
    \hat y = \bar{y}_S.
  $$

These are **optimal** point estimates under 0–1 loss (classification) or squared-error loss (regression).

---

### 2.5 Computational Complexity

* **Per node**, evaluating one feature $j$ takes $O(n_j \log n_j)$ to sort $n_j\approx|S|$ values plus $O(n_j)$ to scan thresholds.
* **Feature subset** of size $m$ ⇒ $O(m \, n_j \log n_j)$.
* **Tree depth** roughly $O(\log n)$ ⇒ total per tree $O\bigl(m\,n\log n \times \log n\bigr)$.
* **Ensemble** of $B$ trees ⇒ $O\bigl(B\,m\,n\,(\log n)^2\bigr)$, but fully parallelizable across trees.

---

### 2.6 Why This Matters

* **Random features** force different trees to explore varied partitions, preventing a single strong predictor from dominating all splits.
* **Impurity-based splitting** directly optimizes local homogeneity, yielding fine-grained decision boundaries.
* **Stopping rules** balance **bias** (too shallow) vs. **variance** (too deep) at the tree level.
* **Bootstrap sampling** ensures each tree is trained on distinct data, further decorrelating errors.

Together, these design choices produce high-variance base learners whose aggregation drives down variance without inflating bias—producing a powerful, robust model.


---

## 3. Pseudocode Summary

```
Input: Data D, trees B, features p, max_features m, stopping rules
Forest = []
for b in 1…B:
    D_b = bootstrap_sample(D)        
    T_b = grow_tree(D_b)
    add T_b to Forest

function grow_tree(S, depth=0):
    if stopping_rule_met(S, depth):
        return leaf_node(prediction(S))
    F = random_subset(features, size=m)
    best_split = argmax_{(j,t) in F×thresholds}(impurity_decrease)
    S_left, S_right = partition(S, best_split)
    left_child = grow_tree(S_left, depth+1)
    right_child = grow_tree(S_right, depth+1)
    return internal_node(best_split, left_child, right_child)
```

---

## 4. Aggregation & Variance Reduction

* **Classification**: new $x$ → each $T_b$ votes → **majority vote**
* **Regression**: new $x$ → each $T_b$ predicts → **mean** of predictions

Because each $T_b$ is an unbiased (high-variance) estimator, averaging or voting across $B$ such trees reduces overall variance by roughly $1/B$ (assuming independence), while bias remains roughly the same.

---

## 5. Practical Considerations

* **Choice of `max_features`**:

  * Classification: often $\sqrt{p}$.
  * Regression: often $p/3$.
* **Computational cost**:

  * Training complexity: $O(B \times n \times m \times \log n)$ per tree (approx.), since each split scans $O(n \times m)$ and trees have depth $O(\log n)$.
  * Embarrassingly parallel: each tree can be grown on a separate CPU core.
* **OOB Error Estimate**:

  * For each sample $x_i$, average predictions from trees where $x_i$ was **not** in the bootstrap.
  * Provides a built-in validation without a hold-out set.

---

### Key Takeaways

* **Bootstrap + feature randomness** = diverse trees → errors largely uncorrelated.
* **Recursive splitting** driven by impurity decrease yields detailed decision boundaries.
* **Stopping rules** control bias–variance tradeoff at the tree level.
* **Aggregation** transforms many high-variance trees into a low-variance, robust ensemble.

This detailed procedure ensures you understand not only the **“how”** of fitting a random forest but also the **“why”** behind each design choice.
