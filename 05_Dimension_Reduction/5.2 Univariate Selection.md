## **When to Use Each Method**

| **Method**            | **Best For**                              | **Avoid When**                                            |
| :-------------------- | :---------------------------------------- | :-------------------------------------------------------- |
| **Variance Filter**   | Quick data quality check                  | Features legitimately constant by design                  |
| **Chi-Squared**       | Categorical features → Categorical target | Small sample sizes or sparse contingency tables           |
| **ANOVA F-test**      | Numeric features → Categorical target     | Non-normal distributions or unequal variances             |
| **Information Value** | Credit scoring, risk modeling             | Outside financial domain or insufficient data for binning |
| **AUC Ranking**       | Binary classification, feature comparison | Multi-class problems or when interactions matter most     |

**💡 Best Practice:** Use multiple methods in sequence—start with variance filter, then apply domain-appropriate statistical tests, and validate with AUC ranking for binary targets.

---
## **1. Variance Filter 🗑️**

### **Core Idea**
Variables with near-zero variance are essentially constants and provide no discriminative power. If a feature has the same value (or nearly the same value) across all observations, it cannot help distinguish between different outcomes.

### **Mathematical Foundation**
**Sample Variance Formula:**
$$\text{Var}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

**For Boolean/Binary Features:**
$$\text{Var}(X) = p(1-p)$$
where $p$ is the proportion of 1s.

> [! Step-by-Step Calculation]-
   > **Example Dataset:**
   > 
   > | ID  | Feature_A | Feature_B | Feature_C | Target |
   > | --- | --------- | --------- | --------- | ------ |
   > | 1   | 5.2       | 1         | 100       | 0      |
   > | 2   | 3.8       | 1         | 100       | 1      |
   > | 3   | 4.1       | 0         | 100       | 0      |
   > | 4   | 6.3       | 1         | 100       | 1      |
   > | 5   | 2.9       | 1         | 100       | 0      |
   > 
   > **Step 1: Calculate variance for each feature**
   > 
   > **Feature_A:**
   > - $\bar{x}_A = \frac{5.2 + 3.8 + 4.1 + 6.3 + 2.9}{5} = 4.46$
   > - $\text{Var}(A) = \frac{1}{4}[(5.2-4.46)^2 + (3.8-4.46)^2 + (4.1-4.46)^2 + (6.3-4.46)^2 + (2.9-4.46)^2]$
   > - $\text{Var}(A) = \frac{1}{4}[0.55 + 0.44 + 0.13 + 3.38 + 2.43] = 1.73$
   > 
   > **Feature_B:**
   > - $p = \frac{4}{5} = 0.8$ (4 ones out of 5 observations)
   > - $\text{Var}(B) = 0.8 \times (1 - 0.8) = 0.16$
   > 
   > **Feature_C:**
   > - All values = 100, so $\text{Var}(C) = 0$
   > 
   > **Step 2: Apply threshold**
   > - Common threshold: $\text{Var} < 0.01$ or $\text{Var} = 0$
   > - **Decision:** Drop Feature_C (variance = 0), keep Feature_A and Feature_B
### **Pros & Cons**
| **Pros**                                    | **Cons/Watch-outs**                                       |
| :------------------------------------------ | :-------------------------------------------------------- |
| **Trivial to implement and lightning-fast** | **Only flags the most obvious duds**                      |
| No hyperparameters to tune                  | **Completely ignores relationship with target**           |
| Works with any data type                    | May remove legitimately constant features in some domains |
| Catches data quality issues early           | Threshold selection can be arbitrary                      |

---

## **2. Chi-Squared (χ²) Test**

### **Core Idea**
Tests whether a categorical feature is independent of a categorical target. If they're independent, the feature provides no information about the target. The test compares observed frequencies to expected frequencies under independence.

### **Mathematical Foundation**
**Chi-Square Statistic:**
$$\chi^2 = \sum_{i,j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

Where:
- $O_{ij}$ = Observed frequency in cell (i,j)
- $E_{ij}$ = Expected frequency under independence = $\frac{\text{Row}_i \times \text{Col}_j}{n}$

> [! Step-by-Step Calculation]-
   > 
   > **Example: Feature = "Color", Target = "Purchased"**
   > 
   > **Step 1: Create contingency table**
   > 
   > | Color  | Purchased=0 | Purchased=1 | Row Total |
   > |--------|-------------|-------------|-----------|
   > | Red    | 30          | 10          | 40        |
   > | Blue   | 20          | 20          | 40        |
   > | Green  | 15          | 25          | 40        |
   > | **Col Total** | **65** | **55** | **120** |
   > 
   > **Step 2: Calculate expected frequencies**
   > - $E_{Red,0} = \frac{40 \times 65}{120} = 21.67$
   > - $E_{Red,1} = \frac{40 \times 55}{120} = 18.33$
   > - $E_{Blue,0} = \frac{40 \times 65}{120} = 21.67$
   > - $E_{Blue,1} = \frac{40 \times 55}{120} = 18.33$
   > - $E_{Green,0} = \frac{40 \times 65}{120} = 21.67$
   > - $E_{Green,1} = \frac{40 \times 55}{120} = 18.33$
   > 
   > **Step 3: Calculate χ² statistic**
   > 
   > $$\chi^2 = \frac{(30-21.67)^2}{21.67} + \frac{(10-18.33)^2}{18.33} + \frac{(20-21.67)^2}{21.67} + \frac{(20-18.33)^2}{18.33} + \frac{(15-21.67)^2}{21.67} + \frac{(25-18.33)^2}{18.33}$$
   > 
   > $$\chi^2 = 3.20 + 3.78 + 0.13 + 0.15 + 2.05 + 2.42 = 11.73$$
   > 
   > **Step 4: Determine significance**
   > - Degrees of freedom: $(rows - 1) \times (cols - 1) = (3 - 1) \times (2 - 1) = 2$
   > - Critical value at $\alpha = 0.05$: $\chi^2_{0.05,2} = 5.99$
   > - Since $11.73 > 5.99$, the feature is **significantly related** to the target
   
### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Cheap to compute** | **Requires sufficient sample size in each category** |
| **Captures non-linear relationships in counts** | Rule of thumb: Expected frequency ≥ 5 in each cell |
| Works naturally with categorical data | **Misses feature interactions** |
| Provides clear statistical significance | Only works for categorical features and targets |
| No assumptions about distributions | Sensitive to sample size (large n → everything significant) |

---

## **3. ANOVA F-test**

### **Core Idea**
For numeric features and categorical targets, ANOVA tests whether the means of the feature differ significantly across target groups. The F-statistic compares between-group variance to within-group variance.

### **Mathematical Foundation**
**F-Statistic:**
$$F = \frac{\text{MSB}}{\text{MSW}} = \frac{\text{SSB}/(k-1)}{\text{SSW}/(n-k)}$$

Where:
- **SSB** = Sum of Squares Between groups = $\sum_{i=1}^{k} n_i(\bar{x}_i - \bar{x})^2$
- **SSW** = Sum of Squares Within groups = $\sum_{i=1}^{k} \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2$
- **MSB** = Mean Square Between, **MSW** = Mean Square Within
- $k$ = number of groups, $n$ = total sample size

> [! Step-by-Step Calculation]-
   > 
   > **Example: Feature = "Income", Target = "Risk Level" (Low / Medium / High)**
   > 
   > **Step 1: Organize data by groups**
   > 
   > | Risk Level | Income Values             | Group Mean            | Group Size     |
   > |------------|---------------------------|------------------------|----------------|
   > | Low        | [45, 50, 48, 52, 46]      | $\bar{x}_1 = 48.2$     | $n_1 = 5$      |
   > | Medium     | [55, 60, 58, 62, 59]      | $\bar{x}_2 = 58.8$     | $n_2 = 5$      |
   > | High       | [70, 75, 72, 78, 73]      | $\bar{x}_3 = 73.6$     | $n_3 = 5$      |
   > 
   > **Step 2: Calculate overall mean**
   > 
   > $$\bar{x} = \frac{5 \times 48.2 + 5 \times 58.8 + 5 \times 73.6}{15} = \frac{904}{15} = 60.27$$
   > 
   > **Step 3: Calculate Sum of Squares Between (SSB)**
   > 
   > $$\text{SSB} = 5(48.2 - 60.27)^2 + 5(58.8 - 60.27)^2 + 5(73.6 - 60.27)^2$$
   > 
   > $$\text{SSB} = 5(145.69) + 5(2.16) + 5(177.69) = 728.45 + 10.80 + 888.45 = 1627.70$$
   > 
   > **Step 4: Calculate Sum of Squares Within (SSW)**
   > 
   > - For Low Risk: $\sum (x_i - 48.2)^2 = (45 - 48.2)^2 + (50 - 48.2)^2 + \dots = 28.8$
   > - For Medium Risk: $\sum (x_i - 58.8)^2 = 28.8$
   > - For High Risk: $\sum (x_i - 73.6)^2 = 86.8$
   > 
   > $$\text{SSW} = 28.8 + 28.8 + 86.8 = 144.4$$
   > 
   > **Step 5: Calculate F-statistic**
   > 
   > $$F = \frac{\text{SSB} / (k - 1)}{\text{SSW} / (n - k)} = \frac{1627.70 / 2}{144.4 / 12} = \frac{813.85}{12.03} = 67.67$$
   > 
   > **Step 6: Determine significance**
   > 
   > - Degrees of freedom: $df_1 = 2$, $df_2 = 12$
   > - Critical value at $\alpha = 0.05$: $F_{0.05,2,12} = 3.89$
   > - Since $67.67 > 3.89$, the feature **significantly differs** across groups
   

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Classic statistical test that's efficient for many features** | **Assumes features are normally distributed with equal variances** |
| Familiar to most analysts | **Only captures linear relationships** |
| Provides clear significance testing | Sensitive to outliers |
| Works well with balanced groups | May miss non-linear patterns |
| Scales well to high dimensions | Requires categorical target variable |

---

## **4. Information Value (IV) / Weight of Evidence (WoE)**

### **Core Idea**
Popular in credit scoring, IV measures how well a feature separates "good" vs "bad" outcomes. WoE quantifies the strength of relationship for each bin, while IV aggregates this across all bins.

### **Mathematical Foundation**
**Weight of Evidence for bin i:**
$$\text{WoE}_i = \ln\left(\frac{\text{Distribution of Goods}_i}{\text{Distribution of Bads}_i}\right) = \ln\left(\frac{\text{Good}_i/\text{Total Good}}{\text{Bad}_i/\text{Total Bad}}\right)$$

**Information Value:**
$$\text{IV} = \sum_{i=1}^{n} (\text{Distribution of Goods}_i - \text{Distribution of Bads}_i) \times \text{WoE}_i$$

> [! Step-by-Step Calculation]-
> 
> **Example: Feature = "Age", Target = "Default" (0 = Good, 1 = Bad)**
> 
> **Step 1: Create bins and count goods/bads**
> 
> | Age Bin | Good Count | Bad Count | Total Count |
> |---------|------------|-----------|-------------|
> | 18–25   | 100        | 50        | 150         |
> | 26–35   | 200        | 80        | 280         |
> | 36–45   | 150        | 30        | 180         |
> | 46+     | 50         | 40        | 90          |
> | **Total** | **500**    | **200**   | **700**     |
> 
> **Step 2: Calculate distributions**
> 
> | Age Bin | Good Dist     | Bad Dist      | WoE Calculation               |
> |---------|---------------|---------------|-------------------------------|
> | 18–25   | $100/500 = 0.20$ | $50/200 = 0.25$  | $\ln(0.20 / 0.25) = -0.223$    |
> | 26–35   | $200/500 = 0.40$ | $80/200 = 0.40$  | $\ln(0.40 / 0.40) = 0.000$     |
> | 36–45   | $150/500 = 0.30$ | $30/200 = 0.15$  | $\ln(0.30 / 0.15) = 0.693$     |
> | 46+     | $50/500 = 0.10$  | $40/200 = 0.20$  | $\ln(0.10 / 0.20) = -0.693$    |
> 
> **Step 3: Calculate Information Value**
> 
> $$\text{IV} = (0.20 - 0.25)(-0.223) + (0.40 - 0.40)(0.000) + (0.30 - 0.15)(0.693) + (0.10 - 0.20)(-0.693)$$
> 
> $$\text{IV} = 0.011 + 0.000 + 0.104 + 0.069 = 0.184$$
> 
> **Step 4: Interpret IV strength**
> 
> | IV Range | Predictive Power |
> |----------|------------------|
> | < 0.02   | Not useful       |
> | 0.02–0.1 | Weak             |
> | 0.1–0.3  | Medium           |
> | 0.3–0.5  | Strong           |
> | > 0.5    | Suspicious       |
> 
> **✅ Result:** IV = 0.184 → **Medium predictive power**


### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Widely used and trusted in finance** | **Requires careful binning** |
| **Binning creates monotonic trends that are easy to explain** | **The standard thresholds (e.g., IV > 0.1) are domain-specific** |
| Works with both continuous and categorical features | Binning choices can significantly affect results |
| Provides interpretable business insights | May not generalize outside financial services |
| Handles missing values naturally through binning | Requires sufficient observations per bin |

---

## **5. AUC/AUROC Ranking**

### **Core Idea**
Treat each feature as a single-variable classifier and measure its ability to discriminate between classes using the Area Under the ROC Curve. Higher AUC = better discriminative power.

### **Mathematical Foundation**
**AUC Interpretation:**
- AUC = 0.5: Random performance (no predictive power)
- AUC = 1.0: Perfect discrimination
- AUC > 0.7: Generally considered good

**Mann-Whitney U Formula:**
$$\text{AUC} = \frac{\sum_{i \in \text{positive}} \sum_{j \in \text{negative}} \mathbf{1}[s_i > s_j]}{|\text{positive}| \times |\text{negative}|}$$

Where $s_i$ is the score (feature value) for sample $i$.

> [! Step-by-Step Calculation]-
> 
> **Example: Feature = "Credit Score", Target = "Default"**
> 
> **Step 1: Organize data**
> 
> | ID | Credit Score | Default | 
> |----|--------------|---------|
> | 1  | 750          | 0       |
> | 2  | 720          | 0       |
> | 3  | 680          | 1       |
> | 4  | 650          | 0       |
> | 5  | 600          | 1       |
> | 6  | 580          | 1       |
> 
> **Step 2: Identify positive and negative classes**
> - **Non-defaults (0):** Credit scores = [750, 720, 650]  
> - **Defaults (1):** Credit scores = [680, 600, 580]
> 
> **Step 3: Count concordant pairs using Mann–Whitney U**
> 
> **Non-default 750 vs defaults:**
> - 750 > 680 ✓ (1 point)
> - 750 > 600 ✓ (1 point)
> - 750 > 580 ✓ (1 point)
> - **Subtotal: 3 points**
> 
> **Non-default 720 vs defaults:**
> - 720 > 680 ✓ (1 point)
> - 720 > 600 ✓ (1 point)
> - 720 > 580 ✓ (1 point)
> - **Subtotal: 3 points**
> 
> **Non-default 650 vs defaults:**
> - 650 < 680 ✗ (0 points)
> - 650 > 600 ✓ (1 point)
> - 650 > 580 ✓ (1 point)
> - **Subtotal: 2 points**
> 
> **Step 4: Calculate AUC**
> 
> $$\text{AUC} = \frac{\text{Total concordant pairs}}{\text{Total possible pairs}} = \frac{3 + 3 + 2}{3 \times 3} = \frac{8}{9} = 0.889$$
> 
> **Step 5: Interpret result**
> - AUC = 0.889 → very good discriminative power
> - Credit score is a strong predictor of default risk
> - ✅ This feature should be **kept** in the model
> 
> ---
> 
> [! Alternative: Trapezoidal Rule Method]-
> 
> **Step 1: Sort by feature value and calculate TPR/FPR at each threshold**
> 
> | Threshold | TP | FP | FN | TN | TPR  | FPR  |
> |-----------|----|----|----|----|------|------|
> | > 750     | 0  | 0  | 3  | 3  | 0.00 | 0.00 |
> | > 720     | 0  | 1  | 3  | 2  | 0.00 | 0.33 |
> | > 680     | 0  | 2  | 3  | 1  | 0.00 | 0.67 |
> | > 650     | 1  | 2  | 2  | 1  | 0.33 | 0.67 |
> | > 600     | 1  | 3  | 2  | 0  | 0.33 | 1.00 |
> | > 580     | 2  | 3  | 1  | 0  | 0.67 | 1.00 |
> | > 0       | 3  | 3  | 0  | 0  | 1.00 | 1.00 |
> 
> **Step 2: Apply trapezoidal rule**
> 
> $$\text{AUC} = \sum_{i=1}^{n-1} \frac{1}{2}(TPR_i + TPR_{i+1}) \times (FPR_{i+1} - FPR_i)$$
> 
> ➤ This also gives: **AUC ≈ 0.889**

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Intuitive and scale-independent** | **Can't see the forest for the trees—misses features that are only powerful when combined** |
| **Direct measure of predictive power** | Only works for binary classification |
| Works with any monotonic transformation | **Doesn't capture feature interactions** |
| Robust to class imbalance | May favor features with extreme values |
| **Scale-free: works across different units** | Single-variable view may miss complementary features |
| Easy to rank and compare features | Requires careful handling of ties in rankings |
