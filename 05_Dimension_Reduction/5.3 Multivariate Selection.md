
## **When to Use Each Method**

| **Method** | **Best For** | **Avoid When** |
|:-----------|:-------------|:---------------|
| **Correlation/VIF Filter** | Quick multicollinearity detection, linear relationships | Nonlinear dependencies, small feature sets |
| **Mutual Information** | Nonlinear relationships, any dependency type | Small sample sizes, high-dimensional spaces |
| **Recursive Feature Elimination** | Model-specific optimization, capturing interactions | Large feature sets (p > 100), time constraints |
| **Stepwise Selection** | Finding small optimal subsets, interpretable models | High-dimensional data, computational limitations |

**💡 Best Practice:** Start with correlation/VIF for linear redundancy, then apply MI or RFE for complex interactions, validating with cross-validation to avoid overfitting.

---

## **[[3.4.2 Variance Inflation Factor (VIF) | 1. Correlation/VIF Filter]]**

### **Core Idea**
Remove features that are highly correlated with each other to eliminate redundancy. VIF (Variance Inflation Factor) measures how much a feature can be predicted from other features - high VIF indicates multicollinearity problems.

### **Mathematical Foundation**

**Pearson Correlation Coefficient:**
$$r_{XY} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

**Variance Inflation Factor:**
$$\text{VIF}_j = \frac{1}{1 - R_j^2}$$

Where $R_j^2$ is the coefficient of determination when regressing feature $j$ on all other features.

> [! Step-by-Step Calculation]-
> 
> **Example: Feature correlation analysis**
> 
> **Step 1: Calculate correlation matrix**
> 
> Dataset:
> | ID | Age | Experience | Income | Education |
> |----|-----|------------|--------|-----------|
> | 1  | 25  | 2          | 35000  | 12        |
> | 2  | 30  | 8          | 45000  | 16        |
> | 3  | 35  | 12         | 55000  | 14        |
> | 4  | 40  | 15         | 65000  | 18        |
> | 5  | 45  | 20         | 75000  | 16        |
> 
> **Age vs Experience correlation:**
> - $\bar{x}_{age} = 35$, $\bar{x}_{exp} = 11.4$
> - $\sum(x_i - \bar{x})(y_i - \bar{y}) = (25-35)(2-11.4) + ... = 370$
> - $\sum(x_i - \bar{x})^2 = (25-35)^2 + ... = 250$
> - $\sum(y_i - \bar{y})^2 = (2-11.4)^2 + ... = 324.8$
> - $r_{age,exp} = \frac{370}{\sqrt{250 \times 324.8}} = \frac{370}{285.1} = 0.898$
> 
> **Step 2: Create correlation matrix**
> 
> |            | Age   | Experience | Income | Education |
> |------------|-------|------------|--------|-----------|
> | **Age**    | 1.000 | **0.898**  | 0.856  | 0.234     |
> | **Experience** | **0.898** | 1.000 | 0.945  | 0.345     |
> | **Income** | 0.856 | 0.945      | 1.000  | 0.456     |
> | **Education** | 0.234 | 0.345   | 0.456  | 1.000     |
> 
> **Step 3: Apply correlation threshold**
> - Threshold: $|r| > 0.8$
> - High correlations: Age-Experience (0.898), Age-Income (0.856), Experience-Income (0.945)
> - **Decision:** Remove either Age or Experience (highest correlation)
> 
> **Step 4: Calculate VIF for remaining features**
> 
> **VIF for Age (regress Age on Experience, Income, Education):**
> - Run regression: $Age = \beta_0 + \beta_1 \times Experience + \beta_2 \times Income + \beta_3 \times Education$
> - Suppose $R^2 = 0.85$
> - $\text{VIF}_{Age} = \frac{1}{1-0.85} = 6.67$
> 
> **VIF for Experience:**
> - Run regression: $Experience = \beta_0 + \beta_1 \times Age + \beta_2 \times Income + \beta_3 \times Education$
> - Suppose $R^2 = 0.90$
> - $\text{VIF}_{Experience} = \frac{1}{1-0.90} = 10.0$
> 
> **Step 5: Apply VIF threshold**
> - Common threshold: VIF > 10
> - **Decision:** Remove Experience (VIF = 10.0 > threshold)

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Simple, intuitive, and quickly spots linear redundancy** | **Only catches linear relationships** |
| Fast computation for moderate-sized datasets | **Threshold is chosen manually** |
| Provides clear interpretation of multicollinearity | Misses nonlinear dependencies |
| Well-established thresholds (r > 0.8, VIF > 10) | Pairwise approach may miss complex interactions |
| Helps with model stability and interpretation | May remove important features due to correlation |

---

## **2. Mutual Information (MI)**

### **Core Idea**
Mutual Information measures the amount of information one variable contains about another. A greedy approach selects features with high MI to target but low MI to already-selected features, capturing any type of dependency (linear or nonlinear).

### **Mathematical Foundation**

**Mutual Information:**
$$I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right)$$

**For continuous variables (using kernel density estimation):**
$$I(X;Y) = \int \int p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right) dx dy$$

**Normalization (optional):**
$$\text{NMI}(X;Y) = \frac{I(X;Y)}{\sqrt{H(X)H(Y)}}$$

Where $H(X) = -\sum p(x) \log p(x)$ is the entropy.

> [! Step-by-Step Calculation]-
> 
> **Example: MI-based feature selection**
> 
> **Step 1: Calculate MI between each feature and target**
> 
> Dataset:
> | ID | Feature_A | Feature_B | Feature_C | Target |
> |----|-----------|-----------|-----------|---------|
> | 1  | 1         | 3         | 5         | 0       |
> | 2  | 2         | 4         | 3         | 1       |
> | 3  | 3         | 2         | 4         | 0       |
> | 4  | 4         | 1         | 2         | 1       |
> | 5  | 5         | 5         | 1         | 1       |
> 
> **For Feature_A vs Target:**
> - Discretize Feature_A into bins: {1-2: "Low", 3-4: "Med", 5: "High"}
> - Create joint probability table:
> 
> | Feature_A | Target=0 | Target=1 | P(Feature_A) |
> |-----------|----------|----------|--------------|
> | Low       | 1/5      | 1/5      | 2/5          |
> | Med       | 1/5      | 1/5      | 2/5          |
> | High      | 0        | 1/5      | 1/5          |
> | P(Target) | 2/5      | 3/5      | 1            |
> 
> **Calculate MI:**
> $$I(A;T) = \sum_{a,t} p(a,t) \log\left(\frac{p(a,t)}{p(a)p(t)}\right)$$
> 
> $$= \frac{1}{5}\log\left(\frac{1/5}{(2/5)(2/5)}\right) + \frac{1}{5}\log\left(\frac{1/5}{(2/5)(3/5)}\right) + ... $$
> 
> $$= \frac{1}{5}\log(1.25) + \frac{1}{5}\log(0.83) + ... \approx 0.13$$
> 
> **Step 2: Calculate MI for all features**
> - $I(A;Target) = 0.13$
> - $I(B;Target) = 0.08$  
> - $I(C;Target) = 0.15$
> 
> **Step 3: Select feature with highest MI**
> - **Selected:** Feature_C (MI = 0.15)
> 
> **Step 4: Calculate MI between remaining features and selected features**
> - $I(A;C) = 0.05$
> - $I(B;C) = 0.12$
> 
> **Step 5: Greedy selection criterion**
> For each remaining feature, calculate: $\text{Score} = I(X;Target) - \alpha \times I(X;Selected)$
> - $\text{Score}_A = 0.13 - 0.5 \times 0.05 = 0.105$
> - $\text{Score}_B = 0.08 - 0.5 \times 0.12 = 0.02$
> 
> **Step 6: Select next feature**
> - **Selected:** Feature_A (higher score)
> - **Final selection:** {Feature_C, Feature_A}

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Detects any kind of dependency (non-linear included)** | **Can be computationally intensive** |
| Model-agnostic approach | **Sensitive to small sample sizes** |
| Captures feature interactions naturally | Requires discretization for continuous variables |
| Provides interpretable information content | MI estimation can be noisy |
| Works with mixed data types | Parameter tuning needed (α, bins) |

---

## **3. Recursive Feature Elimination (RFE)**

### **Core Idea**
The "knockout tournament" method: Train a model, identify the least important feature, remove it, and repeat until reaching the desired number of features. This approach is model-centric and great at capturing interactions.

### **Mathematical Foundation**

**RFE Algorithm:**
1. Train model on all features: $f(X_1, X_2, ..., X_p) \rightarrow Y$
2. Rank features by importance: $w_1, w_2, ..., w_p$
3. Remove feature with lowest importance: $w_{min}$
4. Repeat until $k$ features remain

**Feature Importance Examples:**
- **Linear models:** $|w_j|$ (absolute coefficient)
- **Tree models:** Split importance, gain importance
- **General:** Permutation importance

> [! Step-by-Step Calculation]-
> 
> **Example: RFE with Linear Regression**
> 
> **Step 1: Train initial model**
> 
> Dataset:
> | ID | X1  | X2  | X3  | X4  | Y   |
> |----|-----|-----|-----|-----|-----|
> | 1  | 2   | 4   | 1   | 3   | 10  |
> | 2  | 3   | 2   | 5   | 1   | 8   |
> | 3  | 1   | 6   | 2   | 4   | 12  |
> | 4  | 4   | 3   | 3   | 2   | 9   |
> | 5  | 5   | 1   | 4   | 5   | 11  |
> 
> **Initial model:** $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4$
> 
> Using least squares: $\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T Y$
> 
> **Results:**
> - $\hat{\beta}_1 = 0.8$ (importance = 0.8)
> - $\hat{\beta}_2 = 1.2$ (importance = 1.2)
> - $\hat{\beta}_3 = 0.3$ (importance = 0.3) ← **Lowest**
> - $\hat{\beta}_4 = 0.9$ (importance = 0.9)
> 
> **Step 2: Remove least important feature**
> - **Remove:** X3 (lowest importance = 0.3)
> - **Remaining:** {X1, X2, X4}
> 
> **Step 3: Retrain model**
> 
> **New model:** $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_4 X_4$
> 
> **Results:**
> - $\hat{\beta}_1 = 0.7$ (importance = 0.7) ← **Lowest**
> - $\hat{\beta}_2 = 1.3$ (importance = 1.3)
> - $\hat{\beta}_4 = 1.0$ (importance = 1.0)
> 
> **Step 4: Remove next least important**
> - **Remove:** X1 (lowest importance = 0.7)
> - **Remaining:** {X2, X4}
> 
> **Step 5: Retrain model**
> 
> **New model:** $Y = \beta_0 + \beta_2 X_2 + \beta_4 X_4$
> 
> **Results:**
> - $\hat{\beta}_2 = 1.4$ (importance = 1.4)
> - $\hat{\beta}_4 = 1.1$ (importance = 1.1) ← **Lowest**
> 
> **Step 6: Final selection**
> 
> **If target is 2 features:** Stop here with {X2, X4}
> **If target is 1 feature:** Remove X4, keep only X2
> 
> **RFE Ranking (most to least important):**
> 1. X2 (survived longest)
> 2. X4 (survived second longest)
> 3. X1 (eliminated in round 2)
> 4. X3 (eliminated in round 1)
> 
> **Cross-validation enhancement:**
> - Repeat RFE process on different CV folds
> - Select features that consistently rank high across folds
> - Use average ranking for final selection

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Model-centric: lets your final model architecture have a say** | **Brute-force approach that can be very slow (O(p²))** |
| **Great at capturing interactions** | **Risks overfitting the selection process** |
| Works with any model that provides feature importance | Computationally expensive for large p |
| Provides clear ranking of all features | May be unstable with small sample sizes |
| Can be enhanced with cross-validation | Requires model retraining at each step |

---

## **4. Stepwise Selection**

### **Core Idea**
A "greedy" search strategy that starts with an empty model (forward) or full model (backward) and iteratively adds or removes single features based on cross-validated performance improvement. Can find small, powerful feature subsets but may get stuck in local optima.

### **Mathematical Foundation**

**Forward Selection Algorithm:**
1. Start with empty set: $S = \emptyset$
2. For each feature $x_j \notin S$, evaluate: $\text{Score}(S \cup \{x_j\})$
3. Add feature that maximizes score: $S = S \cup \{x_{best}\}$
4. Repeat until no improvement or desired size reached

**Backward Elimination Algorithm:**
1. Start with full set: $S = \{x_1, x_2, ..., x_p\}$
2. For each feature $x_j \in S$, evaluate: $\text{Score}(S \setminus \{x_j\})$
3. Remove feature that maximizes score: $S = S \setminus \{x_{worst}\}$
4. Repeat until no improvement or desired size reached

**Scoring Functions:**
- **AIC:** $\text{AIC} = -2\log(L) + 2k$
- **BIC:** $\text{BIC} = -2\log(L) + k\log(n)$
- **Cross-validation:** $\text{CV-Score} = \frac{1}{K}\sum_{i=1}^K \text{Performance}(\text{fold}_i)$

> [! Step-by-Step Calculation]-
> 
> **Example: Forward Selection with Cross-Validation**
> 
> **Step 1: Initialize**
> - Available features: {Age, Income, Education, Experience}
> - Selected features: $S = \emptyset$
> - Target: Binary classification (use AUC as score)
> 
> **Step 2: Evaluate each feature individually**
> 
> Train models using 5-fold CV:
> - Model with Age: $\text{CV-AUC} = 0.72$
> - Model with Income: $\text{CV-AUC} = 0.78$ ← **Best**
> - Model with Education: $\text{CV-AUC} = 0.65$
> - Model with Experience: $\text{CV-AUC} = 0.70$
> 
> **Step 3: Add best feature**
> - $S = \{Income\}$
> - **Baseline CV-AUC = 0.78**
> 
> **Step 4: Evaluate adding second feature**
> 
> Train models using 5-fold CV:
> - Model with {Income, Age}: $\text{CV-AUC} = 0.82$ ← **Best**
> - Model with {Income, Education}: $\text{CV-AUC} = 0.79$
> - Model with {Income, Experience}: $\text{CV-AUC} = 0.80$
> 
> **Step 5: Add second best feature**
> - $S = \{Income, Age\}$
> - **New CV-AUC = 0.82** (improvement = 0.04)
> 
> **Step 6: Evaluate adding third feature**
> 
> Train models using 5-fold CV:
> - Model with {Income, Age, Education}: $\text{CV-AUC} = 0.83$
> - Model with {Income, Age, Experience}: $\text{CV-AUC} = 0.84$ ← **Best**
> 
> **Step 7: Add third feature**
> - $S = \{Income, Age, Experience\}$
> - **New CV-AUC = 0.84** (improvement = 0.02)
> 
> **Step 8: Evaluate adding fourth feature**
> 
> Train model using 5-fold CV:
> - Model with {Income, Age, Experience, Education}: $\text{CV-AUC} = 0.83$
> 
> **Step 9: Stop (no improvement)**
> - Adding Education decreases performance
> - **Final selection:** {Income, Age, Experience}
> - **Final CV-AUC:** 0.84
> 

> [! Backward Elimination Example]-
> 
> **Step 1: Start with all features**
> - $S = \{Age, Income, Education, Experience\}$
> - **Baseline CV-AUC = 0.83**
> 
> **Step 2: Evaluate removing each feature**
> 
> Train models using 5-fold CV:
> - Remove Age: $\text{CV-AUC} = 0.79$
> - Remove Income: $\text{CV-AUC} = 0.71$
> - Remove Education: $\text{CV-AUC} = 0.84$ ← **Best** (improvement!)
> - Remove Experience: $\text{CV-AUC} = 0.80$
> 
> **Step 3: Remove Education**
> - $S = \{Age, Income, Experience\}$
> - **New CV-AUC = 0.84** (improvement = 0.01)
> 
> **Step 4: Evaluate removing each remaining feature**
> 
> Train models using 5-fold CV:
> - Remove Age: $\text{CV-AUC} = 0.78$
> - Remove Income: $\text{CV-AUC} = 0.70$
> - Remove Experience: $\text{CV-AUC} = 0.82$
> 
> **Step 5: Stop (no improvement)**
> - No single removal improves performance
> - **Final selection:** {Age, Income, Experience}
> - **Final CV-AUC:** 0.84
> 
> **Note:** Both forward and backward reached the same solution!

### **Pros & Cons**
| **Pros** | **Cons/Watch-outs** |
|:---------|:-------------------|
| **Can sometimes find small, powerful feature subsets** | **Can get stuck in local optima, missing better combinations** |
| Incorporates cross-validation naturally | **Slow for many features** |
| Provides clear selection pathway | Greedy approach may miss global optimum |
| Works with any performance metric | Requires multiple model retraining |
| Can use statistical criteria (AIC, BIC) | May overfit to validation set with many iterations |

---

## **Advanced Variations & Enhancements**

### **Bidirectional Stepwise Selection**
Combines forward and backward steps:
1. Forward step: Add best feature
2. Backward step: Remove worst feature (if it improves score)
3. Repeat until convergence

### **Regularization Path Methods**
**LASSO Path:**
- Vary regularization parameter λ from 0 to ∞
- Features enter/exit the model at specific λ values
- Provides entire regularization path efficiently

**Elastic Net Path:**
- Combines L1 (LASSO) and L2 (Ridge) penalties
- Controls both sparsity and grouping effects
- Better handling of correlated feature groups

### **Stability Selection**
Enhance any method by:
1. Bootstrap sample the data B times
2. Run feature selection on each bootstrap sample
3. Select features that appear in > 50% of bootstrap runs
4. Controls false discovery rate

---

## **Computational Complexity Comparison**

| **Method** | **Time Complexity** | **Space Complexity** | **Scalability** |
|:-----------|:-------------------|:-------------------|:---------------|
| **Correlation/VIF** | O(p²n) | O(p²) | Good up to p ≈ 1000 |
| **Mutual Information** | O(p²n log n) | O(pn) | Moderate, depends on binning |
| **RFE** | O(kp²) × model_cost | O(p) | Poor for large p |
| **Stepwise** | O(p²) × model_cost | O(p) | Poor for large p |

**Where:**
- p = number of features
- n = number of samples  
- k = number of features to select
- model_cost = computational cost of training one model

---

## **Practical Implementation Tips**

### **1. Pipeline Integration**
```python
# Example sklearn pipeline
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ('correlation_filter', CorrelationFilter(threshold=0.8)),
    ('rfe', RFE(LogisticRegression(), n_features_to_select=10)),
    ('classifier', LogisticRegression())
])
```

### **2. Cross-Validation Best Practices**
- **Always** perform feature selection within CV folds
- Use nested CV: outer loop for model evaluation, inner loop for feature selection
- Avoid data leakage by not using test set for selection

### **3. Stopping Criteria**
- **Performance threshold:** Stop when improvement < ε
- **Statistical significance:** Use p-values or confidence intervals
- **Computational budget:** Set maximum time/iterations
- **Diminishing returns:** Stop when marginal gain decreases

**Remember:** The goal is not to find the absolute optimal subset, but a good subset that generalizes well to unseen data.