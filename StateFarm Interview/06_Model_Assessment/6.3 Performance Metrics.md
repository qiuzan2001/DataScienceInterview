## ğŸŸ¦ **Regression Metrics

---
#### âœ… Quick Comparison

| Metric   | Interpretation       | Scale-sensitive | Penalizes large errors? | Robust to outliers? |
| -------- | -------------------- | --------------- | ----------------------- | ------------------- |
| **RMSE** | Avg error (units)    | Yes             | Yes                     | No                  |
| **MAPE** | Avg % error          | No              | No                      | No                  |
| **RÂ²**   | % variance explained | No              | No                      | No                  |

### ğŸ”¸ **1. RMSE â€“ Root Mean Squared Error**

* **What it Measures**:
  The average magnitude of error between predicted and actual values, **penalizing large errors more** due to squaring.

* **Formula**:
	* $$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
$$
	* **Explanation**:
		* $y_i$: Actual (true) value for observation $i$
		* $\hat{y}_i$: Predicted value for observation $i$
		* $n$: Total number of observations

* **Pros**:

  * Sensitive to large errors (useful when you want to penalize big mistakes).
  * Same unit as the target variable (interpretable).

* **Cons**:

  * Not robust to outliers.
  * Hard to compare across different datasets unless scaled.

* **Use When**:

  * You care more about **larger errors** being penalized heavily (e.g., forecasting, cost estimation).

---

### ğŸ”¸ **2. MAPE â€“ Mean Absolute Percentage Error**

* **What it Measures**:
  The average **percentage** difference between predicted and actual values. Expresses error as a percentage of the actual values.

* **Formula**:
	* $$
  \text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
  $$
	* Again, $y_i$ is actual, $\hat{y}_i$ is predicted, $n$ is number of observations.

  
* **Pros**:

  * Scale-independent (can compare across datasets).
  * Interpretable: â€œOn average, predictions are off by X%â€.

* **Cons**:

  * Undefined or distorted if $y_i = 0$ (division by zero).
  * Can overemphasize small values in the denominator.

* **Use When**:

  * You want an **intuitive percentage-based error**.
  * Your data doesnâ€™t include zero or near-zero actual values.

---

### ğŸ”¸ **3. R-squared (RÂ²) â€“ Coefficient of Determination**

* **What it Measures**:
  The proportion of variance in the target variable that is **explained by the model**.

* **Formula**:

  $$
  R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
  $$

  where $\bar{y}$ is the mean of actual values.
	* $y_i$: Actual value
	* $\hat{y}_i$: Predicted value
	* $\bar{y}$: Mean of actual values
	* Numerator: Total **model error** (residual sum of squares, RSS)
	* Denominator: **Total variance** in the actual data (total sum of squares, TSS)

* **Range**:

  * $R^2 = 1$: Perfect prediction
  * $R^2 = 0$: Model performs no better than predicting the mean
  * $R^2 < 0$: Model performs worse than a constant mean predictor

* **Pros**:

  * Indicates goodness-of-fit.
  * Easy to interpret: "X% of variability in target is explained by the model".

* **Cons**:

  * Doesnâ€™t reflect overfitting (adjusted RÂ² is better for that).
  * Can be misleading with non-linear models.

* **Use When**:

  * You want to assess **how well the model captures variation** in the data.

---

Great idea! Including the **Confusion Matrix** helps visualize how classification metrics relate to actual predictions. Here's an updated, detailed explanation of the ğŸŸ© **Classification Metrics** with the **Confusion Matrix** for clarity:

---

## ğŸŸ© **Classification Metrics

^26c0e0

### ğŸ”²  Confusion Matrix**  Overview

A confusion matrix is a table layout that helps evaluate the performance of a classification algorithm:

|                      | **Predicted: Positive** | **Predicted: Negative** |
| -------------------- | ----------------------- | ----------------------- |
| **Actual: Positive** | True Positive (**TP**)  | False Negative (**FN**) |
| **Actual: Negative** | False Positive (**FP**) | True Negative (**TN**)  |

---

### 1. âœ… **Accuracy**

* **What it Measures**: Overall correctness of the model.
* **Formula**:

  $$
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  $$
* **Use Case**: Best for **balanced classes**. Misleading for imbalanced datasets.

---

### 2. ğŸ¯ **Precision**

* **What it Measures**: Among predicted positives, how many are truly positive?
* **Formula**:

  $$
  \text{Precision} = \frac{TP}{TP + FP}
  $$
* **Use Case**: When **false positives** are costly (e.g., spam filters, cancer screening follow-ups).

---

### 3. ğŸ“ˆ **Recall** (Sensitivity or True Positive Rate)

* **What it Measures**: Among actual positives, how many did we correctly predict?
* **Formula**:

  $$
  \text{Recall} = \frac{TP}{TP + FN}
  $$
* **Use Case**: When **false negatives** are more serious (e.g., disease diagnosis, fraud detection).

---

### 4. ğŸ“Š **AUC â€“ Area Under the ROC Curve**

* **What it Measures**: How well the model distinguishes between classes across all thresholds.
* **Based on**: ROC Curve (TPR vs. FPR).
* **ROC Axes**:

  * Y-axis: **True Positive Rate (Recall)** = TP / (TP + FN)
  * X-axis: **False Positive Rate** = FP / (FP + TN)
* **Interpretation**:

  * 1.0 = Perfect separation
  * 0.5 = Random guessing
  * <0.5 = Poor classifier (inverse labeling)

---

### ğŸ§  **Putting It Together with an Example**

Imagine a classifier evaluated on 100 samples with the following confusion matrix:

|                           | **Predicted: Positive** | **Predicted: Negative** |
| ------------------------- | ----------------------- | ----------------------- |
| **Actual: Positive (40)** | 30 (**TP**)             | 10 (**FN**)             |
| **Actual: Negative (60)** | 5 (**FP**)              | 55 (**TN**)             |

#### Metrics Computed:

* **Accuracy**: (30 + 55) / 100 = 0.85
* **Precision**: 30 / (30 + 5) = 0.857
* **Recall**: 30 / (30 + 10) = 0.75
* **AUC**: Depends on model scores but would likely be high (e.g., 0.9+) given these values.
