Here’s a concise at-a-glance comparison, followed by a detailed, assumption-by-assumption discussion of OLS versus the GLM framework (with logistic regression as our running GLM example).

[[1.4.1 Assumptions Explained]]
[[1.4.2 GLM Assumptions Diagnostic]]

| **Assumption**           | **OLS**                                  | **GLM (Logistic Regression)**                                           |
| ------------------------ | ---------------------------------------- | ----------------------------------------------------------------------- |
| **A1. Linearity**        | Mean $E[Y]$ is a linear function of $X$. | Link of mean $g(E[Y])$ is linear in $X$.                                |
| **A2. Zero‐mean errors** | Residuals average to zero at each $X$.   | Score (derivative of log‐likelihood) has mean zero at true $\beta$.     |
| **A3. Homoscedasticity** | Constant variance $Var(\varepsilon)$.    | Variance follows known function $Var(Y)=\phi\,V(\mu)$.                  |
| **A4. Independence**     | Errors uncorrelated across observations. | Observations assumed independent (needed for likelihood factorization). |
| **A5. Distribution**     | Errors Normal for exact inference.       | Response from exponential family (Bernoulli here).                      |
| **A6. No collinearity**  | No exact linear combos among predictors. | Same—design matrix must be full rank for identifiability.               |
| **A7. Specification**    | Right predictors & functional form.      | Correct link, variance function & predictors.                           |

---

## A1. Linearity

* **OLS**

  * **Role:**
    $\,E[Y_i\mid X_i] = \beta_0 + \sum_j \beta_j X_{ij}.$
    Mean response lies exactly on a hyperplane in $X$.
  * **Consequence:**
    If the true mean is linear, OLS can recover it; if it’s curved, OLS mis-fits.
  * **Intuition:**
    “Each extra hour of study gives exactly 5 more points,” fits a straight-line.

* **GLM (General)**

  * **Role:**
    $g\bigl(E[Y_i\mid X_i]\bigr) = X_i\beta$.
    The transformed mean (via link $g$) is linear.
  * **Consequence:**
    Allows modeling non-linear mean relationships on the original scale (e.g.\ probabilities), while still using linear predictors.
  * **Intuition:**
    For binary data, we make $\log\bigl(p/(1-p)\bigr)$ linear even though $p$ itself follows an S-curve.

* **Logistic Regression**

  * **Role:**
    $\displaystyle\log\frac{p_i}{1-p_i} = \beta_0 + \sum_j\beta_j X_{ij}.$
  * **Consequence:**
    Probability $p_i$ can vary non-linearly between 0 and 1 but log-odds remain a straight plane.
  * **Intuition:**
    A change in $X_j$ shifts the sigmoid curve horizontally without bending it.

---

## A2. Zero-Mean Errors

* **OLS**

  * **Role:**
    $E[\varepsilon_i\mid X_i]=0$.
  * **Consequence:**
    $\widehat\beta$ is unbiased: on average you hit the true slope.
  * **Intuition:**
    Above-line and below-line residuals at each $X$ cancel out.

* **GLM (General)**

  * **Role:**
    At the true $\beta$, the expected score (the derivative of log-likelihood) is zero.
  * **Consequence:**
    Maximum-likelihood estimates solve unbiased estimating equations.
  * **Intuition:**
    The weighted residuals $Y_i - \mu_i$, when scaled by variance weights, sum to zero in each direction of $X$.

* **Logistic Regression**

  * **Role:**
    $E[Y_i - p_i \mid X_i] = 0$ at the true $\beta$.
  * **Consequence:**
    Fitted probabilities are, on average, correct (no systematic error left).
  * **Intuition:**
    Among all cases with the same $X$, the average predicted success rate matches the observed.

---

## A3. Homoscedasticity / Variance Function

* **OLS**

  * **Role:**
    $Var(\varepsilon_i\mid X_i)=\sigma^2$ constant.
  * **Consequence:**
    Simple closed-form variances for $\widehat\beta$ and valid standard errors.
  * **Intuition:**
    Scatter around the line looks equally wide at low and high $X$.

* **GLM (General)**

  * **Role:**
    $Var(Y_i\mid X_i)=\phi\,V(\mu_i)$, for a known variance function $V$.
  * **Consequence:**
    Each observation gets its own weight in IRLS: $w_i = [V(\mu_i)]^{-1}$.
  * **Intuition:**
    More precise when variance shrinks or grows with the mean (e.g.\ counts).

* **Logistic Regression**

  * **Role:**
    $Var(Y_i\mid X_i)=p_i(1-p_i)$.
  * **Consequence:**
    Observations with $p$ near 0.5 carry more weight (higher variance) than those near 0 or 1.
  * **Intuition:**
    Predicting a rare event (say $p=0.05$) is “easier” (lower variance) than predicting a 50:50 split.

---

## A4. Independence

* **OLS**

  * **Role:**
    Errors uncorrelated: $Cov(\varepsilon_i,\varepsilon_{i'})=0$.
  * **Consequence:**
    Valid standard errors and tests; no inflated significance from “clumped” errors.
  * **Intuition:**
    One student’s surprise over their score doesn’t affect another’s.

* **GLM (General)**

  * **Role:**
    Observations assumed independent so likelihood factors:
    $\prod_i f(y_i\mid X_i)$.
  * **Consequence:**
    If data are clustered (e.g.\ repeated measures), must adjust (e.g.\ GEE, mixed models).
  * **Intuition:**
    Each trial or individual outcome stands alone.

* **Logistic Regression**

  * **Role & Consequence:**
    Same as GLM general—Bernoulli outcomes must be independent.
  * **Intuition:**
    Predicting pass/fail for different students presumes no peer‐to‐peer error correlation.

---

## A5. Distributional Form

* **OLS**

  * **Role:**
    Errors are Normal: $\varepsilon_i\sim N(0,\sigma^2)$.
  * **Consequence:**
    Exact $t$- and $F$-tests; closed‐form likelihood.
  * **Intuition:**
    Residual histogram is a neat bell curve.

* **GLM (General)**

  * **Role:**
    $Y$ comes from an exponential family distribution.
  * **Consequence:**
    Likelihood is well‐behaved; estimation via IRLS.
  * **Intuition:**
    Choose Gaussian for continuous, Poisson for counts, Bernoulli/binomial for binary, etc.

* **Logistic Regression**

  * **Role:**
    $Y_i\sim\text{Bernoulli}(p_i)$, an exponential‐family member.
  * **Consequence:**
    Log-likelihood is
    $\sum[Y_i\log p_i + (1-Y_i)\log(1-p_i)]$, maximized via IRLS.
  * **Intuition:**
    Modeling “success vs failure” exactly matches the Bernoulli form.

---

## A6. No Perfect Multicollinearity

* **OLS** / **GLM** / **Logistic**

  * **Role:**
    Predictors must be linearly independent (design matrix full rank).
  * **Consequence:**
    If violated, you cannot uniquely solve for $\beta$.
  * **Intuition:**
    Don’t include “hours” and “minutes” together—they convey identical information.

---

## A7. Correct Specification

* **OLS**

  * **Role:**
    Include all relevant predictors, correct transforms (no omitted variable bias).
  * **Consequence:**
    Omitting a confounder biases $\widehat\beta$.
  * **Intuition:**
    Leaving out attendance (which correlates with both study and score) overstates the effect of study hours.

* **GLM (General)**

  * **Role:**
    Right link function, correct variance function $V(\mu)$, and all key predictors.
  * **Consequence:**
    Wrong link (e.g.\ identity for binary data) or omitted confounders leads to bias or misfit.
  * **Intuition:**
    Using identity link on 0/1 data both violates range and mis-models the mean–variance relationship.

* **Logistic Regression**

  * **Role:**
    Link = logit must be appropriate; include all predictors that affect log-odds.
  * **Consequence:**
    If true relationship is probit-like, logit still often works well—but omitted variables still bias estimates.
  * **Intuition:**
    Forgetting a key demographic factor can inflate or deflate the estimated odds ratios for your predictors.

---

By viewing each assumption side-by-side, you can see how GLMs generalize OLS: they replace constant-variance, Gaussian errors with a flexible variance function and link, yet preserve the familiar linear-predictor structure. Logistic regression is simply the Bernoulli GLM with the logit link—perfect for binary outcomes.

---

#### Bottom Line in Our Exam Example

When you check and (reasonably) satisfy all seven assumptions, OLS delivers an **unbiased**, **efficient**, and **exactly inferential** estimate of “points per hour studied.”  If any assumption falters—say errors aren’t constant or you’ve left out attendance—you risk biased slopes or misleading standard errors, and you’d then look to remedies (e.g.\ robust SEs, weighted regression, or a richer model).