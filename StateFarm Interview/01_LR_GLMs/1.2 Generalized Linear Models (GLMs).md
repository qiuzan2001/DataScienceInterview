At its heart, a GLM replaces the “straight-line” assumptions of OLS with three pieces:

1. **An exponential-family distribution** for the response $Y$.
2. **A link** that ties the mean of $Y$ to the linear predictor $X\beta$.
3. **An estimation procedure** (usually maximum likelihood) that generalizes the normal-equations.

---

### 2.1 Exponential-Family Form

Any distribution in the exponential family (Gaussian, binomial, Poisson, Gamma, etc.) can be written in the form

$$
f(y;\,\theta,\phi)
= \exp\!\Bigl(\,\frac{y\,\theta - b(\theta)}{\phi} \;+\; c(y,\phi)\Bigr).
$$

* **Canonical parameter** $\theta$: controls the location/mean of $Y$.
* **Normalization function** $b(\theta)$: ensures the density integrates (or sums) to 1.
* **Dispersion parameter** $\phi$: scales the variance (for many GLMs, $\phi$ is known or set to 1).

#### Why this form?

1. **Unifies many distributions.**

   * **Gaussian** $\bigl(Y\sim N(\mu,\sigma^2)\bigr)$
     $\theta = \mu,\;\; b(\theta)=\tfrac12\theta^2,\;\; \phi=\sigma^2$.
   * **Binomial** $\bigl(Y\sim \text{Binomial}(n,p)\bigr)$
     $\theta=\log\!\tfrac{p}{1-p},\;\; b(\theta)=n\log(1+e^\theta),\;\;\phi=1$.
   * **Poisson** $\bigl(Y\sim\text{Poisson}(\lambda)\bigr)$
     $\theta=\log\lambda,\;\; b(\theta)=e^\theta,\;\;\phi=1$.

2. **Moments from derivatives.**

   * Mean:

     $$
       \mu = E[Y] = b'(\theta).
     $$
   * Variance:

     $$
       \operatorname{Var}(Y) = \phi\,b''(\theta).
     $$

   Thus once you know $b(\cdot)$ and $\phi$, you immediately know how variability changes with $\theta$.

---

### 2.2 Link Function

We want to model how the **mean** $\mu = E[Y]$ depends on predictors $X$.  In OLS we had

$$
\mu = X\beta.
$$

In GLMs, we introduce a **link** $g$ so that

$$
g(\mu) \;=\; \eta \;=\; X\beta,
$$

where $g$ is a smooth, monotonic function.

* **Why** not model $\mu$ directly?

  1. Sometimes $\mu$ is constrained (e.g. $0<p<1$ for a probability, or $\mu>0$ for counts).
  2. A nonlinear $g$ can stabilize variance or make likelihoods simpler.

#### Canonical Link

The **canonical link** sets $g(\mu)=\theta$.  Since $\mu = b'(\theta)$, this means

$$
\theta = g(\mu).
$$

With this choice:

* The score equations (for maximum likelihood) simplify—there’s a clean analogue of the OLS normal equations.
* Iteratively reweighted least squares (IRLS) becomes the natural fitting algorithm.

**Examples of canonical links**

| Distribution | $\theta$ (canonical) | $g(\mu)$                       | Interpretation           |
| ------------ | -------------------- | ------------------------------ | ------------------------ |
| Gaussian     | $\mu$                | identity $g(\mu)=\mu$          | recovers OLS             |
| Binomial     | $\log\frac{p}{1-p}$  | logit $g(p)=\log\frac{p}{1-p}$ | maps $(0,1)\to\mathbb R$ |
| Poisson      | $\log\lambda$        | log $g(\mu)=\log\mu$           | ensures $\mu>0$          |

---

## Putting It All Together

1. **Choose the distribution**
   – Based on whether $Y$ is continuous (Gaussian), binary (Bernoulli/binomial), counts (Poisson), positive skewed (Gamma), etc.

2. **Write down the exponential-family form**
   – Identify $\theta$, $b(\theta)$, and $\phi$.
   – Recognize $E[Y]=b'(\theta)$, $\operatorname{Var}(Y)=\phi\,b''(\theta)$.

3. **Specify the linear predictor**

   $$
     \eta = X\beta.
   $$

   (Same as OLS: a weighted sum of features.)

4. **Pick the link**

   $$
     g\bigl(E[Y]\bigr) = \eta.
   $$

   – Often use canonical link $g(\mu)=\theta$.
   – But you can pick others (e.g. probit link for binomial if you prefer Normal-based inference).

5. **Estimate via Maximum Likelihood (IRLS)**

   * Derive the log-likelihood from the exponential form.
   * Set its derivative (the “score”) to zero—this yields weighted least-squares steps.
   * Iterate until convergence.

---

### Why GLMs Matter

* **Flexibility:** handle many outcome types under one umbrella.
* **Interpretability:** coefficients $\beta$ still describe how predictors shift the link (and thus the mean).
* **Efficiency:** by matching the distributional form, inference (standard errors, tests) is valid.

---

### Logical Flow from OLS to GLM

1. **OLS** assumes Normality + identity link ⇒ constant variance + unbounded outcomes.
2. **GLM** drops “always-Normal” ⇒ allows variance to change with $\mu$ (through $b''(\theta)$).
3. **Link** transforms $\mu$ to $\mathbb R$ so that a linear model still makes sense even when $\mu$ is restricted.
4. **Canonical link** makes the math almost as neat as OLS—only now with data-driven weights that reflect each observation’s variance.

[[1.4 GLM Assumptions]]