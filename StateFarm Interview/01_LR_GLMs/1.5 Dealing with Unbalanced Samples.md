Below is a thorough, step-by-step guide to handling class imbalance in logistic regression. I’ve assumed you know basic probability, calculus, and linear models, and I’ve tried to build each idea from first principles so you can see how everything connects.

---

## 1.5 Dealing with Unbalanced Samples for Logistic Regression

### A. Why Imbalance Matters

Imagine you have a disease-screening test, but only 1% of patients actually have the disease. In a dataset of 10 000 patients:

* **Positive class** (disease): 100
* **Negative class** (healthy): 9 900

A naïve classifier that **always** predicts “healthy” is already 99% accurate—but it never catches any sick patients!

**Key point:**
*Logistic regression’s training (maximum likelihood) will try to fit the probabilities for both classes, but when you convert those probabilities to labels via a default threshold (0.5), you’ll almost always predict the majority class. And if you evaluate by raw accuracy, you’ll be lulled into thinking your model is fantastic, even though it misses every positive.*

---

### B. Strategy 1: Resampling the Training Data

The idea is to rebalance the dataset so that the model “sees” roughly equal numbers of positives and negatives.

| Approach                   | What you do                                                                   | Pros                                                      | Cons                                     |
| -------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------------------- |
| **Random oversampling**    | Duplicate minority examples until balanced                                    | Easy to implement                                         | Can overfit to duplicated rows           |
| **SMOTE**                  | Synthetically generate new minority points by interpolating between neighbors | Less overfitting than plain duplication; adds variability | May create ambiguous (borderline) points |
| **Random undersampling**   | Drop random majority examples                                                 | Fast; reduces dataset size                                | Discards potentially useful data         |
| **Informed undersampling** | Remove majority points near minority (e.g. Tomek links, NearMiss)             | Keeps “hard” negative examples only                       | More complex to tune                     |

> **How SMOTE works (intuitively):**
> For each minority point $x$, pick one of its nearest minority neighbors $x'$, then create
>
> $$
> x_{\text{new}}
> = x + \alpha\,(x' - x),
> \quad \alpha\sim U(0,1).
> $$
>
> This “fills in” the feature space around the minority class instead of just duplicating exact copies.

**Logical trade-off:**
Resampling changes the empirical class frequencies the model sees. That can improve classification performance, but it also distorts the true prevalence of the classes—so if you care about calibrated probabilities you’ll need to correct for that later (e.g. by recalibrating on the original distribution).

---

### C. Strategy 2: Class-Weighted Loss

Instead of touching the data, you change the **objective**.  Standard logistic regression minimizes

$$
-\sum_{i=1}^n \bigl[y_i\log p_i + (1-y_i)\log(1-p_i)\bigr].
$$

With **weights** $w_i$, you do

$$
-\sum_{i=1}^n w_i\;\bigl[y_i\log p_i + (1-y_i)\log(1-p_i)\bigr].
$$

A common choice is

$$
w_i =
\begin{cases}
\displaystyle \frac{1}{\#\text{positives}}
&\text{if }y_i=1,\\[6pt]
\displaystyle \frac{1}{\#\text{negatives}}
&\text{if }y_i=0.
\end{cases}
$$

This makes each class contribute equally to the loss, so the optimizer “pays more attention” to the rare class.

* **Pros**: No data duplication/removal; retains full information.
* **Cons**: May require tuning (e.g.\ slightly higher weights for minority); very large weights can destabilize convergence.

---

### D. Strategy 3: Threshold Tuning

Logistic regression outputs a probability $\hat p_i$.  By default you predict

$$
\hat y_i = 
\begin{cases}
1 & \hat p_i\ge0.5,\\
0 & \hat p_i<0.5.
\end{cases}
$$

But with imbalance you often want a **lower** threshold to increase recall (catch more positives) at the cost of precision:

$$
\hat y_i = 
\begin{cases}
1 & \hat p_i \ge T,\\
0 & \hat p_i < T,
\end{cases}
$$

where you choose $T$ (e.g.\ 0.2 or 0.1) based on your tolerance for false positives vs. false negatives.

> **How to pick $T$**
>
> 1. Split off a validation set.
> 2. For each candidate $T$, compute precision and recall on that set.
> 3. Choose the $T$ that maximizes your preferred metric (F1, or recall subject to a minimum precision, etc.).

---

### E. Strategy 4: Better Evaluation Metrics

When classes are unbalanced, **accuracy** is misleading. Instead look at:

1. **Confusion matrix**

   * True positives (TP), false positives (FP), true negatives (TN), false negatives (FN).

2. **Precision** $\displaystyle \frac{\text{TP}}{\text{TP}+\text{FP}}$
   – Of the examples you called positive, how many really are?

3. **Recall (Sensitivity)** $\displaystyle \frac{\text{TP}}{\text{TP}+\text{FN}}$
   – Of all real positives, how many did you catch?

4. **F1 score**: harmonic mean of precision & recall.

5. **ROC Curve**: plot true positive rate vs. false positive rate as you sweep the threshold.

   * **AUROC** (area under curve) is threshold-independent but can be over-optimistic when positives are very rare.

6. **Precision–Recall Curve**: plot precision vs. recall as threshold varies.

   * **AUPRC** (area under PR curve) is often more informative under heavy imbalance, since it focuses on the minority-class performance.

---

## Putting It All Together

1. **Inspect your data**: compute class frequencies and baseline accuracy.
2. **Choose your strategy** (or combination):

   * If you can afford to discard data: consider undersampling.
   * If you want to keep all data: try class weights or oversampling.
   * Always validate any resampling or weighting via cross-validation.
3. **Tune your decision threshold** on a held-out set to hit your target precision/recall trade-off.
4. **Report the right metrics**: confusion matrix, precision, recall, F1, and curves (ROC & PR).

By understanding the logic—*why* imbalance skews the loss, *how* resampling rebalances your sample, *why* weights shift the optimizer’s focus, and *how* thresholding and evaluation metrics connect to real-world costs—you’ll be able to build fairer, more reliable classifiers whenever your data aren’t neatly 50/50.
