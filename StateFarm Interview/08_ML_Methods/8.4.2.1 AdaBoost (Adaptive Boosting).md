## 1. What Is Boosting?

Boosting is an ensemble technique that combines many “weak” classifiers to form a single, stronger classifier. Unlike bagging (which trains learners independently on bootstrap samples), boosting trains learners **sequentially**, each one focusing more on the mistakes of its predecessors.

---

## 2. AdaBoost at a Glance

* **Goal:** Minimize classification error by reweighting misclassified examples and assigning higher influence to more accurate learners.
* **Weak Learner:** Any classifier slightly better than random (e.g. decision stump).
* **Final Model:** A weighted vote of all weak learners.

---

## 3. Algorithmic Steps

1. **Initialization**

   * Given $n$ examples $\{(x_i,y_i)\}_{i=1}^n$, set each weight

     $$
       w_i^{(1)} = \frac{1}{n}.
     $$

2. **For $m=1$ to $M$ (number of rounds):**
   a. **Train weak learner** $h_m$ on the weighted dataset $\{(x_i,y_i,w_i^{(m)})\}$.
   b. **Compute weighted error**

   $$
     \varepsilon_m \;=\; \sum_{i:\,h_m(x_i)\neq y_i} w_i^{(m)}.
   $$

   c. **Compute learner’s weight**

   $$
     \alpha_m \;=\; \frac{1}{2}\,\ln\!\Bigl(\frac{1 - \varepsilon_m}{\varepsilon_m}\Bigr).
   $$

   d. **Update example weights**

   $$
     w_i^{(m+1)} \;=\; w_i^{(m)} \times
       \begin{cases}
         e^{+\alpha_m}, & \text{if } h_m(x_i)\neq y_i,\\[6pt]
         e^{-\alpha_m}, & \text{if } h_m(x_i)= y_i,
       \end{cases}
   $$

   then **normalize** so $\sum_i w_i^{(m+1)} = 1$.

3. **Final Prediction (Binary)**
   For any new $x$,

   $$
     H(x) \;=\; \operatorname{sign}\!\Bigl(\sum_{m=1}^M \alpha_m\,h_m(x)\Bigr).
   $$

---

## 4. Why These Formulas Make Sense

* **Error $\varepsilon_m$:**
  Measures how often $h_m$ is wrong, **given** the current weights.

* **Weight $\alpha_m$:**

  $$
    \alpha_m = \tfrac12\ln\!\Bigl(\tfrac{1-\varepsilon_m}{\varepsilon_m}\Bigr)
  $$

  * If $\varepsilon_m\ll0.5$, $\alpha_m$ is **large** ⇒ strong learner.
  * If $\varepsilon_m\approx0.5$, $\alpha_m\approx0$ ⇒ learner no better than chance.

* **Updating $w_i$:**

  * **Misclassified examples** ($h_m(x_i)\neq y_i$): multiply by $e^{+\alpha_m}>1$ ⇒ their weight **increases**.
  * **Correctly classified**: multiply by $e^{-\alpha_m}<1$ ⇒ their weight **decreases**.
  * **Normalization** restores $\sum w_i=1$.

This dynamic focuses subsequent learners on the hard cases.

---

## 5. Micro Example (3-Point Dataset)

| Index $i$ | True Label $y_i$ | Initial $w_i^{(1)}$ |
| :-------: | :--------------: | :-----------------: |
|     1     |       $+1$       |        $1/3$        |
|     2     |       $-1$       |        $1/3$        |
|     3     |       $+1$       |        $1/3$        |

### **Iteration 1**

* **Weak learner $h_1$:** “Always predict +1.”
* **Error**
  $\varepsilon_1 = \sum_{i:h_1(x_i)\neq y_i} w_i = w_2 = \tfrac13.$
* **Learner weight**
  $\alpha_1 = \tfrac12\ln\bigl(\tfrac{1-\tfrac13}{\tfrac13}\bigr)            = \tfrac12\ln(2)\approx0.3466.$
* **Weight updates (before normalization):**

  | $i$ | $w_i^{(1)}$ | $h_1(x_i)\stackrel?=y_i$ |           Factor          | $w_i'$ |
  | :-: | :---------: | :----------------------: | :-----------------------: | :----: |
  |  1  |    0.3333   |          Correct         | $e^{-0.3466}\approx0.707$ | 0.2357 |
  |  2  |    0.3333   |         **Wrong**        | $e^{+0.3466}\approx1.414$ | 0.4714 |
  |  3  |    0.3333   |          Correct         |           0.707           | 0.2357 |

  * Sum = 0.2357+0.4714+0.2357 = 0.9428
  * **Normalize**: divide each by 0.9428 ⇒

  $$
    w^{(2)} = \{0.25,\;0.50,\;0.25\}.
  $$

### **Iteration 2**

* **Weak learner $h_2$:** “Always predict +1 for $i=1,2$; predict $-1$ for $i=3$.”

  * Misclassifies only $i=3$.
* **Error**
  $\varepsilon_2 = w^{(2)}_3 = 0.25.$
* **Learner weight**
  $\alpha_2 = \tfrac12\ln\bigl(\tfrac{1-0.25}{0.25}\bigr)            = \tfrac12\ln(3)\approx0.5493.$
* **Weight updates (before normalization):**

  | $i$ | $w_i^{(2)}$ | $h_2(x_i)\stackrel?=y_i$ |           Factor          | $w_i'$ |
  | :-: | :---------: | :----------------------: | :-----------------------: | :----: |
  |  1  |     0.25    |          Correct         | $e^{-0.5493}\approx0.578$ | 0.1445 |
  |  2  |     0.50    |          Correct         |           0.578           | 0.2890 |
  |  3  |     0.25    |         **Wrong**        | $e^{+0.5493}\approx1.732$ | 0.4330 |

  * Sum = 0.1445 + 0.2890 + 0.4330 = 0.8665
  * **Normalize** ⇒

  $$
    w^{(3)} \approx \{0.167,\;0.334,\;0.499\}.
  $$

---

## 6. Final Prediction after 2 Rounds

$$
  H(x) \;=\; \mathrm{sign}\!\bigl(\alpha_1\,h_1(x)
                             \;+\;\alpha_2\,h_2(x)\bigr).
$$

Evaluate on each $i$:

| $i$ | $h_1(x_i)$ | $h_2(x_i)$ | Score $\alpha_1h_1+\alpha_2h_2$ | $H(x_i)$ | True $y_i$ | Correct? |
| :-: | :--------: | :--------: | :-----------------------------: | :------: | :--------: | :------: |
|  1  |     +1     |     +1     |   $0.3466 + 0.5493 = +0.8959$   |    +1    |     +1     |    Yes   |
|  2  |     +1     |     +1     |   $0.3466 + 0.5493 = +0.8959$   |    +1    |     –1     |  **No**  |
|  3  |     +1     |     –1     |   $0.3466 – 0.5493 = –0.2027$   |    –1    |     +1     |  **No**  |

> *Note:* With only two weak learners, the final model still makes mistakes—but it usually outperforms any single weak learner alone.

---

## 7. Key Takeaways

* **Sequential focus:** Each round ups the ante on previously misclassified points.
* **Weighted voting:** More accurate learners get larger $\alpha$ and thus more say in the final decision.
* **No complicated math needed:** The core is reweighting and a log‐ratio to gauge confidence.
* **Empirical success:** Despite its simplicity, AdaBoost often yields top‐tier performance on real datasets.

Keep this note as your go-to reference: the step-by-step flow, the intuition behind the formulas, and the tiny worked example should together make AdaBoost crystal clear!
