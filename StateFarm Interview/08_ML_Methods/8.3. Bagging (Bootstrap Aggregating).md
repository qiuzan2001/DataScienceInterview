#### 3.1 Core Idea

Bagging, short for **Bootstrap Aggregating**, is an ensemble technique designed to **reduce variance** of a learner without substantially increasing bias. The intuition rests on two pillars:

1. **Bootstrapping (Sampling with Replacement):**

   * By drawing multiple random samples (with replacement) from the original dataset, each “bootstrap sample” is both similar to and different from the full dataset.
   * Each sample omits about 36.8% of the original points (on average) and repeats roughly 63.2%, introducing stochastic diversity.

2. **Aggregation of Independent Learners:**

   * Train one base model per bootstrap sample.
   * Combine their predictions—averaging for regression, voting for classification.
   * Random fluctuations of individual learners tend to cancel out, dramatically shrinking variance.

Mathematically, if each individual predictor $f_b(x)$ has variance $\sigma^2$ and pairwise correlation $\rho$, then the variance of the averaged predictor is

$$
\mathrm{Var}\bigl(\tfrac{1}{B}\sum_{b=1}^B f_b(x)\bigr)
= \frac{\sigma^2}{B} + \frac{B-1}{B}\,\rho\,\sigma^2.
$$

As $B$ grows, the first term vanishes and only the correlated component remains—so **lower correlation** among learners yields greater variance reduction.

---

#### 3.2 Algorithmic Steps

1. **Bootstrap Sampling**

   * Given a dataset of size $n$, repeat for $b=1,\dots,B$:

     * Draw $n$ points **with replacement**.
     * Call this the $b$-th bootstrap sample $D_b$.
   * Each $D_b$ contains duplicates and leaves out about $0.368n$ points.

2. **Train Base Learners**

   * Fit a chosen base learner (commonly a decision tree) separately on each $D_b$, yielding hypotheses $f_1, f_2, \dots, f_B$.
   * Since each sample differs slightly, each $f_b$ captures different aspects of the data’s noise.

3. **Aggregate Predictions**

   * **Regression:**

     $$
       \hat{f}(x) = \frac{1}{B}\sum_{b=1}^B f_b(x).
     $$
   * **Classification:**

     $$
       \hat{y}(x) = \operatorname{mode}\bigl\{f_1(x),\,f_2(x),\dots,f_B(x)\bigr\}.
     $$

4. **Out-of-Bag (OOB) Estimation**

   * For each original observation, roughly one third of the learners did **not** see it (since it wasn’t sampled).
   * Aggregating predictions of only those learners yields an unbiased estimate of test error—no separate validation set needed.

---

#### 3.3 Strengths & Weaknesses

* **Pros**

  * **Variance Reduction:**
    Ensemble average smooths out fluctuations of individual trees, leading to more stable predictions.
  * **Easy Parallelization:**
    Each of the $B$ models trains independently—ideal for multi-core or distributed computing.
  * **Resilience to Overfitting:**
    Although single deep trees can overfit, bagged trees collectively generalize much better.
  * **Built-in Error Estimation:**
    OOB error provides a reliable internal measure of generalization performance.

* **Cons**

  * **Bias Remains Unchanged:**
    Bagging does not tackle systematic errors of the base learner. If each tree is consistently wrong (high bias), averaging won’t help.
  * **Computational Cost:**
    Training and storing $B$ models can be memory- and time-intensive, especially for large $B$ and complex learners.
  * **Interpretability Loss:**
    An ensemble of many trees sacrifices the clear decision paths of a single tree, making model explanations harder.

---

#### 3.4 [[7. Random Forest|Random Forest]] (An Extension)

While standard bagging grows each tree on a bootstrap sample, **Random Forests** introduce an extra layer of randomness to further **de-correlate** learners:

* **Feature Subsampling at Splits:**

  * At each node split, instead of considering all $p$ features, choose a random subset of size $m$ (commonly $m=\sqrt{p}$ for classification, $m=p/3$ for regression).
  * Select the best split only among these $m$ features.

* **Why It Helps:**

  * Two deep trees on the same data are often similar (highly correlated) because strong predictors repeatedly dominate splits.
  * Limiting candidate features forces trees to explore alternative predictors, lowering $\rho$ in the variance formula and thus improving ensemble variance reduction.

* **Key Hyperparameters:**

  1. **Number of Trees ($B$):** More trees → lower variance (diminishing returns after a threshold).
  2. **Feature Subset Size ($m$):** Balances strength of individual trees vs. ensemble de-correlation.
  3. **Tree Depth / Min. Samples per Leaf:** Controls individual tree bias; shallower trees reduce overfitting but may introduce bias.

* **Outcomes:**

  * **Superior Accuracy:** Random Forests often outperform bagged trees by a small but consistent margin.
  * **Variable Importance Measures:**
    By observing how prediction error increases when a feature’s values are permuted in OOB data, one can rank feature importance.

---

**Logical Flow Recap:**

1. **Bagging** combats variance by averaging many noisy but unbiased models—bootstrapping injects diversity.
2. **Random Forests** add feature randomness to break the similarity among those models, squeezing even more out of variance reduction.
3. **The bias-variance trade-off** remains central: bagging/forests target the **variance** corner—if bias dominates, one might look to boosting or richer base learners instead.

This consolidated view should arm you with both the **mechanics** and the **intuition** behind bagging and its forest-based extension. Good luck in your studies!
