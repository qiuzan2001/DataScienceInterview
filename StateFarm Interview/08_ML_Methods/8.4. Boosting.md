Boosting is an ensemble technique that builds a strong learner by sequentially adding weak learners, each one attempting to correct the mistakes of the combined ensemble so far. Unlike bagging, which builds learners independently on bootstrap samples, boosting explicitly focuses on “hard” examples—those mispredicted by earlier learners—by reweighting data points or fitting to residual errors. This makes boosting particularly powerful at reducing both bias and variance, but also more sensitive to noise and hyperparameter settings.

---

##### 4.1 Core Idea

1. **Sequential Learning:**

   * Boosting constructs an ensemble in a stage‐wise fashion. At each stage $m$, a new weak learner $h_m(x)$ is trained to improve upon the current ensemble’s mistakes.
   * Formally, if the current ensemble’s prediction is $F_{m-1}(x)$, the new learner is chosen to reduce the remaining error, producing $F_m(x) = F_{m-1}(x) + \nu\,h_m(x)$, where $\nu$ is the learning rate.

2. **Focusing on Hard Examples:**

   * Early misclassifications or large residuals are given more emphasis so that subsequent learners pay greater attention to them.
   * Two common mechanisms:

     * **Reweighting:** Increase data weights for misclassified points (as in AdaBoost).
     * **Gradient-Fitting:** Fit new models to negative gradients of a loss function (as in Gradient Boosting).

3. **Weighted Voting or Additive Modeling:**

   * In classification, each weak learner votes, weighted by its accuracy.
   * In regression, predictions are summed (possibly with shrinkage via $\nu$), yielding a final additive model.

---

##### 4.2 Common Variants

###### [[8.4.2.1 AdaBoost (Adaptive Boosting)]]

* **Initialization:**
  All $n$ training examples start with equal weight $w_i = \tfrac{1}{n}$.
* **Iterative Updates:**

  1. Train weak learner $h_m$ on weighted data.
  2. Compute error rate $\varepsilon_m = \sum_{i\,:\,h_m(x_i)\neq y_i} w_i$.
  3. Compute learner weight $\alpha_m = \tfrac{1}{2}\ln\bigl(\tfrac{1-\varepsilon_m}{\varepsilon_m}\bigr)$.
  4. Update each data weight:

     $$
       w_i \gets w_i \times
       \begin{cases}
         e^{+\alpha_m}, & \text{if } h_m(x_i)\neq y_i,\\
         e^{-\alpha_m}, & \text{if } h_m(x_i)= y_i,
       \end{cases}
     $$

     then renormalize so $\sum_i w_i = 1$.
* **Final Prediction (Binary Classification):**

  $$
    H(x) = \mathrm{sign}\Bigl(\sum_{m=1}^M \alpha_m\,h_m(x)\Bigr).
  $$
* **Key Properties:**

  * Emphasizes misclassified points by boosting their weights.
  * Learner weights $\alpha_m$ reflect reliability (lower error → higher $\alpha$).

---
###### [[8.4.2.2 Gradient Boosting]]

* **Function‐Space View:**
  Treat model fitting as gradient descent in the space of functions, adding one weak learner at a time to follow the negative loss gradient.

* **Algorithmic Outline:**

  1. **Initialization:**

     $$
       F_0(x) = \arg\min_{\gamma} \sum_i L(y_i,\,\gamma)
     $$

     (e.g. for squared‐error regression, $\gamma = \tfrac1n\sum_i y_i$).

  2. **For** $m = 1$ to $M$:

     1. **Compute Pseudo‐Residuals:**

        $$
          r_{im} = -\left[\frac{\partial L\bigl(y_i,\,F_{m-1}(x_i)\bigr)}{\partial F_{m-1}(x_i)}\right]
        $$

     2. **Fit Base Learner:**
        Train weak learner $h_m(x)$ to predict the targets $\{r_{im}\}$.

     3. **Line Search / Step Size:**

        $$
          \rho_m = \arg\min_{\rho}\sum_i L\bigl(y_i,\,F_{m-1}(x_i) + \rho\,h_m(x_i)\bigr)
        $$

     4. **Update Ensemble:**

        $$
          F_m(x) = F_{m-1}(x) + \nu\,\rho_m\,h_m(x)
        $$

        where $\nu\in(0,1]$ is the **learning rate** (shrinkage).

  3. **Final Model:**

     $$
       F_M(x) = F_0(x) + \sum_{m=1}^M \nu\,\rho_m\,h_m(x).
     $$

     * **Regression:** use $F_M(x)$ directly.
     * **Classification:** apply an inverse‐link (e.g. sigmoid) or take $\mathrm{sign}(F_M(x))$.

* **Key Properties:**

  * **Shrinkage ($\nu$)** slows learning, often improving generalization.
  * **Stage‐wise fitting** adds complexity gradually, reducing overfitting.
  * **Pseudo‐residuals** focus each new learner on the current errors of the ensemble.

---

###### 4.2.3 Modern Implementations: XGBoost, LightGBM, CatBoost

| Library      | Key Innovations                                                                                                                                          |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **XGBoost**  | Second‐order (Hessian) approximation for loss, regularization terms (L1/L2), tree pruning based on gain, parallel histogram‐based split finding.         |
| **LightGBM** | Leaf‐wise tree growth (instead of level‐wise), histogram binning, exclusive feature bundling to reduce dimensionality, gradient‐based one‐side sampling. |
| **CatBoost** | Ordered boosting to reduce target leakage, handling of categorical features via target statistics, symmetric tree structure for faster inference.        |

* **Regularization & Pruning:** Control tree complexity to mitigate overfitting.
* **Efficiency:** Histogram‐based splitting and optimized data structures for speed and memory.
* **Categorical Handling (CatBoost):** Native support without manual encoding.

---

##### 4.3 Strengths & Weaknesses

| Strengths                                                                                                | Weaknesses                                                                                                                                      |
| -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **Bias & Variance Reduction:** Sequential correction plus shrinkage reduces both.                        | **Parallelization Challenges:** Inherently sequential; limited boost from distributed training.                                                 |
| **Custom Loss Functions:** Any differentiable $L$ (e.g., logistic, Huber, Poisson).                      | **Overfitting Risk:** Noisy data or outliers can receive excessive focus, unless regularized.                                                   |
| **Feature Importance:** Native measures (gain, split count) help interpret models.                       | **Hyperparameter Sensitivity:** Learning rate $\nu$, number of iterations $M$, tree depth, and regularization terms all require careful tuning. |
| **High Predictive Power:** State‐of‐the‐art on many tabular tasks.                                       | **Longer Training Time:** Especially with small $\nu$ (slower convergence).                                                                     |
| **Flexibility:** Can combine with subsampling (stochastic gradient boosting) to reduce variance further. | **Less Intuitive than Single Trees:** Harder to visualize or explain to non‐technical stakeholders.                                             |

* **Practical Tips:**

  * Use a small learning rate ($\nu=0.01–0.1$) with a larger number of trees for stability.
  * Employ early‐stopping on a validation set to avoid overfitting and choose the optimal $M$.
  * Combine with subsampling of rows (e.g., 50–80%) and features (e.g., column sampling) to improve generalization.

---

**Summary:** Boosting crafts a powerful ensemble by iteratively focusing on points where previous models erred. AdaBoost achieves this by reweighting misclassifications, while gradient boosting generalizes the idea to arbitrary loss functions via gradient descent in function space. Modern variants (XGBoost, LightGBM, CatBoost) push the envelope with algorithmic optimizations and regularization strategies, making boosting the go-to choice for many high-accuracy applications—provided one invests in hyperparameter tuning and manages overfitting.
