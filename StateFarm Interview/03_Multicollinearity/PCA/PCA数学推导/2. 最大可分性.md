
## 🔹 背景回顾

我们已经讨论了：

* **向量表示依赖所选基（basis）**
* **通过基变换可以降维（reduce dimensionality）**
* **选择什么样的基 → 影响投影结果的质量**

---

## ✅ 核心问题：降维时如何选择“最优基”？

> 给定一组 $N$ 维向量，如何选出 $K$ 个基（$K < N$），使得投影后的结果**尽可能有区分度 / 信息量最大**？

---

## 🔍 直觉目标：**最大可分性（Maximally Separable）**

### ✅ 理解目标：

我们希望：

* 投影后的数据尽量**不重叠** 
* 数据点**尽量分散开（variance is large）** 
* 尽量**保留原始信息结构**

---

## 📘 信息论视角：最大熵（Maximum Entropy）

* **熵（Entropy）** 衡量信息的不确定性
* 投影结果“越分散”，熵越大
* **熵越大 ⇒ 信息越丰富 ⇒ 表示越好**

这和“最大方差”方向一致，因此在降维中，我们倾向选择：

> **使投影结果方差最大的基方向**

---

## 📊 数学目标（即将引出 PCA）

* 给定原始数据 $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \in \mathbb{R}^N$
* 找 $K$ 个单位向量 $\mathbf{u}_1, \dots, \mathbf{u}_K \in \mathbb{R}^N$，使得：

$$
\sum_{i=1}^n \sum_{j=1}^K (\mathbf{u}_j^\top \mathbf{x}_i)^2
$$

最大化，即 **在新基上的投影平方和最大（最大方差方向）**

这就是主成分分析（**PCA：Principal Component Analysis**）的核心思想。
