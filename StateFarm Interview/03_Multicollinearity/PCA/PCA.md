下面从**动机 → 数学推导 → 几何直观 → 算法步骤 → 使用要点**五个维度，系统阐述主成分分析 (PCA, Principal Component Analysis) 的基本原理。

---

## 1  为什么需要 PCA？（动机）

* **降维** 在高维空间，样本常稀疏且计算量大；希望在保持信息量（方差）尽量多的前提下，把数据映射到低维空间。
* **去相关** 许多算法假设特征独立。若原始特征相关，PCA 可以找到互不相关（正交）的新坐标轴。
* **可视化** 把高维数据压到 2–3 维后方便绘图与洞察结构。

---

## 2  数学原理：最大化投影方差
[[0 PCA 数学推导]]

---

## 3  几何与概率直观

* **几何**：在数据方差最大的方向“伸展”轴，让第一轴捕捉最长“椭球”半径，后续轴依方差递减且两两正交。
* **概率**：若假设数据由零均值高斯分布产生，PCA 给出最大似然估计中“因子载荷”的正交限制特例，与因子分析关系密切。

---

## 4  算法步骤（矩阵方法

1. 输入 $\mathbf{X}$，中心化（必要时标准化）。
2. 计算协方差矩阵 $\mathbf{S}$ 或直接对 $\tilde{\mathbf{X}}$ 做 SVD。
3. 取前 $k$ 个最大特征值-特征向量对 $(\lambda_i,\mathbf{w}_i)$。
4. 主成分得分 $\mathbf{Z} = \tilde{\mathbf{X}}\mathbf{W}_k$。
5. 解释方差比：

   $$
   \text{EVR}_k = \frac{\sum_{i=1}^{k}\lambda_i}{\sum_{j=1}^{p}\lambda_j}.
   $$

   通过累积 EVR 选择 $k$（常见阈值 80 %–95 %），或用 **Scree Plot** 找弯折点。
6. 如需重构到原空间：$\hat{\mathbf{X}}=\mathbf{Z}\mathbf{W}_k^\top + \boldsymbol\mu$.

---

## 5  使用要点与局限

| 主题        | 说明                                               |
| --------- | ------------------------------------------------ |
| **线性假设**  | PCA 捕捉的是线性相关结构；非线性流形用 Kernel PCA、t-SNE、UMAP 更合适。 |
| **可解释性**  | 新轴是线性组合，常难以直接映射到原始实体含义；可查看 loading 大小做解释。        |
| **尺度敏感**  | 不同量纲需标准化；否则大方差特征主导。                              |
| **异常值**   | 极端值会极度影响协方差；可考虑 Robust PCA。                      |
| **稀疏性需求** | 若希望 loading 稀疏，采用 Sparse PCA / Lasso PCA。        |

---

## 6  与相关技术的比较

| 方法        | 目标函数                                                  | 是否正交 | 备注        |
| --------- | ----------------------------------------------------- | ---- | --------- |
| **PCA**   | 最大方差 / 最小重构误差                                         | 是    | 无监督降维基准   |
| **LDA**   | 最大类间距 / 最小类内距                                         | 否    | 监督降维，侧重判别 |
| **Ridge** | $\|\mathbf{y}-\mathbf{X}\beta\|^2+\lambda\|\beta\|^2$ | –    | 回归正则，无降维  |
| **因子分析**  | 概率模型最大似然                                              | 可旋转  | 允许特征向量非正交 |

---

## 7  小结

* **核心思想**：通过求协方差矩阵最大特征值对应向量，把数据投影到方差最大的正交子空间。
* **两大等价视角**：①最大投影方差；②最小重构误差（SVD）。
* **实际流程**：中心化 → SVD/特征分解 → 选 k → 得分与方差解释率。
* **注意事项**：量纲、异常值、线性假设及可解释性。

掌握以上原理后，PCA 不再只是“把数据丢给库函数”，而能清晰理解每一步的数学与几何含义，从而在特征工程与可视化中更游刃有余。
