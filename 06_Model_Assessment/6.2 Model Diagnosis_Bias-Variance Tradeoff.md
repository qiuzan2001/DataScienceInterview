Understanding **bias** and **variance** is essential to diagnosing **underfitting** and **overfitting** in machine learning models.

---

### ‚úÖ **What is Bias?**

* **Definition**: Bias is the error caused by incorrect assumptions in the learning algorithm. It reflects how far off the model's predictions are from the actual values.
* **High Bias Characteristics**:

  * Model makes overly simplistic assumptions (e.g., linear models for non-linear data).
  * Poor fit on **training data** and **test data**.
  * Leads to **underfitting** ‚Äì the model is too simple to capture underlying patterns.
  * Example: Predicting complex relationships using only a straight line.
* **Goal**: Minimize bias to reduce both training and testing errors.

---

### ‚úÖ **What is Variance?**

* **Definition**: Variance is the amount by which the model's predictions would change if we used a different training dataset. It shows model sensitivity to small data changes.
* **High Variance Characteristics**:

  * Model is too complex and tries to learn even the noise in training data.
  * Very low training error but high error on **unseen/test data**.
  * Leads to **overfitting** ‚Äì the model memorizes rather than generalizes.
  * Example: Using a high-degree polynomial to perfectly fit training points.
* **Goal**: Minimize variance to ensure generalization to new data.

---

### ‚öñÔ∏è **Bias-Variance Tradeoff**

* **The Tradeoff**:

  * A **simpler model** tends to have **high bias and low variance**.
  * A **complex model** tends to have **low bias and high variance**.
  * The objective is to find a **balance** that minimizes **total error**.

* **Total Error Formula**:

  $$
  \text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
  $$

  * **Irreducible error** comes from noise inherent in the data.

* **Goal of the Tradeoff**:

  * Achieve the **optimal model complexity** where both bias and variance are balanced.
  * This point gives the **lowest possible generalization error**.
![[Pasted image 20250605141224.png]]
---

### üß† **Practical Implications**

* **Underfitting (High Bias)**:

  * Model is too basic.
  * Symptoms: High training and test error.
  * Solution: Use more complex models or add features.

* **Overfitting (High Variance)**:

  * Model is too complex.
  * Symptoms: Low training error but high test error.
  * Solution: Regularization, more training data, or simpler models.
