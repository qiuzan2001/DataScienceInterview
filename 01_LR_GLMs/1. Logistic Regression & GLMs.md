| Feature / Assumption         | OLS           | GLM      | Logistic Regression  |
| ---------------------------- | ------------- | -------- | -------------------- |
| Model Fit Method             | Least Squares | MLE      | MLE                  |
| Outcome Type                 | Continuous    | Flexible | Binary               |
| Linearity (outcome/logit)    | ✅             | ✅        | ✅ (in log-odds)      |
| Independence of observations | ✅             | ✅        | ✅                    |
| Constant error variance      | ✅             | ❌        | ❌                    |
| Normality of residuals       | ✅             | ❌        | ❌                    |
| No multicollinearity         | ✅             | ✅        | ✅                    |
| Correct distribution & link  | ❌             | ✅        | ✅ (binomial + logit) |
| Large sample needed          | ❌             | Depends  | ✅                    |

## [[1.1 Ordinary Least Squares (OLS)]] Recap

1. **Model form**

   $$
   Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \varepsilon_i
   $$

**Used for:** Continuous dependent variables.

**Key Assumptions:**

* **Linearity:** The relationship between predictors and outcome is linear.
* **Independence of Errors:** Residuals are uncorrelated across observations.
* **Homoscedasticity:** Constant variance of residuals.
* **Normality of Errors:** Residuals are normally distributed (important for inference).
* **No Perfect Multicollinearity:** Predictors aren’t perfectly correlated.
* **Exogeneity:** Predictors are uncorrelated with the error term.

🧠 *Purpose:* These ensure that OLS estimates are **Best Linear Unbiased Estimators (BLUE)**.


> **Why move beyond OLS?**
>
> * When $Y$ is binary or counts, residuals violate normality & homoscedasticity
> * Predictions can fall outside valid range (e.g.\ probabilities < 0 or > 1)

---

## [[1.2 Generalized Linear Models (GLMs)]] – Conceptual Leap

**Goal**: Model non-Normal $Y$ using a **link function** and appropriate distribution (exponential family).

**Used for:** Non-normal dependent variables (e.g., binary, count data).

**Three Components:**

1. **Random Component:** Outcome follows a distribution from the exponential family. $Y_i$ distribution (e.g.\ Binomial for binary, Poisson for counts)
2. **Systematic Component:** Predictors enter linearly (like in OLS). Linear predictor $\eta_i = \beta_0 + \sum \beta_j X_{ij}$
3. **Link Function:** Transforms the expected value of the outcome to a scale where it relates linearly to predictors. **Link**: $g(\mu_i) = \eta_i$, where $\mu_i = E[Y_i]$

**Key Assumptions:**

* Correct **distribution** and **link function**.
* **Independence of observations**.
* **No perfect multicollinearity**.

🧠 *GLMs generalize OLS by allowing non-normal outcomes and using a link function to model nonlinearity.*

---

## [[1.3 Introducing Logistic Regression]]

* **When?** Binary outcomes (0/1) and you need probabilities.
* **Link function**: **Logit**

  $$
    \text{logit}(p_i) \;=\; \log\!\bigl(p_i/(1-p_i)\bigr)
  $$

  * **What is a logit?** The log-odds of an event: transforms $p\in(0,1)$ to $\mathbb{R}$.
* **Model form**:

  $$
    \log\frac{p_i}{1-p_i}
    = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik}
  $$

  * Each $\beta_j$ is the **log-odds ratio** change per unit $X_j$.

**Key Assumptions:**

* **Binary Outcome**
* **Linearity of the Logit:** Log-odds are linearly related to predictors.
* **Independence of Observations**
* **No Perfect Multicollinearity**
* **Large Sample Size:** ≥10–20 cases per predictor (for rarest class).

❌ **Not Required:**

* Normality of predictors
* Homoscedasticity
* Linearity between raw predictors and probability

🧠 *Logistic regression is a GLM with a binomial distribution and a logit link.*

---

## [[1.4 Model Fitting]]: Least Squares vs. Maximum Likelihood Estimation

### 🔹 OLS: **Least Squares**

* **Goal:** Minimize the **sum of squared residuals**:

  $$
  \min \sum (y_i - \hat{y}_i)^2
  $$
* **Works best when:** Errors are normally distributed and have constant variance.
* **Solution:** Closed-form, solved directly with linear algebra.
* **Use case:** Continuous outcomes.

---

### 🔹 GLMs & Logistic Regression: **Maximum Likelihood Estimation (MLE)**

* **Goal:** Find parameters that **maximize the likelihood** of observing the data.
* **Logistic Regression Example:**

  $$
  \ell(\beta) = \sum \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
  $$
* **Solving method:** Iterative (e.g., Newton-Raphson).
* **Use case:** Binary, count, or other non-normal outcomes.


## [[1.5 Dealing with Unbalanced Samples]]

1. **Problem**: Rare positive class → model biased to majority.
2. **Strategies**:

   * **Resampling**:

     * **Oversample** minority (SMOTE, duplication)
     * **Undersample** majority
   * **Class weights**: Penalize misclassification of minority more heavily.
   * **Threshold tuning**: Pick decision threshold to balance precision/recall.
   * **Use AUROC / Precision–Recall curves** instead of raw accuracy.

---

## [[1.6 Estimation Issues & Separation]]

* **Complete separation**: A combination of predictors perfectly predicts class → MLE does not converge.
* **Quasi-separation**: Almost perfect → inflated SEs.
* **Remedies**:

  * **Penalized MLE** (Ridge / Lasso)
  * **Firth’s bias correction**
  * **Bayesian priors** on $\beta$s

---

## 1.7 Evaluation via Confusion Matrices & Metrics
[[6.3 Performance Metrics#^26c0e0]]

* **Confusion matrix**:

  $$
    \begin{array}{c|cc}
      & \text{Pred 0} & \text{Pred 1} \\ \hline
    \text{Actual 0} & TN & FP \\
    \text{Actual 1} & FN & TP
    \end{array}
  $$
* **Derived metrics**:

  * **Accuracy** $\tfrac{TP+TN}{\text{total}}$
  * **Precision** $\tfrac{TP}{TP+FP}$, **Recall** $\tfrac{TP}{TP+FN}$
  * **F1-Score** harmonic mean of precision & recall
  * **AUROC**: Probability model ranks a random positive above a random negative

---

## 1.8 GLM vs. Gradient Boosting Machines (GBM)

| Feature          | GLM                | GBM          |
| ---------------- | ------------------ | ------------ |
| Non-linearity    | No                 | Yes          |
| Interactions     | No                 | Yes (auto)   |
| Monotonicity     | Yes (link space)   | No           |
| Interpretability | High (odds ratios) | Lower        |
| Predictive power | Moderate           | Often higher |

> **Take-away**:
>
> * Use **GLMs** when interpretability, simple structure, and direct probability estimates matter.
> * Use **GBMs** when you need maximum predictive accuracy and can tolerate a “black-box.”

---

