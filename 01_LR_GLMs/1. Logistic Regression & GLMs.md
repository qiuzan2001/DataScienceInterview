## [[1.1 Ordinary Least Squares (OLS)]] Recap

1. **Model form**

   $$
   Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \varepsilon_i
   $$

| **Assumption**                          | **Description**                                             | **Diagnostic Tool**                                 |
| --------------------------------------- | ----------------------------------------------------------- | --------------------------------------------------- |
| **Independence & Normality**            | $Y_i$ i.i.d., $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ | Q–Q plot of residuals                               |
| **Linearity**                           | $E[Y_i]$ is linear in the predictors $X$                    | Residuals vs. fitted values, LOESS smoothing        |
| **Homoscedasticity**                    | Constant variance $\operatorname{Var}(\varepsilon_i)$       | Scale–location plot                                 |
| **No Multicollinearity**                | Predictors are not perfectly linearly related               | Variance Inflation Factor (VIF), correlation matrix |
| **Correct Model Specification**         | No omitted variables, correct functional form               | Residual analysis, AIC/BIC comparisons              |
| **No High Leverage/Influential Points** | No extreme outliers or leverage points                      | Cook’s distance, leverage statistics                |


> **Why move beyond OLS?**
>
> * When $Y$ is binary or counts, residuals violate normality & homoscedasticity
> * Predictions can fall outside valid range (e.g.\ probabilities < 0 or > 1)

---

## [[1.2 Generalized Linear Models (GLMs)]] – Conceptual Leap

1. **Goal**: Model non-Normal $Y$ using a **link function** and appropriate distribution (exponential family).
2. **Three components**:

   * **Random**: $Y_i$ distribution (e.g.\ Binomial for binary, Poisson for counts)
   * **Systematic**: Linear predictor $\eta_i = \beta_0 + \sum \beta_j X_{ij}$
   * **Link**: $g(\mu_i) = \eta_i$, where $\mu_i = E[Y_i]$

---

## [[1.3 Introducing Logistic Regression]]

* **When?** Binary outcomes (0/1) and you need probabilities.
* **Link function**: **Logit**

  $$
    \text{logit}(p_i) \;=\; \log\!\bigl(p_i/(1-p_i)\bigr)
  $$

  * **What is a logit?** The log-odds of an event: transforms $p\in(0,1)$ to $\mathbb{R}$.
* **Model form**:

  $$
    \log\frac{p_i}{1-p_i}
    = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik}
  $$

  * Each $\beta_j$ is the **log-odds ratio** change per unit $X_j$.

---

## [[1.4 GLM Assumptions]]

| Assumption            | Logistic GLM                          | Diagnostic                             |
| --------------------- | ------------------------------------- | -------------------------------------- |
| Response distribution | Binomial (exponential family)         | Check deviance residuals, Q–Q          |
| Link linearity        | $\eta$ linear in $X$                  | Residual vs.\ predictor plots, LOESS   |
| Multicollinearity     | Not inherently problematic, but still | VIF, correlation matrix                |
| Outliers & leverage   | Can unduly influence coefficients     | Cook’s distance, leverage diag         |
| Model specification   | Correct choice of predictors & link   | AIC/BIC comparisons, residual patterns |

---

## [[1.5 Dealing with Unbalanced Samples]]

1. **Problem**: Rare positive class → model biased to majority.
2. **Strategies**:

   * **Resampling**:

     * **Oversample** minority (SMOTE, duplication)
     * **Undersample** majority
   * **Class weights**: Penalize misclassification of minority more heavily.
   * **Threshold tuning**: Pick decision threshold to balance precision/recall.
   * **Use AUROC / Precision–Recall curves** instead of raw accuracy.

---

## [[1.6 Estimation Issues & Separation]]

* **Complete separation**: A combination of predictors perfectly predicts class → MLE does not converge.
* **Quasi-separation**: Almost perfect → inflated SEs.
* **Remedies**:

  * **Penalized MLE** (Ridge / Lasso)
  * **Firth’s bias correction**
  * **Bayesian priors** on $\beta$s

---

## 1.7 Evaluation via Confusion Matrices & Metrics
[[6.3 Performance Metrics#^26c0e0]]

* **Confusion matrix**:

  $$
    \begin{array}{c|cc}
      & \text{Pred 0} & \text{Pred 1} \\ \hline
    \text{Actual 0} & TN & FP \\
    \text{Actual 1} & FN & TP
    \end{array}
  $$
* **Derived metrics**:

  * **Accuracy** $\tfrac{TP+TN}{\text{total}}$
  * **Precision** $\tfrac{TP}{TP+FP}$, **Recall** $\tfrac{TP}{TP+FN}$
  * **F1-Score** harmonic mean of precision & recall
  * **AUROC**: Probability model ranks a random positive above a random negative

---

## 1.8 GLM vs. Gradient Boosting Machines (GBM)

| Feature          | GLM                | GBM          |
| ---------------- | ------------------ | ------------ |
| Non-linearity    | No                 | Yes          |
| Interactions     | No                 | Yes (auto)   |
| Monotonicity     | Yes (link space)   | No           |
| Interpretability | High (odds ratios) | Lower        |
| Predictive power | Moderate           | Often higher |

> **Take-away**:
>
> * Use **GLMs** when interpretability, simple structure, and direct probability estimates matter.
> * Use **GBMs** when you need maximum predictive accuracy and can tolerate a “black-box.”

---

