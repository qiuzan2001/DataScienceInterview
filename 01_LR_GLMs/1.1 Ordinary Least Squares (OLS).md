

### 1. Model Form

$$
Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \varepsilon_i
$$

* $Y_i$ is the response for observation $i$.
* $X_{ij}$ are the $k$ predictors.
* $\beta_j$ are the unknown coefficients we wish to estimate.
* $\varepsilon_i$ is the “error” or “noise” – the part of $Y_i$ not explained by the linear combination of $X$s.

---

### 2. The OLS Estimator: Minimizing Squared Errors

We choose $\boldsymbol\beta=(\beta_0,\dots,\beta_k)'$ to minimize

$$
S(\boldsymbol\beta)
=\sum_{i=1}^n \bigl(Y_i - \beta_0 - \sum_{j=1}^k \beta_j X_{ij}\bigr)^2
=\|\mathbf Y - \mathbf X\boldsymbol\beta\|^2.
$$

Taking gradients and setting to zero gives the **normal equations**

$$
\mathbf X'(\mathbf Y - \mathbf X\widehat{\boldsymbol\beta})=0
\quad\Longrightarrow\quad
\widehat{\boldsymbol\beta}
=(\mathbf X'\mathbf X)^{-1}\mathbf X'\mathbf Y.
$$

* Here $\mathbf X$ is the $n\times (k+1)$ design matrix (including a column of 1s).
* This solution is unique provided $\mathbf X'\mathbf X$ is invertible (i.e.\ no perfect multicollinearity).

---

### 3 Assumptions: Logic & Mathematical Consequences 
[[1.7 Assumptions]]

| Assumption                           | Role                                                        | Consequence                                                  | Example (Exam vs. Hours)                                                           |
| ------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------------------------------- |
| **A1. Linearity**                    | $E[Y\mid X]=\beta_0+\beta_1X$                               | Model can capture the true mean relationship                 | Each extra hour adds exactly 5 points on average                                   |
| **A2. Zero-mean errors**             | $E[\varepsilon\mid X]=0$                                    | $E[\widehat\beta]=\beta$ (unbiasedness)                      | Among all 4-hour students, residuals average to zero                               |
| **A3. Homoscedasticity**             | $\mathrm{Var}(\varepsilon\mid X)=\sigma^2$                  | $\mathrm{Var}(\widehat\beta)=\sigma^2(X'X)^{-1}$ (valid SEs) | All students—whether low or high study time—have \~±8 points scatter               |
| **A4. Independence**                 | $\mathrm{Cov}(\varepsilon_i,\varepsilon_j)=0$ for $i\neq j$ | Combined with A3 ⇒ valid covariance of $\widehat\beta$       | No copying: each student’s “luck” is their own                                     |
| **A5. Normality of errors**          | $\varepsilon\sim N(0,\sigma^2)$                             | $\widehat\beta$ is exactly Gaussian ⇒ exact $t$/$F$ tests    | Residual histogram is bell-shaped around zero                                      |
| **A6. No perfect multicollinearity** | No exact linear combo among $X$s                            | $(X'X)^{-1}$ exists ⇒ unique coefficients                    | Don’t include both “hours” and “minutes” studied                                   |
| **A7. Correct specification**        | All relevant predictors in right form                       | Omitting $Z$ ⇒ bias $=(X'X)^{-1}X'Z\,\gamma$                 | Leaving out attendance (correlated with hours) inflates “points per hour” estimate |


---

### 4. Diagnostic Tools: Purpose & Implementation

| Diagnostic Tool                | Assumption Checked               | Purpose (Goal)                                      |
| ------------------------------ | -------------------------------- | --------------------------------------------------- |
| **Residuals vs. fitted plot**  | A1. Linearity                    | Confirm $E[Y]$ is linear in $X$                     |
| **Scale–location plot**        | A3. Homoscedasticity             | Check constant error variance                       |
| **Leverage & Cook’s distance** | A4. Independence (and A3)        | Identify observations that unduly influence the fit |
| **Q–Q plot of residuals**      | A5. Normality of errors          | Verify $\varepsilon_i\sim N(0,\sigma^2)$            |
| **Variance Inflation Factor**  | A6. No perfect multicollinearity | Detect near-linear dependence among predictors      |


---

### 5. Why Move Beyond OLS for Binary/Count Outcomes

1. **Non-constant variance & non-Normal errors**

   * A binary $Y\in\{0,1\}$ has $\operatorname{Var}=\!p(1-p)$, so errors are heteroscedastic and Bernoulli, not Gaussian.

2. **Predictions can leave the feasible range**

   * OLS may predict $\widehat Y<0$ or $\widehat Y>1$, which is invalid for probabilities.

3. **Inefficiency & invalid inference**

   * Standard errors and $p$-values rely on Normal/constant-variance assumptions.
   * GLMs (e.g.\ logistic for binary, Poisson for counts) build in the correct distribution and link, giving valid inference.

---

**Take-Home:**
OLS is powerful when its assumptions hold, but with bounded, non-Normal, or heteroscedastic outcomes you need the GLM framework and the right link/distribution.
