## 1. Why Not Just OLS on a 0/1 Outcome?

Imagine you tried to predict a binary outcome $Y_i\in\{0,1\}$ (say “did the student pass?”) by fitting

$$
\hat Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik}.
$$

You’d quickly run into two problems:

1. **Predictions outside $[0,1]$.**  A linear fit might give $\hat Y<0$ or $\hat Y>1$, which makes no sense as a probability.
2. **Heteroscedasticity & non‐Normal errors.**  The variance of a Bernoulli $Y$ is $p(1-p)$, which changes with $p$.  OLS assumes constant variance and Normally distributed errors—both broken here.

We need a model that:

1. Always spits out values in $(0,1)$.
2. Respects the way variance actually behaves for 0/1 data.
3. Lets us still use linear predictors $X\beta$ in some transformed world.

---

## 2. The Logit Link: From Probability to the Real Line

### 2.1 Odds and Log‐Odds

* **Probability** $p_i = P(Y_i=1\mid X_i)$ lives in $[0,1]$.
* **Odds**: $\displaystyle \frac{p_i}{1-p_i}$.  This is how much more likely “1” is versus “0.”

  * If $p=0.8$, odds $=0.8/0.2 = 4$: four to one.
  * As $p\to 1$, odds $\to\infty$.  As $p\to 0$, odds $\to0$.

But odds are still **positive** only.  We’d like a quantity on the whole real line so we can tie it to $X\beta$.

* **Log‐odds (logit)**:

  $$
    \text{logit}(p)
    = \log\!\Bigl(\tfrac{p}{1-p}\Bigr).
  $$

  This maps $p\in(0,1)$ one‐to‐one onto $\mathbb R$.

  * If $p=0.5$, logit $=\log(1)=0$.
  * If $p>0.5$, logit $>0$.
  * If $p<0.5$, logit $<0$.

Visually, the logit “stretches” probabilities near 0 and 1 out toward large negative or positive values, respectively.

---

## 3. Logistic Regression Model Form

Putting it together, **logistic regression** says:

$$
\boxed{
  \log\!\Bigl(\tfrac{p_i}{1-p_i}\Bigr)
  = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik}.
}
$$

* **Left-hand side**: log‐odds of “success.”
* **Right-hand side**: the usual **linear predictor** $\eta_i = X_i\beta$.

Equivalently, solving for $p_i$ gives the **inverse‐logit** or **sigmoid**:

$$
p_i 
= \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}
= \frac{1}{1 + \exp(-\eta_i)}.
$$

This guarantees $p_i\in(0,1)$.

---

## 4. Interpreting the Coefficients

Each $\beta_j$ measures how a one‐unit increase in $X_j$ changes the **log‐odds**:

$$
\beta_j 
= \text{change in }\log\!\bigl[p/(1-p)\bigr]
  \quad\text{per 1‐unit change in }X_j.
$$

More concretely:

* If $\beta_j = 0.7$, then raising $X_j$ by 1 multiplies the **odds** by $e^{0.7}\approx2.01$.
* If $\beta_j = -1.2$, a one‐unit increase in $X_j$ multiplies the odds by $e^{-1.2}\approx0.30$, i.e. cuts the odds to 30% of their previous value.

You can always convert back to probabilities.  For small $|\beta_j|$, $e^{\beta_j}-1$ is roughly the **percentage change** in odds.
[[1.5 Dealing with Unbalanced Samples]]

---

## 5. Fitting the Model: Maximum Likelihood & IRLS

Unlike OLS, there is no closed‐form “$(X'X)^{-1}X'y$” for logistic regression.  Instead:

1. **Write down the likelihood**
   For independent $Y_i\sim\text{Bernoulli}(p_i)$:

   $$
     L(\beta)
     = \prod_{i=1}^n p_i^{Y_i}(1-p_i)^{1-Y_i},
     \quad
     p_i = \frac{1}{1+\exp(-X_i\beta)}.
   $$

2. **Take logs**

   $$
     \ell(\beta)
     = \sum_{i=1}^n \Bigl[Y_i\log p_i + (1-Y_i)\log(1-p_i)\Bigr].
   $$

3. **Solve for $\beta$ by setting the gradient (“score”) to zero.**
   This yields nonlinear equations.  We solve them by **iteratively reweighted least squares (IRLS)**:

   * At each step, approximate the log‐likelihood by a second‐order Taylor expansion
   * This reduces to a **weighted least squares** update
   * Repeat until convergence (changes in $\beta$ become tiny)

Most software does this under the hood and reports:

* **Coefficients** $\widehat\beta$
* **Standard errors** (from the observed information matrix)
* **Wald tests**, **likelihood‐ratio tests**, **deviance**, etc.

---

## 6. Connecting Back to GLM

Logistic regression is the special case of a GLM for a Bernoulli outcome, with

1. **Exponential‐family form**
   $\theta = \log(p/(1-p))$, $b(\theta)=\log(1+e^\theta)$, $\phi=1$.

2. **Canonical link**
   $g(\mu) = \theta$, i.e. $\text{logit}(p) = X\beta$.

Because it uses the canonical link, all the nice IRLS simplifications apply directly.

---

## 7. Why Logistic Regression “Makes Sense” [[1.7 Assumptions]]

1. **Probabilities in range.**  The sigmoid output is always between 0 and 1.
2. **Variance modeled correctly.**  A Bernoulli’s variance $p(1-p)$ is built into the likelihood, so inference is valid.
3. **Interpretability.**  Coefficients are log‐odds ratios, which have a clear practical meaning for “how much more likely.”
4. **Computationally tractable.**  IRLS converges quickly for most problems.

