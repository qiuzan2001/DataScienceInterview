
### **Study Note: Understanding Model Assumptions**

In statistics, models are like tools. To use them correctly and trust their results, you must understand their operating rules, or **assumptions**. Getting these wrong is like using a wrench to hammer a nail—you might get a result, but it won't be the right one. This note breaks down the core assumptions for three fundamental models: Ordinary Least Squares (OLS) Regression, Generalized Linear Models (GLMs), and Logistic Regression.

---

### 1. Ordinary Least Squares (OLS) Regression: The Classic Linear Model

OLS is your starting point for predictive modeling. Its goal is to model a **continuous** outcome (e.g., house price, temperature) by finding the "best" straight line through the data.

**Core Logic:** OLS finds the line that **minimizes the sum of the squared residuals**. A "residual" (or error, denoted by $\epsilon$) is simply the vertical distance between an actual data point and the regression line. By squaring these distances, we treat positive and negative errors equally and penalize larger errors more heavily. The model equation is:

$Y = \beta_0 + \beta_1X + \epsilon$

For the estimates of $\beta_0$ (intercept) and $\beta_1$ (slope) to be the Best Linear Unbiased Estimates (BLUE), the following assumptions must hold.

#### **OLS Assumptions Explained**

* **Linearity:** The relationship between the predictors ($X$) and the outcome ($Y$) must be linear.
    * **Why it Matters:** The model itself is a line, so it can only capture a linear trend. If the true relationship is a curve (e.g., quadratic), the line will be a poor fit, and your predictions will be systematically wrong. You're forcing a straight peg into a round hole.

* **Independence of Errors:** The residuals ($\epsilon$) must be independent of each other.
    * **Why it Matters:** This means that the error for one data point tells you nothing about the error for another. This is often violated in time-series data (e.g., today's stock price error might be related to yesterday's). If errors are correlated, your model will be overly confident. It will underestimate the true uncertainty, leading to standard errors that are too small and p-values that are misleadingly significant.

* **Homoscedasticity (Constant Error Variance):** The variance of the errors must be the same across all levels of the predictors.
    * **Why it Matters:** Imagine a scatterplot with the regression line. The data points should form a cloud of roughly equal vertical spread all along the line. The opposite is **heteroscedasticity**, where the spread fans out (or in). If this happens, your coefficient estimates are still unbiased, but their standard errors are wrong. This skews hypothesis tests, meaning you might conclude a variable is statistically significant when it's not, or vice versa.

* **Normality of Errors:** The residuals ($\epsilon$) should be normally distributed with a mean of zero.
    * **Why it Matters:** This assumption is key for **inference**—calculating reliable confidence intervals and p-values. Thanks to the Central Limit Theorem, this assumption is less critical in large samples. However, in smaller samples, violating it can make your significance tests inaccurate. Note: it's the *errors* that need to be normal, not the variables themselves!

* **No Perfect Multicollinearity:** The independent variables cannot be perfectly correlated with each other.
    * **Why it Matters:** Imagine trying to explain a person's weight using their height in inches *and* their height in centimeters. Since these two predictors are perfectly correlated, the model can't tell which one is responsible for the effect. It's mathematically impossible to isolate their individual impacts, and the model will fail. In the more common case of high (but not perfect) multicollinearity, the model becomes unstable. Coefficient estimates can swing wildly with small changes in the data, and their standard errors become huge, making it hard to assess their true importance.

* **Exogeneity:** The predictors must be uncorrelated with the error term.
    * **Why it Matters:** This is arguably the most important assumption. The error term represents everything else that affects $Y$ but isn't in your model. If a predictor ($X$) is correlated with this error term, it means there's some unmeasured factor (an "omitted variable") that is influencing both your predictor and your outcome. This creates **omitted variable bias**, and your model will incorrectly attribute the effect of the unmeasured factor to the predictor you included, giving you a biased coefficient.

---

### ## 2. Generalized Linear Models (GLMs): The Flexible Successor

What if your outcome isn't a continuous number on an infinite scale? What if it's a "yes/no" answer, or a count of events? OLS breaks down here. GLMs "generalize" the linear model to handle these situations.

**Core Logic:** GLMs introduce flexibility through three components.

1.  **Random Component:** This specifies the probability distribution of the outcome variable ($Y$), chosen from the exponential family.
    * **The Logic:** Instead of forcing a Normal distribution (like OLS), you choose a distribution that makes sense for your data. For a binary yes/no outcome, you'd use the **Binomial** distribution. For a count of events (e.g., number of emails received per hour), you'd use the **Poisson** distribution.

2.  **Systematic Component:** This is the familiar linear combination of predictors. It's called the **linear predictor** ($\eta$, eta).
    * $\eta = \beta_0 + \beta_1X_1 + \dots + \beta_kX_k$
    * **The Logic:** This part is the same as in OLS. GLMs assume that the relationship is linear *on some scale*, even if not on the original scale of the outcome.

3.  **Link Function, $g(.)$:** This is the crucial bridge. It connects the *expected value* (mean) of the outcome to the linear predictor.
    * $g(E[Y]) = \eta$
    * **Why it's Needed (The "Aha!" Moment):** Let's say your outcome is binary (0 or 1), so its expected value, $E[Y]$, is a probability, $p$, which must be between 0 and 1. Your linear predictor, $\eta$, can be any real number from -∞ to +∞. You can't just set $p = \eta$ because the model could predict a probability of 1.5 or -0.2, which is nonsense. The link function's job is to transform the constrained range of your mean (e.g., [0,1] for probability) to the unconstrained range [-∞, +∞] of the linear predictor. This ensures that the model always produces valid predictions on the original scale.

By combining these, GLMs can model a wide variety of outcomes while relaxing the strict OLS assumptions of normality and homoscedasticity. However, they still assume **independence of observations** and **no perfect multicollinearity**.

---

### ## 3. Logistic Regression: The Premier Model for Binary Outcomes

Logistic Regression is not a different type of model; it is a **specific and very common type of GLM**. It is the go-to model when your outcome variable is binary.

**Core Logic:** Let's see how it fits the GLM framework.

* **Random Component:** The outcome follows a **Binomial distribution**, as it represents a series of success/failure trials.
* **Systematic Component:** The standard linear predictor, $\eta = \beta_0 + \beta_1X_1 + \dots$
* **Link Function:** The **Logit function**. This is the key to understanding logistic regression.

#### **Deep Dive into the Logit Link**

1.  We want to model the probability of success, $P(Y=1) = p$.
2.  We calculate the **odds**: This is the ratio of the probability of success to the probability of failure.
    * $Odds = \frac{p}{1-p}$
    * While $p$ is bounded by [0, 1], the odds are bounded by [0, +∞). We're closer, but still not unbounded.
3.  We take the natural logarithm of the odds to get the **log-odds**, or **logit**.
    * $logit(p) = ln(\frac{p}{1-p})$
    * The log-odds *are* unbounded, ranging from -∞ to +∞. This makes it a perfect match for the linear predictor, $\eta$.

This gives us the core equation of logistic regression:

$ln(\frac{p}{1-p}) = \beta_0 + \beta_1X_1 + \dots$

#### **Logistic Regression Assumptions Explained**

* **Binary Outcome:** The dependent variable must be dichotomous (e.g., pass/fail, alive/dead).
* **Linearity of Log-Odds:** This is the most crucial assumption. It means that a one-unit change in a predictor variable results in a constant, linear change in the *log-odds* of the outcome. The relationship between the predictor and the *probability* itself is not linear; it's an S-shaped (sigmoid) curve, which is exactly what we want when modeling probabilities.
* **Independence of Observations:** Same as OLS. Each observation should be an independent event.
* **Absence of Perfect Multicollinearity:** Same as OLS. Predictors should not be perfectly correlated.
* **Large Sample Size:** Logistic regression uses a different method to find the coefficients called Maximum Likelihood Estimation (MLE). MLE is an iterative process that works best with more data. A common guideline is to have at least 10-20 observations of the less frequent outcome for each predictor variable you include. Without enough data, the estimates can be unstable.