
## 🎯 **Big Picture Overview**

**Problem**: Given an $n \times p$ matrix $\mathbf{X}$ with potentially correlated variables, partition the $p$ variables into $K$ disjoint clusters where:
- Each cluster represents a single underlying trait
- Traits are as independent as possible

**Key Insight**: Instead of rotating data space axes (like PCA), VARCLUS groups the axes themselves.

---

## 🔧 **Core Mechanics**

### **Cluster Representation Methods**

| Method                  | Definition                                                      | Properties                                                             |
| ----------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **Principal** (default) | First principal component of correlation matrix $\mathbf{R}_C$  | • Maximizes captured variance<br>• Variable loadings vary in magnitude |
| **Centroid**            | Simple (signed) average: $z_j = \frac{1}{C} \sum_{i \in C} x_i$ | • Equal weighting of all variables<br> • More robust to outliers       |

> **Note**: All components are standardized to variance = 1 for comparable diagnostics

### **Critical Diagnostics**

**For each variable $x_i$:**

1. **Own-cluster $R^2$**: $R_i^2 = \operatorname{corr}^2(x_i, z_{C(i)})$
2. **Nearest-other $R^2$**: $R_{i,\text{next}}^2 = $ largest squared correlation with any other cluster component
3. **1-$R^2$ Ratio**: $\rho_i = \frac{1-R_i^2}{1-R_{i,\text{next}}^2}$

**Interpretation of 1-$R^2$ Ratio:**
- Numerator = residual variance if variable stays in current cluster
- Denominator = residual variance if it moves to best alternative cluster
- **$\rho_i > 1$** → Variable would fit better elsewhere

**For each cluster:**
- **$\lambda_2$** (second eigenvalue of $\mathbf{R}_C$): Measures leftover common variance
- **$\lambda_2 > 1$** → Cluster contains another full dimension → candidate for splitting

---

## 🔄 **The Divisive Algorithm**

### **Step-by-Step Process**

1. **Initialize**: All variables in one giant cluster

2. **Find worst-fitting variable**: $i^* = \arg\max_i \rho_i$

3. **Check cluster homogeneity**: 
   - If $\lambda_2 \leq \text{threshold}$ (default = 1) → Stop (cluster is homogeneous)
   - If $\lambda_2 > \text{threshold}$ → Split the cluster

4. **Execute split**:
   - Find eigenvector $\mathbf{v}$ associated with $\lambda_2$
   - Positive loadings → Cluster A
   - Negative loadings → Cluster B

5. **Reassign variables**: Move each variable to cluster with highest $R^2$

6. **Iterate**: Repeat until stopping criteria met

### **Stopping Rules**
- $\lambda_2 \leq \text{MAXEIGEN}$ for all clusters
- Maximum number of clusters $K_{\max}$ reached

---

## 💡 **Why It Works: Intuitive Proof**

1. **Problem Detection**: Low $R^2$ identifies poorly-fit variables; highest $\rho_i$ pinpoints the clearest symptom

2. **Mathematical Foundation**: 
   - Total cluster variance = $\lambda_1 + \lambda_2 + \ldots$
   - $\lambda_1$ is captured by cluster component
   - $\lambda_2 > 1$ means $\geq 1$ additional factor exists

3. **Optimal Splitting**: Eigenvector direction maximally separates hidden factors

4. **Greedy Improvement**: Reassignment increases within-cluster $R^2$ and reduces average $\rho_i$

---

## ⚖️ **Principal vs. Centroid Comparison**

| Aspect | Principal | Centroid |
|--------|-----------|----------|
| **Weighting** | Optimal (variance-maximizing) | Equal weights |
| **Outlier Sensitivity** | Moderate | Lower |
| **Interpretability** | Variable loadings | Equal influence |
| **Computational Cost** | Eigen-decomposition | Simple arithmetic |

**Use Principal** when statistical efficiency matters or variables contribute unequally  
**Use Centroid** when robustness and interpretability are priorities

---

## 📊 **Key Diagnostic Plots**

1. **Histogram of $\rho_i$** → Spots misfit variables
2. **$\lambda_2$ across clusters** → Identifies clusters needing splits
3. **Inter-cluster correlations** → Should approach zero

---

## 🎯 **Master These Three Statistics**

| Statistic | What It Measures | Key Threshold |
|-----------|------------------|---------------|
| **Component Loadings** | Variable-cluster relationships | Higher = stronger association |
| **Second Eigenvalue ($\lambda_2$)** | Remaining cluster dimensionality | $>1$ = needs splitting |
| **1-$R^2$ Ratio ($\rho_i$)** | Variable misplacement | $>1$ = better fit elsewhere |

---

## 🔑 **Key Takeaways**

✅ **Cluster component = surrogate factor**  
✅ **$\lambda_2 > 1$ = mathematical red flag for hidden dimensions**  
✅ **1-$R^2$ ratio pinpoints where hidden dimensions leak out**  
✅ **Split → reassign drives toward internal unidimensionality and external orthogonality**  

---

## 📝 **Quick Review Questions**

1. What does $\lambda_2 > 1$ indicate about a cluster?
2. How is the 1-$R^2$ ratio interpreted?
3. What's the key difference between Principal and Centroid methods?
4. Why does the algorithm split along the second eigenvector?

---

## 🧮 **Mathematical Summary**

**Core Objective Function:**
$$\text{Maximize within-cluster } R^2 \text{ while minimizing between-cluster correlations}$$

**Splitting Criterion:**
$$\text{Split cluster } C \text{ if } \lambda_2(\mathbf{R}_C) > 1$$

**Variable Reassignment Rule:**
$$x_i \rightarrow C^* \text{ where } C^* = \arg\max_C R^2(x_i, z_C)$$

**Convergence Condition:**
$$\forall C: \lambda_2(\mathbf{R}_C) \leq \text{MAXEIGEN}$$

---

**Final Note**: Master these core concepts and you understand VARCLUS. Everything else is implementation detail.