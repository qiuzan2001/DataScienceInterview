https://zhuanlan.zhihu.com/p/38052390

## 1  Why Do We Need PCA?  — The Motivation

| Pain point (in high-dimensional data)                                                                            | How PCA helps                                                                                   | Intuitive picture                                                                                          |
| ---------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| **Curse of dimensionality**: samples are sparse, distance measures get noisy, algorithms slow down.              | Finds a *lower-dimensional subspace* that preserves as much variance (information) as possible. | Imagine flattening a thick “point cloud” along its thinnest directions so most of the spread stays intact. |
| **Correlated features** break methods that assume independence (e.g., naive Bayes) and inflate model parameters. | Produces **uncorrelated** (orthogonal) axes—the principal components.                           | Rotating the coordinate system until the cloud’s axes line up with its widest directions.                  |
| **Visualization**: humans can only look at 2-D or 3-D plots.                                                     | Compresses data to 2–3 components for plotting without arbitrary feature picks.                 | Like shining a flashlight on the cloud and looking at the shadow that keeps as much shape as possible.     |

---

## 2  Mathematical Core: “Maximizing Projected Variance”

### 2.1 Problem Statement

Given centered data matrix $\tilde{\mathbf{X}}\in\mathbb{R}^{n\times p}$, find a unit vector $\mathbf{w}_1$ (the **first principal axis**) that maximizes the projected variance:

$$
\mathbf{w}_1
=\arg\max_{\|\mathbf{w}\|=1}
\;\mathrm{Var}\bigl(\tilde{\mathbf{X}}\mathbf{w}\bigr)
=\arg\max_{\|\mathbf{w}\|=1}
\mathbf{w}^\top\mathbf{S}\mathbf{w},
$$

where $\mathbf{S}=\frac{1}{n-1}\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}$ is the sample covariance.

### 2.2 Lagrange Multiplier Trick

Maximize under $\|\mathbf{w}\|=1$:

$$
\mathcal{L}(\mathbf{w},\lambda)
=\mathbf{w}^\top\mathbf{S}\mathbf{w}
-\lambda\bigl(\mathbf{w}^\top\mathbf{w}-1\bigr).
$$

Set $\nabla_\mathbf{w}\mathcal{L}=0$ ⇒ $\mathbf{S}\mathbf{w}=\lambda\mathbf{w}$.
So **principal axes are eigenvectors of the covariance;** variance captured equals the eigenvalue.

### 2.3 Higher Components

Add orthogonality constraints $\mathbf{w}_k^\top \mathbf{w}_j =0$ for $j<k$. By induction, the $k$-th axis is the eigenvector with the $k$-th largest eigenvalue. Thus

$$
\underbrace{\mathbf{S}\mathbf{W}}_{\text{covariance × eigenvectors}}
=\underbrace{\mathbf{W}\boldsymbol{\Lambda}}_{\text{eigenvectors × diag(eigenvalues)}}.
$$

### 2.4 Equivalence to Minimum Reconstruction Error (SVD view)

PCA also solves

$$
\min_{\operatorname{rank}(\hat{\mathbf{X}})=k}
\|\tilde{\mathbf{X}}-\hat{\mathbf{X}}\|_F^2.
$$

SVD says $\tilde{\mathbf{X}} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top$.
Taking the first $k$ singular triplets yields the same subspace as the top $k$ eigenvectors of $\mathbf{S}$ (because $\mathbf{S}=\mathbf{V}\boldsymbol{\Sigma}^2\mathbf{V}^\top/(n-1)$).

> **Take-away**: “Max-variance” and “min-error” stories describe *one* optimization.

---

## 3  Geometric & Probabilistic Intuition

1. **Geometric**

   * Data cloud ≈ p-D ellipsoid.
   * Principal axes align with longest radii; lengths = √eigenvalues.
   * Each new axis is orthogonal, so we’re rotating—not shearing or scaling.

2. **Probabilistic**

   * Assume data come from $ \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$.
   * PCA gives the maximum-likelihood estimate of a *factor-analysis* model if factor loadings are forced orthogonal and noise is isotropic.
   * Hence PCA ≈ a *very constrained* latent-variable model (no unique mapping to original features—explains the interpretability caveat later).

---

## 4  Algorithmic Recipe (Matrix Form)

| Step                                                                                                           | What & Why                                                                                    |
| -------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| **1. Center (and often standardize) data**<br> $\tilde{\mathbf{X}}=\mathbf{X}-\mathbf{1}\boldsymbol{\mu}^\top$ | PCA assumes mean = 0; standardization neutralizes differing units.                            |
| **2. Obtain covariance or SVD**                                                                                | Either compute $\mathbf{S}$ or call a thin‐SVD routine (preferred if $n\gg p$ or vice-versa). |
| **3. Eigendecompose / SVD**                                                                                    | Get eigenvalues $\lambda_i$ and eigenvectors $\mathbf{w}_i$.                                  |
| **4. Choose number $k$**                                                                                       | (a) Cumulative explained variance ratio (EVR) ≥ 80–95 %;<br>(b) Scree plot “elbow”.           |
| **5. Compute scores**<br>$\mathbf{Z}=\tilde{\mathbf{X}}\mathbf{W}_k$                                           | Rows = observations in PC space; use them for downstream tasks.                               |
| **6. (Optional) Reconstruct**<br>$\hat{\mathbf{X}}=\mathbf{Z}\mathbf{W}_k^\top+\boldsymbol{\mu}$               | Useful for denoising or compression benchmarking.                                             |

---

## 5  Practical Tips & Limitations

| Theme                 | What to watch for                                             | Remedies / Variants                                                                      |
| --------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **Linearity**         | PCA only captures linear structure.                           | Kernel PCA, t-SNE, UMAP for nonlinear manifolds.                                         |
| **Scale Sensitivity** | Big-variance features dominate.                               | Standardize (z-score) before PCA.                                                        |
| **Outliers**          | Extreme points skew covariance.                               | Robust PCA, trimming, Winsorizing.                                                       |
| **Interpretability**  | PCs are mixtures of raw features.                             | Examine *loadings* (elements of $\mathbf{W}$); use Sparse PCA to zero out small weights. |
| **Sparsity Need**     | Sometimes you want few non-zero loadings (feature selection). | Sparse PCA, Lasso or Elastic-Net penalties on loadings.                                  |

---

## 6  Comparison With Related Techniques

| Method               | Optimizes                                                               | Orthogonality?        | Supervised? | Typical Use                                            |
| -------------------- | ----------------------------------------------------------------------- | --------------------- | ----------- | ------------------------------------------------------ |
| **PCA**              | Max variance / min reconstruction error                                 | Yes                   | ❌           | Unsupervised baseline, preprocessing, visualization    |
| **LDA (Fisher)**     | Max class separability (ratio of between-class to within-class scatter) | No                    | ✅           | Dimensionality reduction *for classification*          |
| **Ridge Regression** | Min squared error + L2 penalty                                          | N/A                   | ✅           | Regularized prediction, *not* dimensionality reduction |
| **Factor Analysis**  | Max log-likelihood under latent Gaussian model                          | Axes *can* be rotated | ❌           | Discover latent constructs; allows correlated factors  |

---

## 7  Worked Micro-Example (No Code, Only Logic)

1. **Toy data**: 100 students × 3 features = \[math score, physics score, writing score].
2. **Center & standardize** → zero-mean, unit-variance.
3. **Covariance**: correlations say math & physics highly correlated (0.85).
4. **Eigenvalues**: $\lambda_1=2.3$, $\lambda_2=0.6$, $\lambda_3=0.1$.

   * EVR for PC-1 = 2.3 / 3 = 76 %.
   * Cumulative (PC-1 + 2) = 97 % ⇒ keep 2 components.
5. **Loading vectors**:

   * PC-1 ≈ \[0.71, 0.70, 0.06] → “quantitative” axis.
   * PC-2 ≈ \[0.05, −0.14, 0.99] → “writing vs STEM” contrast.
6. **Interpretation**: One can plot students on PC-1 & PC-2, cluster study profiles, or feed PCs into k-means.

---

## 8  Summary of Key Ideas

1. **Core insight**: Rotate the axes so that **variance is concentrated in as few directions as possible**; those directions are eigenvectors of the covariance matrix.
2. Two equivalent lenses:

   * **Max-variance projection.**
   * **Min-error reconstruction (rank-$k$ SVD).**
3. **Operational checklist**: Center → (optionally scale) → SVD → choose $k$ → inspect EVR & loadings → use scores or reconstruct.
4. **Watch‐outs**: linearity, scaling, outliers, and interpretability.
5. **Why still learn PCA today?** It remains the **baseline yard-stick**—if a fancy nonlinear method can’t beat PCA in cross-validation or visualization clarity, revisit your assumptions!

Absolutely — here's an additional section that critically examines **why PCA, despite its theoretical elegance, is often less useful in real-world business or applied feature-engineering settings**.

---

## 9  When PCA Is *Not* Very Useful: The Feature Space & Business Perspective

While PCA is a mathematically rigorous and elegant tool, it’s not always the right choice in practice — especially when interpretability and actionability of features matter. Here's why:

### 9.1 Loss of Feature Semantics — No Direct Meaning of PCs
   
   * **Problem:** Each principal component (PC) is a **linear combination** of many (often all) original features.
   * In business contexts, features such as `age`, `income`, `education_level`, etc. **carry clear real-world meaning**.
   * After PCA, PC₁ might look like
   
     $$
     \text{PC}_{1}=0.41\,\text{income}-0.33\,\text{credit\_score}+0.12\,\text{age}+\dots
     $$
   
     — a mixture that is hard to **interpret** or explain to stakeholders.
   * **Why it matters:** In domains like finance, healthcare, or HR analytics, **decisions and policies are built around the original features**, not around abstract linear blends.
   
---

### 9.2 **Hard to Trace Back to Feature Importance**

* Business analysts often ask:

  > “Which features are driving the model’s prediction or classification?”

* With PCA:

  * You can’t say "credit score is important" anymore — it's now buried inside a component.
  * Even loadings are hard to interpret unless you're already deeply familiar with the domain.

---

### 9.3 **Transformation Breaks Business Rules**

* Many workflows involve **rules** or **thresholds** on original features:

  * e.g., “Reject loan if `income < $30,000`” or “Send promotion to users aged 25–34.”

* PCA **destroys this structure** — components are continuous, mixed axes with no clear decision thresholds.

* Regulatory and compliance settings may *require* feature transparency.

---

### 9.4 **Overkill When Features Are Already Good**

* In many modern datasets:

  * Features are already well-engineered (e.g., from domain expertise or prior feature selection).
  * Correlations may be weak or manageable.
  * Feature count $p$ is moderate, and interpretability matters more than compactness.

* Applying PCA might **degrade model performance** or **obfuscate useful information**, especially in tree-based models (like XGBoost or Random Forests) that naturally handle feature correlations.

---

### 9.5 **Incompatibility with Real-Time Systems**

* PCA is a **global transformation**: computing components requires access to the full training data (or covariance structure).
* In streaming, real-time, or online-learning settings, this can be infeasible or costly.
* Also, **re-training PCA after feature drift** is non-trivial and may cause instability in downstream models.

---

## Summary: PCA ≠ Always Useful in Practice

| Practical Concern                                    | PCA Weakness                                               |
| ---------------------------------------------------- | ---------------------------------------------------------- |
| Need to understand, act on, or report feature impact | PCs lose original semantics                                |
| Rule-based business logic                            | PCs don’t support thresholding or categorical segmentation |
| Real-time inference                                  | Global transform is hard to update or maintain             |
| Sparse features or one-hot encoding                  | PCA can “smear” discrete signal                            |
| Tree-based models                                    | Don’t need PCA; they handle correlated input well          |

---

### So When Should You Still Use PCA?

✅ **Exploratory analysis** (e.g., to visualize clusters or reduce noise)
✅ **Preprocessing for linear models** when feature multicollinearity is severe
✅ **Unsupervised anomaly detection**
✅ **Compression** in imaging or signal data
❌ **Feature interpretation or production pipelines** in business-facing models

---

In conclusion, PCA is best seen as a **mathematically sound tool for dimensionality reduction**, but it’s **not a magic bullet**. Always ask:

> “Am I solving a geometric/statistical problem, or a human-readable, action-driven one?”

Use PCA wisely, not routinely.
