
## 1. 线性回归中的基本假设

在经典线性回归模型中，我们假设

$$
\mathbf{y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon
$$

* **$\mathbf{y}$**：$n\times1$ 的因变量向量
* **$\mathbf{X}$**：$n\times p$ 的设计矩阵（各列为不同的自变量）
* **$\boldsymbol\beta$**：$p\times1$ 的回归系数向量
* **$\boldsymbol\varepsilon$**：误差向量，满足 $\mathbb{E}[\boldsymbol\varepsilon]=\mathbf{0}$，$\mathrm{Var}(\boldsymbol\varepsilon)=\sigma^2\mathbf{I}$

**回归系数的OLS估计**：

$$
\hat{\boldsymbol\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
$$

这一推导的前提是矩阵 $\mathbf{X}^\top\mathbf{X}$ 必须可逆。

---

## 2. 多重共线性的定义

> **多重共线性（Multicolinearity）**：当设计矩阵 $\mathbf{X}$ 的某些列（自变量）之间存在高度（或完全）线性相关时，$\mathbf{X}^\top\mathbf{X}$ 会变得病态（接近奇异）或真正奇异，导致OLS估计不稳定。

* **完全共线性（[[Perfect Colinearity]]）**：存在非零向量 $\mathbf{a}$，使得

  $$
  \mathbf{X}\mathbf{a} = \mathbf{0},
  $$

  即某自变量可精确写成其他自变量的线性组合，此时 $\mathbf{X}^\top\mathbf{X}$ 严重奇异，OLS无法求出唯一解。

* **[[近似多重共线性]]（[[Near Multicolinearity]]）**：自变量高度相关，但不完全线性相关。此时 $\mathbf{X}^\top\mathbf{X}$ 条件数（condition number）很大，矩阵接近奇异，导致数值不稳定。

---

## 3. 多重共线性的成因

1. **模型设定**

   * 包含了重复信息的变量（如“身高（厘米）”和“身高（英寸）”同时入模）。
2. **数据采集**

   * 不同特征在样本中天然高度相关（如“家庭收入”与“消费支出”）。
3. **衍生变量**

   * 在原始变量基础上构造的交互项、二次项往往与原变量高度相关。

---

## 4. 多重共线性的危害 [[Effect]]

* **系数估计方差膨胀**

  $$
  \mathrm{Var}(\hat\beta_j) = \sigma^2\bigl[(\mathbf{X}^\top\mathbf{X})^{-1}\bigr]_{jj}
  $$

  当 $\mathbf{X}^\top\mathbf{X}$ 条件数很大，上述对角元增大，导致估计不精确。

* **估计不稳定**
  样本稍有变动，$\hat\beta$ 就可能出现剧烈波动，影响模型可靠性。

* **显著性检验失效**
  虽然整体回归可能显著，但单个回归系数 t 统计量可能不显著，混淆解释。

---

## 5. 多重共线性的检验方法 [[Detect]]

1. **方差膨胀因子（VIF, Variance Inflation Factor）**
   对第 $j$ 个自变量，构建辅助回归

   $$
   X_j = \sum_{k\neq j}\alpha_k X_k + u_j,
   $$

   计算 $R_j^2$，则

   $$
   \mathrm{VIF}_j = \frac{1}{1 - R_j^2}.
   $$

   * 若 $\mathrm{VIF}_j > 10$（有时以5为阈值），表明严重多重共线性。

2. **条件数（Condition Number）**
   计算标准化 $\mathbf{X}$ 的奇异值 $\{\sigma_{\max},\dots,\sigma_{\min}\}$，条件数

   $$
   \kappa = \frac{\sigma_{\max}}{\sigma_{\min}}.
   $$

   * $\kappa > 30$ 通常表明多重共线性问题；$\kappa > 100$ 则非常严重。

3. **特征值分解**
   分析 $\mathbf{X}^\top\mathbf{X}$ 的特征值谱。若存在接近于零的特征值，对应的特征向量指向高度相关的变量组合。

---

## 6. 应对多重共线性的方法 [[Solutions]]

1. **删除或合并变量**

   * 去掉冗余或对模型贡献有限的自变量。
2. **主成分回归（PCR）**

   * 对 $\mathbf{X}$ 先做主成分分析，取前 $k$ 个主成分做回归。
3. **岭回归（Ridge Regression）**

   * 在最小二乘目标中加 $\ell_2$ 罚项：

     $$
     \hat{\boldsymbol\beta}_{\text{ridge}} = \arg\min_\beta\;\|\mathbf{y}-\mathbf{X}\beta\|^2 + \lambda\|\beta\|^2.
     $$
   * λ>0 可改善 $\mathbf{X}^\top\mathbf{X}$ 的条件数。
4. **偏最小二乘（PLS）**

   * 同时考虑自变量与因变量，将降维与回归结合。

---

## 7. 小结

* **定义**：多重共线性是指自变量之间存在线性相关，从而使 $\mathbf{X}^\top\mathbf{X}$ 病态或奇异。
* **危害**：导致系数方差膨胀、估计不稳定、检验结论失真。
* **检测**：VIF、条件数、特征值分解等。
* **应对**：删除变量、PCR、岭回归、PLS 等方法。

掌握多重共线性的原理与检测方法，能帮助我们构建更稳健的线性回归模型。
