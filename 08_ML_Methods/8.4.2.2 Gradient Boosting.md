### 0 | The Big Picture

Boosting, in general, builds an **additive model**

$$
F_M(x)=\sum_{m=1}^{M}\beta_m\,h_m(x)
$$

by fitting one **base learner** $h_m$ at a time.
With **AdaBoost** you saw one specific recipe:

* **Loss** fixed (exponential)
* **β-weights** chosen in closed form
* **Re-weighting** done implicitly by the loss

**Gradient Boosting (GB)** keeps the same *additive* flavour but replaces the rigid AdaBoost mechanics with a **general-purpose “steepest-descent in function space”** approach.
That single shift lets us:

* optimize **any differentiable loss** (squared error, logistic, Huber, quantile…);
* use **any base learner** (trees, linear rules, splines, even neural nets);
* control over-fitting via a *learning rate* and other regularizers.

---

### 1 | From AdaBoost to Gradient Boosting in Four Steps

| AdaBoost Concept                            | Gradient-Boosting Generalisation                                      |
| ------------------------------------------- | --------------------------------------------------------------------- |
| Exponential loss $e^{-yF}$                  | **Generic differentiable loss** $L(y,F)$                              |
| Closed-form $\alpha_t$                      | **Numerical line search** (or fixed small step)                       |
| Weight update $D_{t+1}$ via loss derivative | **Gradient of loss** gives residuals / pseudo-responses               |
| Binary labels $\pm1$                        | Works for regression or classification (probabilities) out-of-the-box |

So, think “AdaBoost = gradient boosting with exponential loss and tiny decision stumps.”

---

### 2 | Core Mathematics: Gradient Descent in Function Space

1. **Objective:** minimise

   $$
   \mathcal{L}(F)=\sum_{i=1}^{N}L\bigl(y_i,\;F(x_i)\bigr)
   $$

   where $L$ is any differentiable loss in its second argument.

2. **View $F$ as a point in a (huge) function space.**
   The **negative gradient** of $\mathcal{L}$ w\.r.t. $F$ at the training points is

   $$
   r_{im}=-\left.\frac{\partial L(y_i,F)}{\partial F}\right|_{F=F_{m-1}(x_i)}
   $$

   These $r_{im}$ are called **residuals** (regression) or **pseudo-responses** (classification).

3. **Best descent direction:** fit a base learner $h_m(x)$ that approximates these residuals:

   $$
   h_m \approx \arg\min_{h\in\mathcal{H}}\sum_{i=1}^{N}\bigl(r_{im}-h(x_i)\bigr)^2
   $$

4. **Line search / step size:** find

   $$
   \beta_m=\arg\min_{\beta}\sum_{i=1}^{N}L\bigl(y_i,\;F_{m-1}(x_i)+\beta\,h_m(x_i)\bigr)
   $$

   (Often simplified to a **shrinkage factor** $\nu$, e.g. $\beta_m=\nu$.)

5. **Update additive model:**

   $$
   F_m(x)=F_{m-1}(x)+\beta_m h_m(x)
   $$

Repeat for $m=1,\dots,M$.

---

### 3 | Connecting the Dots with Concrete Losses

#### 3.1  **Squared-Error Loss**  (classical regression)

*Loss* $L(y,F)=\tfrac12(y-F)^2$
*Gradient* $r_{im}=y_i-F_{m-1}(x_i)$ (literal residuals)
So each boosting round:

1. **Fit $h_m$** to the ordinary residuals.
2. **Line search** has closed form: $\beta_m=1$.
   Result: **Gradient Boosting = stage-wise additive regression** (a.k.a. Friedman’s MART).

#### 3.2  **Logistic Loss**  (probability-level classification)

*Loss* $L(y,F)=\ln\!\bigl(1+e^{-2yF}\bigr)$
(with $y\in\{\pm1\}$, so $F$ is half the log-odds).
*Gradient* $r_{im}= \frac{2y_i}{1+e^{2y_iF_{m-1}(x_i)}}$
This produces “soft” residuals between $-2$ and $+2$.

Boosting **decision trees** with this loss is exactly what popular libraries (XGBoost, LightGBM, CatBoost) do for classification.

---

### 4 | Algorithm Cheat-Sheet

```
Input : {(x_i, y_i)}, differentiable loss L, base-learner family H,
        learning rate ν (0<ν≤1), rounds M
Init  : F_0(x) = argmin_c Σ_i L(y_i, c)      # e.g. mean(y) for squared error
for m = 1 … M:
    # 1. Compute negative gradient (pseudo-residuals)
    r_im = -∂L/∂F |_{F=F_{m-1}(x_i)}   for i=1…N
    # 2. Fit base learner to residuals
    h_m = argmin_{h∈H} Σ_i (r_im - h(x_i))^2
    # 3. Step size
    β_m = argmin_β Σ_i L(y_i, F_{m-1}(x_i)+β h_m(x_i))
          (often β_m←ν if expensive to solve)
    # 4. Update model
    F_m(x) = F_{m-1}(x) + β_m h_m(x)
Output: F_M(x)   # use argmax/ sign/ identity depending on task
```

---

### 5 | Why Trees?  (Gradient-Boosted Decision Trees, GBDT)

* Shallow trees (depth 3-8) capture **non-linear interactions** with minimal feature engineering.
* Each tree corrects *just the current errors* → ensemble remains flexible but avoids huge individual trees.
* Training & inference parallelise well across features and samples.

Modern variants (XGBoost, LightGBM, CatBoost) add:

| Trick                                | Purpose                                          |
| ------------------------------------ | ------------------------------------------------ |
| Histogram & GPU split finding        | Speed + memory                                   |
| Column subsampling & row subsampling | Extra variance reduction (random-forest flavour) |
| $L_1/L_2$ regularisation on leaves   | Shrink overly-specific leaf values               |
| Monotonic constraints / cat handling | Domain knowledge & categorical support           |

---

### 6 | Relationship to AdaBoost at a Glance

| Aspect                     | AdaBoost                               | Gradient Boosting                             |
| -------------------------- | -------------------------------------- | --------------------------------------------- |
| Loss                       | Exponential                            | **Any** differentiable (exponential included) |
| Step size $\beta_m$        | Closed form $\tfrac12\ln\frac{1-e}{e}$ | Numeric line search / constant shrinkage      |
| Base learner focus         | Implicit via sample weights            | Explicit via residuals                        |
| Typical base learner       | Decision stump                         | Deeper trees (or others)                      |
| Outlier sensitivity        | High (weights explode)                 | Depends on loss (Huber, quantile are robust)  |
| Interpretability (small M) | High (few stumps)                      | Moderate (many small trees)                   |

AdaBoost is **a special case**: exponential loss + step size solved exactly + weight-view instead of residual-view.

---

### 7 | Practical Hyper-parameters & Tips

| Knob                                  | Typical Range    | Effect / Advice                                             |
| ------------------------------------- | ---------------- | ----------------------------------------------------------- |
| **Learning rate $\nu$**               | 0.01 – 0.3       | Smaller ⇒ need more rounds but often better generalisation. |
| **Number of rounds $M$**              | 100 – 10 000     | Tune jointly with $\nu$. Early-stop on validation loss.     |
| **Tree depth**                        | 3 – 8            | Depth = interaction order captured.                         |
| **min\_child\_weight / leaf samples** | library-specific | Prevent tiny leaves (regularises).                          |
| **Subsample (rows)**                  | 0.5 – 1          | Adds randomness, combats over-fitting, speeds up.           |
| **Colsample (features)**              | 0.5 – 1          | Helpful on wide datasets.                                   |

**Workflow**: grid-search $(\nu, M)$ first, then fine-tune tree & sampling parameters.

---

### 8 | Worked Mini-Example (Squared-Error, Depth-1 Trees)

| $x$ | $y$ |
| --- | --- |
| 1   | 5   |
| 2   | 7   |
| 3   | 6   |
| 4   | 10  |

**Round 0.**  $F_0$ = mean = 7.0
Residuals $r_{i1}=y_i-7=[-2,0,-1,3]$

**Fit stump $h_1(x)=\begin{cases}6.0 & x<3\\8.5&x\ge 3\end{cases}$** (least-squares split at $x=3$)
Learning rate $\nu=0.1$ ⇒ $\beta_1=0.1$

Update
$F_1(x)=7+0.1\,h_1(x)$ (shrunk)

Repeat: compute new residuals $y−F_1$, fit another tiny stump, etc.
After \~100 rounds the ensemble of shallow trees nails the pattern with low variance.

*(Try coding this in a notebook—the logic mirrors AdaBoost but with familiar residuals instead of sample weights.)*

---

### 9 | Strengths & Weaknesses

| **Pros**                                                          | **Cons**                                                         |
| ----------------------------------------------------------------- | ---------------------------------------------------------------- |
| Handles many losses; works for regression & classification alike. | Training is sequential ⇒ slower than bagging for very large $M$. |
| Excellent predictive accuracy with minimal feature engineering.   | Sensitive to hyper-parameters; needs CV or early-stopping.       |
| Built-in variable importance & partial-dependence plots.          | Interpretability degrades as hundreds of trees accumulate.       |
| Robust variants (Huber, quantile) handle outliers well.           | Large ensembles can be memory-heavy.                             |

---

### 10 | Key Takeaways

1. **Same additive spirit** as AdaBoost, but driven by *gradients* instead of fixed formulas.
2. **Surrogate loss choice = behaviour control** (squared → mean prediction, logistic → probabilities, etc.).
3. **Residual fitting viewpoint** makes GB easy to understand and implement.
4. **Shrinkage + many small trees** > few large trees — bias-variance sweet spot.
5. Modern libraries add engineering tricks, but the *heart* is still:

> *Compute gradients → fit learner to them → take a small step.*

Master this loop and you will demystify not only Gradient Boosting but also its industrial-strength descendants like **XGBoost, LightGBM, CatBoost**, and even **Gradient-Boosted Neural Nets**.
