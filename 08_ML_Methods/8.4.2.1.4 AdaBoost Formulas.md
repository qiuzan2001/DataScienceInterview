*(forward-stagewise additive modeling & exponential loss)*
#### 1  The Core Optimization View

1. **Define a surrogate loss**
   Instead of the discontinuous 0–1 loss, AdaBoost minimizes the **exponential loss**

   $$
     \mathcal L(F)=\sum_{i=1}^{N}\exp\!\bigl(-y_i\,F(x_i)\bigr),
     \quad F(x)=\sum_{t=1}^{T}\alpha_t G_t(x),
   $$

   where
   • $y_i\in\{-1,+1\}$ are labels,
   • each $G_t$ is a weak learner returning $\pm1$,
   • $F(x)$ is the running “score” (a weighted sum of learners).

2. **Why exponential?**
   *Smooth & convex* in $\alpha_t$; *upper-bounds* the 0–1 indicator (whenever a point is mis-classified, its loss ≥ 1). Thus, driving the exponential loss down **guarantees the training error never increases**—each round can only help.

---

#### 2  Forward-Stagewise Additive Modeling (Greedy Boosting)

AdaBoost builds $F(x)$ **one small additive piece at a time**:

| Stagewise idea                                                                                                                                                                                           | Practical effect                                                       |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| *Freeze what you already have.* At round $t$ treat $F_{t-1}(x)$ as constant.                                                                                                                             | Turns the global problem into a **one-step optimization**.             |
| *Find the best new learner & its weight.* Choose $(\alpha_t,G_t)$ that minimize the loss **with respect to** them only:  $\underset{\alpha,G}{\arg\min}\;\sum_i e^{-y_i\,[F_{t-1}(x_i)+\alpha G(x_i)]}$. | Gives the **closed-form formulas** below and keeps the algorithm fast. |
| *Update* $F_t(x)=F_{t-1}(x)+\alpha_t G_t(x)$.                                                                                                                                                            | A **greedy, round-by-round** procedure; no back-tracking needed.       |

---

#### 3  Deriving the Two Famous Formulas

---

##### 3.1 Weighted error $e_t$

Inside the loss for a fixed candidate $G_t$:

$$
\mathcal L_t(\alpha)=
\sum_{\text{correct}} e^{-\alpha}
+\sum_{\text{wrong}} e^{+\alpha}
=(1-e_t)\,e^{-\alpha}+e_t\,e^{+\alpha},
$$

where

$$
e_t=\sum_{i}\!D_t(i)\,\mathbf 1\!\bigl[G_t(x_i)\neq y_i\bigr],
\qquad 
D_t(i)=\frac{e^{-y_i F_{t-1}(x_i)}}{Z_{t-1}}
$$

acts as a **probability weight** for each sample.

---

##### 3.2 Learner vote $\alpha_t$

Minimize $\mathcal L_t(\alpha)$ w\.r.t. $\alpha$:

$$
\frac{\partial\mathcal L_t}{\partial\alpha}=-(1-e_t)\,e^{-\alpha}+e_t\,e^{+\alpha}=0
\;\Longrightarrow\;
\boxed{\displaystyle
\alpha_t=\tfrac12\ln\!\frac{1-e_t}{e_t}}
$$

Properties

* • Monotone: better learners ($e_t\downarrow$) get larger votes.
* • Symmetric: $e_t=0.5\Rightarrow\alpha_t=0$ (no influence); $e_t>0.5\Rightarrow\alpha_t<0$ (flip its sign).

---

##### 3.3 Weight-update rule

Plug optimal $\alpha_t$ back into the sample weights:

$$
D_{t+1}(i)
\;=\;
\frac{D_t(i)\,e^{-\alpha_t\,y_i\,G_t(x_i)}}{Z_t}, 
$$

so

* Mis-classified ($y_i\neq G_t(x_i)$): multiply by $e^{+\alpha_t}$ → **weight increases**.
* Correct: multiply by $e^{-\alpha_t}$ → **weight decreases**.

The normalizer $Z_t$ ensures $D_{t+1}$ sums to 1 and is exactly the minimum loss value; hence each round cannot raise the training loss.

---

#### 4  Margins and Their Growth

The **margin** of sample $i$ after $t$ rounds is

$$
m_i^{(t)} \;=\; y_i\,F_t(x_i).
$$

Because every update pushes on the exponential loss, which penalizes small/negative margins exponentially, the algorithm tends to **drive margins upward**. Higher margins mean:

* Stronger classification confidence.
* Better empirical generalization (per margin theory).

---

#### 5  Connecting the Dots

1. **Start** with uniform weights (all samples equally important).
2. **Iterate**:

   * pick a learner that does best on the **current** weighted data,
   * compute its optimal vote $\alpha_t$ analytically,
   * reweigh the data to focus on the remaining hard cases.
3. **Repeat** until you run out of iterations or cannot find $e_t<0.5$.

Because each step provably lowers an upper bound on 0–1 error, the training error is non-increasing, and the cumulative margins rise—turning a sequence of “just-better-than-random” rules into a single, highly accurate classifier.

---

### Key Takeaways for Study

* **Exponential loss** is the bridge between intuitive “count the mistakes” training and tractable calculus.
* **Forward-stagewise** (greedy) fitting lets us solve for each learner’s weight in closed form.
* The famous formulas $\alpha_t=\tfrac12\ln\!\tfrac{1-e_t}{e_t}$ and
  $D_{t+1}(i)\propto D_t(i)\,e^{-\alpha_t y_i G_t(x_i)}$ are *not arbitrary*—they are the exact minimizers of the chosen surrogate loss at each stage.
* Growing **margins** explain why AdaBoost often keeps improving even after training error hits zero.

Master these links—loss choice → one-step optimization → weight dynamics—and the mechanics of AdaBoost become clear and memorable.
