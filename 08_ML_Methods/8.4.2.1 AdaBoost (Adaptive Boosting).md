### 0  |  Recap: What Is Boosting?

**Boosting** is an *ensemble* strategy that converts many weak learners (slightly-better-than-chance models) into a single strong learner.

1. **Iterative training.**

   * Train the first weak learner on the entire training set with uniform sample weights.
   * Re-weight the samples: raise the weight of misclassified (or high-error) instances so the next learner focuses on them.
   * Repeat for *T* rounds.
2. **Questions every boosting method must answer**

   * *How* is the weighted training error measured?
   * *How* are sample weights updated?
   * *How* is each learner‚Äôs vote (its ‚Äúimportance‚Äù) determined?
   * *How* are all learners combined at the end?

AdaBoost‚Äîshort for *Adaptive Boosting*‚Äîprovides one coherent set of answers.

---

### 1  |  AdaBoost

#### 1.1  Core Idea & Terminology

| Term                   | Meaning                                                                            |
| ---------------------- | ---------------------------------------------------------------------------------- |
| **Weakly learnable**   | A concept can be learned with accuracy only slightly better than random guessing.  |
| **Strongly learnable** | The concept can be learned to arbitrarily high accuracy with polynomial resources. |

A foundational PAC-learning theorem (Freund & Schapire, 1990) states that *weak learnability implies strong learnability*. Boosting‚ÄîAdaBoost in particular‚Äîconstructs the strong learner through a sequence of weak ones.

---

#### 1.2  AdaBoost for Binary **Classification** and its multiclass extension (SAMME)

##### 1.  Big Picture

AdaBoost turns many **weak learners**‚Äîrules that do only slightly better than chance‚Äîinto a **strong classifier**.
It does this by **re-weighting the training points** after every round so that the next weak learner focuses on the hardest-to-classify examples. The final decision is a **weighted vote** of all weak learners.

---

##### 2.  Key Ingredients

| Symbol                           | Meaning                                                     |
| -------------------------------- | ----------------------------------------------------------- |
| $x_i$                            | feature vector of the *i-th* example                        |
| $y_i \in \{-1,+1\}$              | binary class label                                          |
| $G_t(x)$                         | weak learner trained in round *t*                           |
| $D_t(i)$                         | probability weight of example *i* at the start of round *t* |
| $e_t$                            | weighted error of $G_t$                                     |
| $\alpha_t$                       | vote (importance) of $G_t$ in the final model               |
| $F(x)=\sum_{t}\alpha_t G_t(x)$   | real-valued score (‚Äúconfidence‚Äù)                            |
| $H(x)=\operatorname{sign}(F(x))$ | final binary prediction                                     |

---

##### 3.  The Four Repeating Steps

1. **Train a weak learner**
   Use the current distribution $D_t$. A sample with high weight appears more often or is emphasized more strongly, so the learner tries harder to get it right.

2. **Measure its weighted error**

   $$
     e_t=\sum_{i=1}^{N} D_t(i)\,\mathbf 1\bigl[G_t(x_i)\neq y_i\bigr].
   $$

   If $e_t=0.5$, the learner is no better than flipping a coin (for balanced weights). AdaBoost demands $e_t<0.5$; otherwise we discard the learner or stop training.

3. **Compute the learner‚Äôs vote**

   $$
     \alpha_t=\tfrac12\ln\!\left(\frac{1-e_t}{e_t}\right).
   $$

   *Why this shape?*

   * ‚Ä¢\* It grows as $e_t$ shrinks, so more accurate learners get more say.
   * ‚Ä¢\* Symmetry: if $e_t=0.5$ then $\alpha_t=0$ (no influence).
   * ‚Ä¢\* It arises exactly when we minimize **exponential loss** (see ¬ß4).

4. **Update the example weights**

   $$
     D_{t+1}(i)=\frac{D_t(i)\,e^{-\alpha_t y_i G_t(x_i)}}{Z_t},
   $$

   where $Z_t$ is a normalization constant so the weights sum to 1.

   * ‚Ä¢\* If an example was **misclassified** ($y_i\neq G_t(x_i)$), the exponent is $+\alpha_t$ so its weight **increases**.
   * ‚Ä¢\* If it was **correctly classified**, the exponent is $-\alpha_t$ so its weight **decreases**.
   * ‚Ä¢\* The larger $\alpha_t$, the stronger this re-weighting.

After **T** rounds we simply add the weak learners‚Äô votes:

$$
H(x)=\operatorname{sign}\!\bigl(\alpha_1 G_1(x)+\dots+\alpha_T G_T(x)\bigr).
$$
>[!example]-
>  
>  ## üéØ Goal
>  
>  Use AdaBoost to create a strong classifier from weak learners over a small dataset.
>  
>  ---
>  
>  ## üìò Dataset
>  
>  We‚Äôll use a **binary classification** problem with features and labels:
>  
| Sample $i$ | Feature $x_i$ | Label $y_i$ |
| ---------- | ------------- | ----------- |
| 1          | Sunny         | +1          |
| 2          | Rainy         | -1          |
| 3          | Sunny         | -1          |
| 4          | Cloudy        | +1          |
>  
>  We‚Äôll use **decision stumps** that ask:
>  **"Is the weather Sunny?"**, or **"Is it Rainy?"**
>  
>  ---
>  
>  ## üîÅ Round 1
>  
>  ### üîπ Step 1: Initialize Weights
>  
>  Uniform distribution:
>  
>  $$
>  D_1(i) = \frac{1}{4} = 0.25 \quad \text{for all } i
>  $$
>  
>  ---
>  
>  ### üîπ Step 2: Train Weak Learners and Pick the Best
>  
>  Try stump:
>  **"Is weather Sunny? ‚Üí then predict +1, else -1"**
>  
| Sample | Weather | True $y_i$ | Prediction $G_1(x_i)$ | Correct? |
| ------ | ------- | ---------- | --------------------- | -------- |
| 1      | Sunny   | +1         | +1                    | ‚úÖ        |
| 2      | Rainy   | -1         | -1                    | ‚úÖ        |
| 3      | Sunny   | -1         | +1                    | ‚ùå        |
| 4      | Cloudy  | +1         | -1                    | ‚ùå        |
>  
>  Misclassified: samples 3 and 4
>  ‚Üí **Error**:
>  
>  $$
>  e_1 = D_1(3) + D_1(4) = 0.25 + 0.25 = 0.5
>  $$
>  
>  Too high ‚Äî not usable.
>  
>  ---
>  
>  Try **another stump**:
>  **"Is weather Rainy? ‚Üí then predict -1, else +1"**
>  
| Sample | Weather | True $y_i$ | Prediction | Correct? |
| ------ | ------- | ---------- | ---------- | -------- |
| 1      | Sunny   | +1         | +1         | ‚úÖ        |
| 2      | Rainy   | -1         | -1         | ‚úÖ        |
| 3      | Sunny   | -1         | +1         | ‚ùå        |
| 4      | Cloudy  | +1         | +1         | ‚úÖ        |
>  
>  Only **sample 3** is wrong.
>  
>  $$
>  e_1 = D_1(3) = 0.25
>  $$
>  
>  ‚úÖ **Acceptable!** We choose this weak learner $G_1$
>  
>  ---
>  
>  ### üîπ Step 3: Compute Learner's Vote
>  
>  $$
>  \alpha_1 = \frac{1}{2} \ln\left(\frac{1 - e_1}{e_1}\right)
>  = \frac{1}{2} \ln\left(\frac{0.75}{0.25}\right)
>  = \frac{1}{2} \ln(3)
>  \approx \frac{1}{2} \cdot 1.0986
>  \approx 0.5493
>  $$
>  
>  ---
>  
>  ### üîπ Step 4: Update Weights
>  
>  We use:
>  
>  $$
>  D_2(i) = \frac{D_1(i) \cdot \exp(-\alpha_1 y_i G_1(x_i))}{Z_1}
>  $$
>  
>  Let‚Äôs compute $y_i G_1(x_i)$ and exponent:
>  
| $i$ | $y_i$ | $G_1(x_i)$ | $y_i G_1(x_i)$ | $\exp(-\alpha_1 y_i G_1(x_i))$ |
| --- | ----- | ---------- | -------------- | ------------------------------ |
| 1   | +1    | +1         | +1             | $e^{-0.5493} \approx 0.577$    |
| 2   | -1    | -1         | +1             | $e^{-0.5493} \approx 0.577$    |
| 3   | -1    | +1         | -1             | $e^{+0.5493} \approx 1.741$    |
| 4   | +1    | +1         | +1             | $e^{-0.5493} \approx 0.577$    |
>  
>  Unnormalized weights:
>  
>  $$
>  D_2'(i) = D_1(i) \cdot \exp(-\alpha_1 y_i G_1(x_i))
>  $$
>  
| $i$ | $D_1(i)$ | Unnormalized $D_2'(i)$      |
| --- | -------- | --------------------------- |
| 1   | 0.25     | $0.25 \cdot 0.577 = 0.1443$ |
| 2   | 0.25     | $0.1443$                    |
| 3   | 0.25     | $0.25 \cdot 1.741 = 0.4352$ |
| 4   | 0.25     | $0.1443$                    |
>  
>  Normalize:
>  
>  $$
>  Z_1 = \sum D_2'(i) = 0.1443 + 0.1443 + 0.4352 + 0.1443 = 0.8681
>  $$
>  
>  Now normalize:
>  
>  $$
>  D_2(i) = \frac{D_2'(i)}{Z_1}
>  $$
>  
| $i$ | $D_2(i)$                 |
| --- | ------------------------ |
| 1   | 0.1443 / 0.8681 ‚âà 0.1662 |
| 2   | ‚âà 0.1662                 |
| 3   | 0.4352 / 0.8681 ‚âà 0.5012 |
| 4   | ‚âà 0.1662                 |
>  
>  ‚úÖ Now **sample 3 has highest weight**, so the next learner will focus more on classifying it correctly.
>  
>  ---
>  
>  ## üîÅ Round 2
>  
>  ### üîπ Step 1: Train New Learner on $D_2$
>  
>  Try stump:
>  **"Is weather Sunny? ‚Üí predict -1, else +1"**
>  
| Sample | Weather | True $y_i$ | Prediction | Correct? |
| ------ | ------- | ---------- | ---------- | -------- |
| 1      | Sunny   | +1         | +1         | ‚úÖ        |
| 2      | Rainy   | -1         | -1         | ‚úÖ        |
| 3      | Sunny   | -1         | +1         | ‚ùå        |
| 4      | Cloudy  | +1         | +1         | ‚úÖ        |
>  
>  Misclassified: samples 1 and 2
>  Weighted error:
>  
>  $$
>  e_2 = D_2(1) + D_2(2) = 0.1662 + 0.1662 = 0.3324
>  $$
>  
>  ---
>  
>  ### üîπ Step 2: Compute Learner's Vote
>  
>  $$
>  \alpha_2 = \frac{1}{2} \ln\left( \frac{1 - 0.3324}{0.3324} \right)
>  = \frac{1}{2} \ln(2.007)
>  \approx \frac{1}{2} \cdot 0.695
>  \approx 0.3475
>  $$
>  
>  ---
>  
>  ## üß† Final Classifier
>  
>  The final classifier is:
>  
>  $$
>  F(x) = \alpha_1 G_1(x) + \alpha_2 G_2(x)
>  $$
>  
>  $$
>  H(x) = \operatorname{sign}(F(x))
>  $$
>  
>  Where:
>  
>  * $G_1(x)$: ‚ÄúIs weather Rainy? ‚Üí -1, else +1‚Äù
>  * $G_2(x)$: ‚ÄúIs weather Sunny? ‚Üí -1, else +1‚Äù
>  
>  ---
>  
>  ### ‚úÖ Prediction Example
>  
>  Let‚Äôs classify $x = \text{Sunny}$:
>  
>  * $G_1(x) = +1$
>  * $G_2(x) = -1$
>  * $F(x) = 0.5493 \cdot (+1) + 0.3475 \cdot (-1) = 0.2018$
>  * $H(x) = \operatorname{sign}(0.2018) = +1$
>  
>  Predicted label: **+1**
>  
>  ---
>  
>  ## ‚úÖ Summary
>  
| Step                   | Purpose                                   |
| ---------------------- | ----------------------------------------- |
| **Initialize weights** | Equal focus on all samples                |
| **Train weak learner** | Minimize weighted error                   |
| **Compute $\alpha_t$** | Assign influence based on accuracy        |
| **Update weights**     | Increase weight on misclassified examples |
| **Repeat**             | Focus shifts to harder cases              |
>  
>  This small example shows exactly **how AdaBoost adaptively focuses** on hard cases and builds a **strong final classifier** through weighted votes.
>  
---

##### [[8.4.2.1.4 AdaBoost Formulas | 4.  Where Do the Formulas Come From?]]

AdaBoost can be derived by posing the training objective

$$
\mathcal{L}(F)=\sum_{i=1}^N e^{-y_i F(x_i)}
$$

and performing **forward-stagewise additive modeling**:

| **Idea**                                                   | **Effect**                                                            |
| ---------------------------------------------------------- | --------------------------------------------------------------------- |
| Add one function $G_t$ at a time, keep earlier ones fixed. | Leads to a greedy, round-by-round procedure.                          |
| Minimize $\mathcal{L}$ w\.r.t. $\alpha_t$ and $G_t$.       | Gives exactly the $\alpha_t$ and weight update formulas.              |
| Exponential loss upper-bounds 0-1 loss.                    | Guarantees the training error never **increases** as rounds progress. |

A useful by-product: the margin $y_i F(x_i)$ grows with each round, which boosts confidence as well as accuracy.

---

##### 5.  Multiclass Extension ‚Äî **SAMME**

For $K>2$ classes, keep weak learners that output a single class label. Replace step 3 with

$$
\alpha_t = \ln\!\Bigl(\frac{1-e_t}{e_t}\Bigr) + \ln(K-1).
$$

Everything else stays the same, and when $K=2$ the extra term $\ln(K-1)=0$, reducing to the binary formula.

---

##### 6.  Practical Tips & Intuition

* **Weak learner choices**: decision stumps (1-split trees) are common; they satisfy the $e_t<0.5$ requirement on most real data.
* **Number of rounds**: more rounds reduce training error quickly but may over-fit noisy data. Early stopping or a validation set helps.
* **No learning-rate parameter**: classic AdaBoost relies purely on $\alpha_t$. Variants like AdaBoost-SAMME.R or Gradient Boosting introduce shrinkage.
* **Outliers**: because weights on hard examples can explode, AdaBoost is sensitive to mislabeled data. Pre-cleaning or using ‚Äúrobust‚Äù loss alternatives can help.
* **Interpretation of $F(x)$**: it‚Äôs proportional to the log-odds of the positive class under an exponential-loss assumption:

  $$
    \Pr(y=+1\mid x)\approx \frac{1}{1+e^{-2F(x)}}.
  $$

---

##### 7.  Minimal Pseudocode (Binary AdaBoost)

```
Input: training set {(x_i, y_i)}, y_i‚àà{‚àí1,+1}, rounds T
Initialize D_1(i) = 1/N for all i
for t = 1 ‚Ä¶ T:
    Train weak learner G_t using weights D_t
    e_t = Œ£_i D_t(i) [G_t(x_i) ‚â† y_i]
    if e_t ‚â• 0.5: break   # or skip this learner
    Œ±_t = 0.5 * ln((1 - e_t) / e_t)
    Update weights:
        D_{t+1}(i) = D_t(i) * exp(-Œ±_t * y_i * G_t(x_i))
    Normalize D_{t+1} so Œ£_i D_{t+1}(i) = 1
Output: F(x) = Œ£_t Œ±_t G_t(x);  H(x) = sign(F(x))
```

---

#### 1.3  AdaBoost for **Regression**

Replace classification error with a relative error measure.

1. **Maximum absolute error for round *t***
   $M_t=\max_i |\,y_i - G_t(x_i)\,|$.
2. **Relative error for each sample**
   $r_{ti}=|\,y_i - G_t(x_i)\,|/M_t$ (for squared loss use $r_{ti}=((y_i-G_t(x_i))/M_t)^2$; for exponential loss use $e^{r_{ti}}$).
3. **Round error** $e_t=\sum_i D_t(i) r_{ti}$.
4. **Learner weight** $\alpha_t = \frac12\ln\frac{1-e_t}{e_t}$ (same functional form).
5. **Weight update** identical in spirit to classification.

The final prediction is the **weighted median** of weak outputs:

$$
F(x)=\operatorname{median}_{\alpha\text{-weighted}}\{G_t(x)\}.
$$

---

### 2  |  Training-Error Analysis

For the binary case AdaBoost guarantees

$$
\text{Training error}\le \prod_{t=1}^{T} 2\sqrt{e_t(1-e_t)}.
$$

If every weak learner satisfies $e_t<\tfrac12-\gamma$ for some $\gamma>0$, then

$$
\text{Training error}\le e^{-2\gamma^2 T},
$$

an **exponential decay** in rounds.
*Take-away:* as long as each learner is at least slightly better than chance, AdaBoost drives the training error toward zero extremely fast.

---

### 3  |  Understanding AdaBoost via Forward Stagewise Additive Modeling

#### 3.1  Forward Stagewise Principle

* **Model form**: $F_m(x)=\sum_{t=1}^{m}\alpha_t h_t(x)$ (an *additive model*).
* **Algorithm**: in round *m* choose $\alpha_m,\;h_m$ that most reduce a chosen loss on the weighted data produced by earlier rounds; then add them to the current model.

#### 3.2  AdaBoost as a Special Case

Choose

* **Basis functions** $h_t$ = weak classifiers $G_t$.
* **Loss** = exponential $\ell(y,F)=\exp(-yF)$.

Solving the per-round minimization under this loss **exactly reproduces** AdaBoost‚Äôs formulas for $\alpha_t$ and $D_{t+1}$.
Hence *AdaBoost = forward stagewise additive modeling with exponential loss.*
This viewpoint demystifies the update rules and links AdaBoost to other additive-model methods such as gradient boosting.

---

### 4  |  Regularization: The Learning Rate

To curb potential overfitting we scale each learner‚Äôs vote:

$$
\alpha_t \leftarrow \nu\,\alpha_t,\qquad 0<\nu\le 1
$$

where $\nu$ is called the **learning rate** or **shrinkage** parameter (common default $\nu\approx0.1$).

* Smaller $\nu$ ‚Üí each learner is less influential ‚Üí need more rounds.
* Grid-search *both* $\nu$ and the maximum $T$ to balance bias-variance trade-offs.

---

### 5  |  Summary of Strengths and Weaknesses

| **Pros**                                                                                          | **Cons**                                                                           |
| ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| Often achieves very high classification accuracy.                                                 | Highly sensitive to *outliers*‚Äîthey may get ever-growing weights.                  |
| Flexible: any base learner (trees, neural nets, etc.) can be plugged in.                          | Weights can explode numerically without proper safeguards.                         |
| Simple final model for binary tasks: a weighted vote.                                             | Interpretability decreases as *T* grows (especially with non-trivial weak models). |
| Empirically resistant to overfitting on many data sets (when early-stopped or shrinkage is used). | Training is sequential, so less parallelizable than bagging.                       |

*Most popular weak learner:* shallow **CART decision trees** (a.k.a. decision stumps for depth = 1).

---

### Quick Checklist for Implementing AdaBoost

1. **Pick a base learner** (e.g., depth-1 or depth-3 trees).
2. **Set hyper-parameters**: learning rate $\nu$, number of rounds $T$.
3. **Initialize weights** $D_1(i)=1/N$.
4. **Loop** over $t=1,\dots,T$:

   * Fit learner on weighted data.
   * Compute $e_t$; if $e_t\ge0.5$ discard and retry.
   * Compute $\alpha_t$ (with shrinkage).
   * Update and normalize sample weights.
5. **Prediction**: for classification, take $\operatorname{sign}\bigl(\sum_t \alpha_t G_t(x)\bigr)$; for regression, take the weighted median.

Keep this note handy while studying or coding‚Äîunderstanding *why* each step exists will make the algorithm far less mysterious and far more powerful in your toolbox.
