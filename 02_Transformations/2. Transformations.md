### 2.1 Definition & Purpose

A **transformation** is any mathematical operation applied to a predictor (or response) variable before modeling.  Its goals are to:

1. **Improve linearity** between $X$ and $Y$, so that linear models (or additive models) fit better.
2. **Stabilize variance** (homoscedasticity) of residuals.
3. **Normalize** heavy-tailed or skewed distributions, making inference (e.g. confidence intervals, hypothesis tests) more reliable.
4. **Control outlier influence** by capping extreme values.
5. **Encode** categorical variables into numeric form for algorithms that require numeric inputs.

---

### 2.2 When & How to Identify the Need for a Transformation

1. **Residual Diagnostics**

   * **Residual vs. fitted plot** shows curvature ⇒ consider non-linear terms.
   * **Residual vs. predictor plot** shows funnel shape ⇒ variance instability, try variance-stabilizing transform.
2. **Distributional Checks**

   * **Histogram / density** of a feature is markedly skewed.
   * **Skewness / kurtosis** statistics far from zero.
   * **Q–Q plot** of residuals or raw $X$ departs from straight line.
3. **Model Performance**

   * Low $R^2$ or high test error despite complex model ⇒ perhaps relationship is non-linear or heteroscedastic.

> **Rule of thumb**: always explore univariate distributions and residual plots before and after fitting your baseline model.

---

### 2.3 Core Transformation Techniques

| Target-Unaware                                    | Target-Aware                        |
| :------------------------------------------------ | :---------------------------------- |
| **Dummy coding**                                  | **[[2.3.1 WOE & IV\| WOE coding]]** |
| **Binning** (unsupervised: equal-width, quantile) | **Binning** (supervised/optimal)    |
| **Winsorisation (Cap/Floor)**                     | **GAM smoothing**                   |
| **Box–Cox transform**                             |                                     |
| **Polynomial terms**                              |                                     |
| **Splines** (basis generation only)               |                                     |

# Core Transformation Techniques - Detailed Explanations

## Target-Unaware Techniques

### 1. One-Hot Encoding (Dummy Coding)

**Purpose**: Converts categorical variables into numerical format for machine learning algorithms that require numerical input.

**Mathematical Representation**:
For a categorical variable with $k$ categories, create $k$ binary indicator variables:
$$X_{ij} = \begin{cases} 1 & \text{if observation } i \text{ belongs to category } j \\ 0 & \text{otherwise} \end{cases}$$

**Detailed Example**:
```
Original: Color = [Red, Blue, Green, Red, Blue]
After One-Hot:
Red:   [1, 0, 0, 1, 0]
Blue:  [0, 1, 0, 0, 1]
Green: [0, 0, 1, 0, 0]
```

**Key Considerations**:
- **Multicollinearity**: If all $k$ columns are included, they sum to 1, creating perfect multicollinearity
- **Reference Coding**: Drop one category (reference level) to avoid this issue
- **Sparse Representation**: For high-cardinality categories, results in many zero values
- **Memory Impact**: Can significantly increase feature space dimensions

**When to Use**: 
- Nominal categorical variables with moderate cardinality
- When the algorithm cannot handle categorical data directly
- When categories have no inherent ordering

---

### 2. Dummy Coding

**Purpose**: A specific form of categorical encoding that explicitly uses a reference category.

**Mathematical Foundation**:
For $k$ categories, create $k-1$ binary variables:
$$\text{If category } = j, \text{ then } D_j = 1, \text{ all other } D_i = 0$$
$$\text{Reference category represented when all } D_i = 0$$

**Detailed Implementation**:
```
Color = {Red, Blue, Green}
Reference = Red (omitted)
Model variables:
- Blue_dummy = 1 if Blue, 0 otherwise
- Green_dummy = 1 if Green, 0 otherwise
- Red is represented when both dummies = 0
```

**Interpretation Benefits**:
- Coefficients represent **difference from reference category**
- Reference category coefficient is absorbed into intercept
- Easier to interpret in regression models
- Avoids the "dummy variable trap"

**Best Practices**:
- Choose meaningful reference category (e.g., most common, control group)
- Document which category is the reference
- Consider impact on coefficient interpretation

---

### 3. Unsupervised Binning

**Purpose**: Reduce continuous variable complexity and handle non-linear relationships without using target information.

#### Equal-Width Binning
**Formula**:
$$\text{bin}(x) = \left\lfloor \frac{x - \min(x)}{\text{width}} \right\rfloor$$
$$\text{where } \text{width} = \frac{\max(x) - \min(x)}{n_{\text{bins}}}$$

**Detailed Process**:
1. Calculate range: $R = \max(x) - \min(x)$
2. Determine width: $w = R / n_{\text{bins}}$
3. Create boundaries: $[x_{\min}, x_{\min} + w, x_{\min} + 2w, \ldots, x_{\max}]$
4. Assign observations to bins based on boundaries

**Example**:
```
Data: [1, 5, 8, 12, 15, 18, 22, 25, 30]
5 equal-width bins:
- Range: 30 - 1 = 29
- Width: 29/5 = 5.8
- Bins: [1-6.8], [6.8-12.6], [12.6-18.4], [18.4-24.2], [24.2-30]
```

#### Quantile Binning
**Formula**:
$$\text{bin}(x) = \text{rank}\left(\frac{x - \text{quantile boundaries}}{n_{\text{bins}}}\right)$$

**Detailed Process**:
1. Sort data in ascending order
2. Calculate percentile boundaries (e.g., 25th, 50th, 75th for quartiles)
3. Assign equal number of observations to each bin
4. Handle ties consistently

**Advantages & Limitations**:
- **Equal-width**: Simple, interpretable, but sensitive to outliers
- **Quantile**: Robust to outliers, balanced bin sizes, but boundaries may not be meaningful

---

### 4. Winsorisation (Capping/Flooring)

**Purpose**: Reduce impact of extreme outliers by capping values at specified percentiles.

**Mathematical Definition**:
$$x'_i = \begin{cases}
P_{\alpha/2}, & \text{if } x_i < P_{\alpha/2} \\
x_i, & \text{if } P_{\alpha/2} \leq x_i \leq P_{1-\alpha/2} \\
P_{1-\alpha/2}, & \text{if } x_i > P_{1-\alpha/2}
\end{cases}$$

where $P_q$ is the $q$-th percentile and $\alpha$ is the winsorization level.

**Detailed Implementation**:
```python
# 5% winsorization (2.5% each tail)
lower_bound = np.percentile(data, 2.5)
upper_bound = np.percentile(data, 97.5)
winsorized_data = np.clip(data, lower_bound, upper_bound)
```

**Example**:
```
Original: [1, 2, 3, 4, 5, 95, 96, 97, 98, 99]
5% Winsorization:
- 2.5th percentile: 1.225
- 97.5th percentile: 98.775
- Result: [1.225, 2, 3, 4, 5, 95, 96, 97, 98, 98.775]
```

**Key Considerations**:
- **Preserves sample size** (unlike trimming)
- **Reduces skewness** and kurtosis
- **Choice of percentile** affects results significantly
- **Information loss** at extremes

---

### 5. Box-Cox Transformation

**Purpose**: Normalize data distribution and stabilize variance through power transformations.

**Mathematical Definition**:
$$x^{(\lambda)} = \begin{cases}
\frac{x^\lambda - 1}{\lambda}, & \text{if } \lambda \neq 0 \\
\ln(x), & \text{if } \lambda = 0
\end{cases}$$

**Parameter Selection**:
The optimal $\lambda$ is found by maximizing the log-likelihood:
$$\ell(\lambda) = -\frac{n}{2}\ln\left(\frac{1}{n}\sum_{i=1}^n(x_i^{(\lambda)} - \bar{x}^{(\lambda)})^2\right) + (\lambda-1)\sum_{i=1}^n\ln(x_i)$$

**Common Lambda Values**:
- $\lambda = 2$: Square transformation (increases right skew)
- $\lambda = 1$: No transformation (original data)
- $\lambda = 0.5$: Square root transformation
- $\lambda = 0$: Natural log transformation
- $\lambda = -0.5$: Reciprocal square root
- $\lambda = -1$: Reciprocal transformation

**Detailed Process**:
1. **Check prerequisites**: Data must be positive ($x > 0$)
2. **Grid search**: Test various $\lambda$ values
3. **Evaluate normality**: Use Shapiro-Wilk test or Q-Q plots
4. **Apply transformation**: Use optimal $\lambda$
5. **Validate**: Check if transformation achieved desired properties

**Limitations**:
- Requires positive values only
- May not always achieve normality
- Interpretation becomes complex
- Inverse transformation needed for predictions

---

### 6. Polynomial Terms

**Purpose**: Capture non-linear relationships by adding power terms of existing features.

**Mathematical Representation**:
$$f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \ldots + \beta_p x^p$$

**Detailed Implementation**:
```python
# Creating polynomial features
X_poly = np.column_stack([X**i for i in range(1, degree+1)])
```

**Feature Engineering Example**:
```
Original feature: x = [1, 2, 3, 4, 5]
Degree 3 polynomial:
- x¹: [1, 2, 3, 4, 5]
- x²: [1, 4, 9, 16, 25]
- x³: [1, 8, 27, 64, 125]
```

**Key Considerations**:
- **Degree selection**: Higher degrees can cause overfitting
- **Multicollinearity**: High correlation between polynomial terms
- **Scaling issues**: Higher powers can have vastly different scales
- **Extrapolation problems**: Polynomials can behave poorly outside training range

**Best Practices**:
- Start with low degrees (2-3)
- Use cross-validation for degree selection
- Consider standardizing features first
- Monitor for overfitting with validation curves

---

### 7. Splines (Basis Generation)

**Purpose**: Create flexible non-linear transformations using piecewise polynomials.

**Mathematical Foundation**:
A spline of degree $d$ with knots $\kappa_1, \kappa_2, \ldots, \kappa_K$ is defined as:
$$s(x) = \sum_{j=0}^{d} \beta_j x^j + \sum_{k=1}^{K} \gamma_k (x - \kappa_k)_+^d$$

where $(x - \kappa_k)_+^d = \max(0, x - \kappa_k)^d$

**Basis Functions for Cubic Splines**:
$$\text{Basis} = \{1, x, x^2, x^3, (x-\kappa_1)_+^3, (x-\kappa_2)_+^3, \ldots, (x-\kappa_K)_+^3\}$$

**Detailed Construction**:
1. **Choose knot locations**: Often at quantiles or evenly spaced
2. **Generate basis matrix**: Each column is a basis function
3. **Fit coefficients**: Use standard regression techniques
4. **Ensure continuity**: Splines are continuous at knots

**Example with 2 knots**:
```
Knots at x = 3, 7
Basis functions:
- f₁(x) = 1
- f₂(x) = x
- f₃(x) = x²
- f₄(x) = x³
- f₅(x) = (x-3)₊³
- f₆(x) = (x-7)₊³
```

**Advantages**:
- **Flexibility**: Can model complex non-linear relationships
- **Locality**: Changes in one region don't affect others
- **Smoothness**: Continuous derivatives at knots
- **Interpretability**: Piecewise nature is understandable

---

## Target-Aware Techniques

### [[2.3.1 WOE & IV | 1. Weight of Evidence (WOE)]]

**Purpose**: Transform categorical variables based on their relationship with a binary target, creating a monotonic relationship.

**Mathematical Definition**:
$$\text{WOE}_i = \ln\left(\frac{P(X_i|Y=1)}{P(X_i|Y=0)}\right) = \ln\left(\frac{\text{Distribution of Goods}}{\text{Distribution of Bads}}\right)$$

**Detailed Calculation**:
For category $i$:
$$\text{WOE}_i = \ln\left(\frac{n_{i1}/n_{1\cdot}}{n_{i0}/n_{0\cdot}}\right)$$

where:
- $n_{i1}$ = number of events (Y=1) in category $i$
- $n_{i0}$ = number of non-events (Y=0) in category $i$
- $n_{1\cdot}$ = total number of events
- $n_{0\cdot}$ = total number of non-events

**Step-by-Step Example**:
```
Category | Events | Non-Events | Total | % Events | % Non-Events | WOE
A        | 100    | 200        | 300   | 20%      | 40%          | ln(0.2/0.4) = -0.693
B        | 150    | 150        | 300   | 30%      | 30%          | ln(0.3/0.3) = 0.000
C        | 250    | 150        | 400   | 50%      | 30%          | ln(0.5/0.3) = 0.511
Total    | 500    | 500        | 1000  | 100%     | 100%         |
```

**Key Properties**:
- **Monotonic encoding**: Creates ordered relationship with target
- **Handles missing values**: Can assign specific WOE to missing category
- **Reduces dimensionality**: Single value per category
- **Interpretable**: Positive WOE indicates higher likelihood of event

**Best Practices**:
- Ensure sufficient sample size in each category (>30 observations)
- Handle zero counts by adding small constant (Laplace smoothing)
- Use out-of-fold encoding to prevent overfitting
- Monitor for extreme WOE values (>±3)

---

### 2. Information Value (IV)

**Purpose**: Measure the predictive power of a feature for binary classification.

**Mathematical Definition**:
$$\text{IV} = \sum_{i=1}^{n} (\text{WOE}_i) \times (P(X_i|Y=1) - P(X_i|Y=0))$$

**Alternative Formulation**:
$$\text{IV} = \sum_{i=1}^{n} \left(\frac{n_{i1}}{n_{1\cdot}} - \frac{n_{i0}}{n_{0\cdot}}\right) \times \ln\left(\frac{n_{i1}/n_{1\cdot}}{n_{i0}/n_{0\cdot}}\right)$$

**Detailed Calculation Example**:
```
Using previous WOE example:
Category | WOE    | %Events | %Non-Events | Diff    | IV Contribution
A        | -0.693 | 0.20    | 0.40        | -0.20   | (-0.693) × (-0.20) = 0.139
B        | 0.000  | 0.30    | 0.30        | 0.00    | (0.000) × (0.00) = 0.000
C        | 0.511  | 0.50    | 0.30        | 0.20    | (0.511) × (0.20) = 0.102
Total IV = 0.139 + 0.000 + 0.102 = 0.241
```

**Interpretation Guidelines**:
- **IV < 0.02**: Weak predictive power (not useful)
- **0.02 ≤ IV < 0.1**: Weak predictive power (limited use)
- **0.1 ≤ IV < 0.3**: Medium predictive power (good)
- **0.3 ≤ IV < 0.5**: Strong predictive power (very good)
- **IV ≥ 0.5**: Very strong predictive power (check for overfitting)

**Applications**:
- **Feature selection**: Rank features by IV
- **Model validation**: Compare IV across different samples
- **Business understanding**: Identify key drivers
- **Regulatory compliance**: Document feature importance

---

### 3. Supervised Binning

**Purpose**: Create optimal bins for continuous variables that maximize separation between target classes.

**Common Algorithms**:

#### Chi-Square Binning
**Objective**: Maximize chi-square statistic between bins and target
$$\chi^2 = \sum_{i=1}^{k} \sum_{j=1}^{2} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

where $O_{ij}$ is observed frequency and $E_{ij}$ is expected frequency.

#### Entropy-Based Binning
**Objective**: Minimize weighted entropy across bins
$$\text{Entropy} = -\sum_{i=1}^{k} w_i \sum_{j=1}^{2} p_{ij} \log(p_{ij})$$

where $w_i$ is the weight of bin $i$ and $p_{ij}$ is the probability of class $j$ in bin $i$.

**Detailed Process**:
1. **Initialize**: Start with fine-grained bins or continuous values
2. **Evaluate splits**: Test potential split points
3. **Optimize criterion**: Choose split that maximizes separation
4. **Recursive splitting**: Continue until stopping criterion met
5. **Validate**: Ensure statistical significance of splits

**Example**:
```
Income data with binary target (default/no default):
Initial range: [20K, 200K]
Optimal splits found at: 45K, 80K, 120K
Final bins:
- [20K-45K]: 15% default rate
- [45K-80K]: 8% default rate  
- [80K-120K]: 3% default rate
- [120K-200K]: 1% default rate
```

**Advantages**:
- **Optimal separation**: Maximizes discriminative power
- **Automatic**: No manual threshold selection
- **Interpretable**: Clear business meaning
- **Robust**: Less sensitive to outliers than continuous variables

---

### 4. Generalized Additive Models (GAM) Smoothing

**Purpose**: Capture non-linear relationships between features and target using smooth functions.

**Mathematical Framework**:
$$g(E[Y|X]) = \beta_0 + \sum_{j=1}^{p} f_j(X_j)$$

where $g(\cdot)$ is the link function and $f_j(\cdot)$ are smooth functions.

**Detailed Components**:

#### Smooth Functions
Common choices for $f_j(X_j)$:
- **Splines**: Piecewise polynomials with knots
- **LOESS**: Local regression smoothing
- **Penalized splines**: Regularized to prevent overfitting

#### Estimation Process
1. **Backfitting Algorithm**:
   - Initialize: $f_j^{(0)}(X_j) = 0$ for all $j$
   - Iterate until convergence:
     $$f_j^{(m+1)}(X_j) = \text{Smoother}\left(X_j, Y - \beta_0 - \sum_{k \neq j} f_k^{(m)}(X_k)\right)$$

2. **Smoothing Parameter Selection**:
   - Cross-validation
   - Generalized cross-validation (GCV)
   - AIC/BIC criteria

**Practical Implementation**:
```python
# Conceptual GAM for binary classification
from pygam import LogisticGAM, s

# Define smooth terms
gam = LogisticGAM(s(0) + s(1) + s(2))
gam.fit(X, y)

# Extract smooth functions
for i in range(X.shape[1]):
    smooth_func = gam.partial_dependence(term=i, X=X)
```

**Feature Engineering Applications**:
- **Automatic binning**: Use GAM predictions as bin boundaries
- **Feature transformation**: Apply learned smooth functions
- **Interaction detection**: Identify where non-linearity matters most
- **Interpretability**: Visualize partial dependence plots

**Key Advantages**:
- **Flexibility**: Can model any smooth relationship
- **Interpretability**: Individual feature effects are visible
- **Automatic**: No need to specify functional form
- **Regularization**: Built-in overfitting protection

**Considerations**:
- **Computational cost**: More expensive than linear models
- **Sample size**: Requires sufficient data for stable estimates
- **Extrapolation**: Behavior outside training range uncertain
- **Feature interactions**: Standard GAM assumes additivity

---

## Choosing Between Techniques

### Decision Framework:

1. **Target Available?**
   - Yes → Consider target-aware techniques
   - No → Use target-unaware techniques

2. **Data Type?**
   - Categorical → One-hot, dummy, WOE
   - Continuous → Binning, transformations, splines

3. **Relationship with Target?**
   - Linear → Simple transformations
   - Non-linear → Splines, GAM, polynomial

4. **Interpretability Requirements?**
   - High → Binning, WOE, simple transformations
   - Low → Complex splines, high-degree polynomials

5. **Sample Size?**
   - Small → Simple techniques
   - Large → Complex techniques like GAM

This comprehensive framework ensures appropriate technique selection based on data characteristics and modeling objectives.